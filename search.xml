<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Twitter Dataset - Using LSTM to predict the emotion of the article</title>
      <link href="/en/posts/nlp-twitter-emotion-diagnoise/"/>
      <url>/en/posts/nlp-twitter-emotion-diagnoise/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction">Introduction</h1><p>Recently, I took an AI course, and this is the sixth assignment. The main content taught includes the following topics:</p><ol><li>Learn to use LSTM</li><li>Use SpaCy</li></ol><h1 id="Homework-Requirements">Homework Requirements</h1><p>Train a text classification on the <a href="https://github.com/cardiffnlp/tweeteval">TweetEval</a> emotion recognition dataset using LSTMs and GRUs.</p><ol><li><strong>Build an LSTM model</strong>: Follow the example described <a href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html">here</a>. Use the same architecture, but:<ol><li>only use the last output of the LSTM in the loss function</li><li>use an embedding dim of 128</li><li>use a hidden dim of 256.</li></ol></li><li><strong>Use SpaCy to split words</strong>: Use spaCy to split the tweets into words.</li><li><strong>Select the Top 5000 words</strong>: Limit your vocabulary (i.e., the words that you converted to an index) to the most frequent 5000 words and replace all other words with a placeholder index (e.g., 1001).</li><li><strong>Train the model and calculate accuracy</strong>: Evaluate the accuracy on the test set. (Note: If the training takes too long, try to use only a fraction of the training data.)</li><li><strong>Build and train a GRU model</strong>: Do the same, but this time use GRUs instead of LSTMs.</li></ol><h1 id="Task-0-Download-the-Dataset">Task 0: Download the Dataset</h1><div class="note info flat"><p>In this section, we need to do the following:</p><ol><li>Download the dataset</li><li>Use pandas to convert the dataset into the format we need</li></ol></div><h2 id="Download-the-Dataset">Download the Dataset</h2><ol><li>Refer to this link to download the required data: <a href="https://github.com/cardiffnlp/tweeteval">TweetEval</a></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/cardiffnlp/tweeteval.git</span><br></pre></td></tr></table></figure><ol start="2"><li>After downloading, you can see the following information, the <code>emotion</code> folder is the information we will use this time:</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── README.md</span><br><span class="line">├── TweetEval_Tutorial.ipynb</span><br><span class="line">├── datasets</span><br><span class="line">│   ├── README.txt</span><br><span class="line">│   ├── emoji</span><br><span class="line">│   ├── emotion <span class="comment"># This is the data we need</span></span><br><span class="line">│   │   ├── mapping.txt <span class="comment"># Emotion corresponding to numbers e.g. &#123;0:&#x27;angry&#x27;, 1:&#x27;happy&#x27;&#125;</span></span><br><span class="line">│   │   ├── test_labels.txt <span class="comment"># Emotion labels for test data, i.e., the answers e.g. 0 </span></span><br><span class="line">│   │   ├── test_text.txt <span class="comment"># Content of the test data e.g. &quot;I&#x27;m so angry&quot;</span></span><br><span class="line">│   │   ├── train_labels.txt <span class="comment"># Emotion labels for training data, i.e., the answers e.g. 0</span></span><br><span class="line">│   │   ├── train_text.txt <span class="comment"># Content of the training data e.g. &quot;I&#x27;m so angry&quot;</span></span><br><span class="line">│   │   ├── val_labels.txt <span class="comment"># Emotion labels for validation data, i.e., the answers e.g. 0</span></span><br><span class="line">│   │   └── val_text.txt <span class="comment"># Content of the validation data e.g. &quot;I&#x27;m so angry&quot;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="Covert-data-format">Covert data format</h2><p>We first introduce the required packages:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CNN</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># others</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> Flowers102</span><br><span class="line"></span><br><span class="line"><span class="comment"># read file </span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># label</span></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure><p>Then we convert the data into the format we need, this time we use <code>panda</code> to process the data and read the data into variables for later use.</p><div class="note warning flat"><p>Make sure to change the root path to the folder path of your git clone!</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the relative path of each file first </span></span><br><span class="line">root = <span class="string">&#x27;../../Data/tweeteval/datasets/emotion/&#x27;</span></span><br><span class="line">mapping_file = os.path.join(root, <span class="string">&#x27;mapping.txt&#x27;</span>)</span><br><span class="line">test_labels_file = os.path.join(root, <span class="string">&#x27;test_labels.txt&#x27;</span>)</span><br><span class="line">test_text_file = os.path.join(root, <span class="string">&#x27;test_text.txt&#x27;</span>)</span><br><span class="line">train_labels_file = os.path.join(root, <span class="string">&#x27;train_labels.txt&#x27;</span>)</span><br><span class="line">train_text_file = os.path.join(root, <span class="string">&#x27;train_text.txt&#x27;</span>)</span><br><span class="line">val_labels_file = os.path.join(root, <span class="string">&#x27;val_labels.txt&#x27;</span>)</span><br><span class="line">val_text_file = os.path.join(root, <span class="string">&#x27;val_text.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use panda to read the data and read the labels </span></span><br><span class="line">mapping_pd = pd.read_csv(mapping_file, sep=<span class="string">&#x27;\t&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">test_label_pd = pd.read_csv(test_labels_file, sep=<span class="string">&#x27;\t&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">train_label_pd = pd.read_csv(train_labels_file, sep=<span class="string">&#x27;\t&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line">val_label_pd = pd.read_csv(val_labels_file, sep=<span class="string">&#x27;\t&#x27;</span>, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use \n to split the content of the training and testing data, and remove the last blank word_embeddings </span></span><br><span class="line"><span class="comment"># because test_dataset[-1] is empty, and the length will be consistent with the length of labels after removing the length </span></span><br><span class="line">test_dataset = <span class="built_in">open</span>(test_text_file).read().split(<span class="string">&#x27;\n&#x27;</span>)[:-<span class="number">1</span>] <span class="comment"># remove last empty line </span></span><br><span class="line">train_dataset = <span class="built_in">open</span>(train_text_file).read().split(<span class="string">&#x27;\n&#x27;</span>)[:-<span class="number">1</span>] <span class="comment"># remove last empty line</span></span><br><span class="line">val_dataset = <span class="built_in">open</span>(val_text_file).read().split(<span class="string">&#x27;\n&#x27;</span>)[:-<span class="number">1</span>] <span class="comment"># remove last empty line</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the length of the dataset </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;len(train_dataset)= <span class="subst">&#123;<span class="built_in">len</span>(train_dataset)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;len(train_label_pd)= <span class="subst">&#123;<span class="built_in">len</span>(train_label_pd)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;=== train_label_pd === \n<span class="subst">&#123;train_label_pd.value_counts()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;len(test_dataset)= <span class="subst">&#123;<span class="built_in">len</span>(test_dataset)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;len(test_label_pd)= <span class="subst">&#123;<span class="built_in">len</span>(test_label_pd)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;=== test_label_pd === \n<span class="subst">&#123;test_label_pd.value_counts()&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(train_dataset)= <span class="number">3257</span></span><br><span class="line"><span class="built_in">len</span>(train_label_pd)= <span class="number">3257</span></span><br><span class="line">=== train_label_pd === </span><br><span class="line"><span class="number">0</span>    <span class="number">1400</span></span><br><span class="line"><span class="number">3</span>     <span class="number">855</span></span><br><span class="line"><span class="number">1</span>     <span class="number">708</span></span><br><span class="line"><span class="number">2</span>     <span class="number">294</span></span><br><span class="line">Name: count, dtype: int64</span><br><span class="line"><span class="built_in">len</span>(test_dataset)= <span class="number">1421</span></span><br><span class="line"><span class="built_in">len</span>(test_label_pd)= <span class="number">1421</span></span><br><span class="line">=== test_label_pd === </span><br><span class="line"><span class="number">0</span>    <span class="number">558</span></span><br><span class="line"><span class="number">3</span>    <span class="number">382</span></span><br><span class="line"><span class="number">1</span>    <span class="number">358</span></span><br><span class="line"><span class="number">2</span>    <span class="number">123</span></span><br><span class="line">Name: count, dtype: int64</span><br></pre></td></tr></table></figure><h1 id="Task-1-5-Build-LSTM-GRU-Models">Task 1 + 5: Build LSTM, GRU Models</h1><div class="note info flat"><ol><li><strong>Build an LSTM model</strong>: Follow the example described <a href="https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html">here</a>. Use the same architecture, but:<ol><li>only use the last output of the LSTM in the loss function</li><li>use an embedding dim of 128</li><li>use a hidden dim of 256.</li></ol></li><li><strong>Build and train a GRU model</strong>: Do the same, but this time use GRUs instead of LSTMs.</li></ol></div><p>From the official example, we can learn how to build an LSTM model, which basically includes the following elements:</p><ul><li><code>hidden_dim</code>: The dimension of the hidden layer, representing the number of neurons in the hidden layer.</li><li><code>word_embeddings</code>: Converts each word in the input sentence into word vectors.<ul><li><code>embedding_dim(vocab_size, embedding_dim)</code>:<ul><li><code>vocab_size</code>: The size of the dictionary, i.e., the total number of words we have. In this example, we will input 5001 words: 5000 common words + 1 unrecognized word.</li><li><code>embedding_dim</code>: Represents mapping each word or symbol to a fixed-size vector space. For instance, if your <code>embedding_dim</code> is set to 6, and your input vector is [1, 2, 3, 5], the model will map each number to a six-dimensional vector space, forming a representation like [1, 2, 3, 5, 6, 4].</li></ul></li><li><code>lstm(input_size, hidden_size, dropout)</code><ul><li><code>input_size</code>: The dimension of the input, which is our word vector dimension.</li><li><code>hidden_size</code>: The dimension of the hidden layer, representing the number of neurons in the hidden layer.</li><li><code>dropout</code>: The proportion of dropout, default is 0, meaning no dropout is used.</li></ul></li><li><code>hidden2tag(in_features, out_features)</code><ul><li><code>in_features</code>: The input dimension, which is also the word vector dimension.</li><li><code>out_features</code>: The output dimension, which is the dimension of our emotion labels.</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMTagger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMTagger, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        <span class="comment"># Convert the input word into a word vector </span></span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The LSTM takes word embeddings as inputs, and outputs hidden states</span></span><br><span class="line">        <span class="comment"># with dimensionality hidden_dim.</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, dropout=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The linear layer that maps from hidden state space to tag space</span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sentence</span>):</span><br><span class="line">        <span class="comment"># convert the input word into a word vector. Now the sentence is already an index vector</span></span><br><span class="line">        embeds = self.word_embeddings(sentence) </span><br><span class="line">        <span class="comment"># Use the index vector as the input of the LSTM model to get the output and hidden state of the LSTM layer</span></span><br><span class="line">        lstm_out, _ = self.lstm(embeds.view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>)) </span><br><span class="line">        <span class="comment"># Take only the last output of the LSTM</span></span><br><span class="line">        last_output = lstm_out[-<span class="number">1</span>].view(<span class="number">1</span>, -<span class="number">1</span>)  </span><br><span class="line">        tag_space = self.hidden2tag(last_output) <span class="comment">#  Use the last output of the LSTM model to convert to the word tag space</span></span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>) <span class="comment"># Use log_softmax to convert to probability </span></span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure><p>GRU and LSTM are similar, except that the only thing to modify is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRUTagger</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># Here !!! Change to GRU  </span></span><br><span class="line">        self.gru = nn.GRU(embedding_dim, hidden_dim, dropout=dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sentence</span>):</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># Here !!! Change to GRU </span></span><br><span class="line">        <span class="comment">## Use the index vector as the input of the GRU model to get the output and hidden state of the GRU layer</span></span><br><span class="line">        gru_out, _ = self.gru(embeds.view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>)) <span class="comment"># 將詞向量作為LSTM模型的輸入 得到LSTM曾的輸出和隱藏狀態</span></span><br><span class="line">        last_output = gru_out[-<span class="number">1</span>].view(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># Selecting the last output 為了滿足作業要求，我們只取最後一個輸出 </span></span><br><span class="line">        ... </span><br></pre></td></tr></table></figure><p>After the above modification, the completed LSTM program is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GRUTagger</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GRUTagger, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.gru = nn.GRU(embedding_dim, hidden_dim, dropout=dropout) <span class="comment"># &lt;== Here ! </span></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, sentence</span>):</span><br><span class="line">        embeds = self.word_embeddings(sentence) </span><br><span class="line">        gru_out, _ = self.gru(embeds.view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>))  <span class="comment"># &lt;== Here ! </span></span><br><span class="line">        last_output = gru_out[-<span class="number">1</span>].view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># &lt;== Here ! </span></span><br><span class="line">        tag_space = self.hidden2tag(last_output)</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure><h1 id="Task-2-3-Split-Words-Using-SpaCy-Find-Top-5000-Words">Task 2 + 3: Split Words Using SpaCy, Find Top 5000 Words</h1><div class="note info flat"><p>We have already placed the necessary data into a list variable in Task 0, with each data entry being a sentence. Now, we need to do a few things:<br>2. <strong>Split Words Using SpaCy</strong>: Use spaCy to split the tweets into words.<br>3. <strong>Select Top 5000 Words</strong>: Limit your vocabulary (i.e., the words that you converted to an index) to the most frequent 5000 words and replace all other words with a placeholder index (e.g., 1001).</p></div><h2 id="Install-SpaCy">Install SpaCy</h2><p>We need to execute the following commands to install the SpaCy package:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you are using Python3 </span></span><br><span class="line">pip install -U spacy</span><br><span class="line"><span class="comment"># If you are using Anaconda </span></span><br><span class="line">conda install -c conda-forge spacy</span><br></pre></td></tr></table></figure><p>As we are analyzing English text, we need to download the English model. Execute the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m spacy download en_core_web_sm</span><br></pre></td></tr></table></figure><p>Only then can we import the spacy package in the notebook and use the English model.</p><div class="note warning flat"><p>If the above command is not executed, you will encounter an error here!!<br>nlp = spacy.load(“en_core_web_sm”)</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy </span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="comment"># use spacy to tokenize the sentence with english model </span></span><br><span class="line">nlp = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>) <span class="comment"># &lt;=== If the above command is not executed, you will encounter an error here!!</span></span><br></pre></td></tr></table></figure><h2 id="Prepare-a-Dictionary-of-Top-5000-Common-Words">Prepare a Dictionary of Top 5000 Common Words</h2><p>We need to identify the top 5000 common words and create a dictionary for this purpose:</p><ol><li>First, prepare a string concatenating all sentences.</li><li>Then, send the entire string to spacy for data segmentation, filtering out <code>punctuation (punct)</code>, <code>stop words</code>, and <code>spaces</code>.</li><li>Use the Counter package to count the words, which facilitates identifying the top 5000 common words.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># join all the sentence together </span></span><br><span class="line"><span class="comment"># e.g. [&#x27;today is good&#x27;, &#x27;today is bad&#x27;] =&gt; [&#x27;today is good today is bad&#x27;]</span></span><br><span class="line">text = <span class="string">&#x27; &#x27;</span>.join(train_dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># use spacy to tokenize the sentence </span></span><br><span class="line">doc = nlp(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># filter out the punctuation and stop words</span></span><br><span class="line">word_freq = Counter(token.text <span class="keyword">for</span> token <span class="keyword">in</span> doc \</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> token.is_punct <span class="keyword">and</span> \</span><br><span class="line">                        <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> \</span><br><span class="line">                            <span class="keyword">not</span> token.is_space )</span><br><span class="line">word_freq</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Counter(&#123;<span class="string">&#x27;@user&#x27;</span>: <span class="number">2019</span>, <span class="comment">--  @user appear 2019 times</span></span><br><span class="line">         <span class="string">&#x27;like&#x27;</span>: <span class="number">212</span>,</span><br><span class="line">         <span class="string">&#x27;amp&#x27;</span>: <span class="number">148</span>,</span><br><span class="line">         <span class="string">&#x27;people&#x27;</span>: <span class="number">126</span>,</span><br><span class="line">         <span class="string">&#x27;know&#x27;</span>: <span class="number">96</span>,</span><br><span class="line">         <span class="string">&#x27;think&#x27;</span>: <span class="number">92</span>,</span><br><span class="line">         <span class="string">&#x27;sad&#x27;</span>: <span class="number">90</span>,</span><br><span class="line">         <span class="string">&#x27;got&#x27;</span>: <span class="number">85</span>,</span><br><span class="line">         <span class="string">&#x27;day&#x27;</span>: <span class="number">81</span>,</span><br><span class="line">         <span class="string">&#x27;u&#x27;</span>: <span class="number">80</span>,</span><br><span class="line">         <span class="string">&#x27;time&#x27;</span>: <span class="number">78</span>,</span><br><span class="line">         <span class="string">&#x27;✨&#x27;</span>: <span class="number">75</span>,</span><br><span class="line">         <span class="string">&#x27;😂&#x27;</span>: <span class="number">75</span>,</span><br><span class="line">         <span class="string">&#x27;want&#x27;</span>: <span class="number">74</span>,</span><br><span class="line">         <span class="string">&#x27;life&#x27;</span>: <span class="number">73</span>,</span><br><span class="line">         <span class="string">&#x27;going&#x27;</span>: <span class="number">69</span>,</span><br><span class="line">         <span class="string">&#x27;feel&#x27;</span>: <span class="number">67</span>,</span><br><span class="line">         <span class="string">&#x27;angry&#x27;</span>: <span class="number">66</span>,</span><br><span class="line">         <span class="string">&#x27;2&#x27;</span>: <span class="number">65</span>,</span><br><span class="line">         ...&#125;)</span><br></pre></td></tr></table></figure><p>Next, we can select the top 5000 words based on the number of times they appear:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># select the top 5000 most common words </span></span><br><span class="line">most_common_words = word_freq.most_common(<span class="number">5000</span>)</span><br><span class="line"><span class="comment"># Build a dictionary mapping words to indexes e.g. &#123;&#x27;hello&#x27;:0, &#x27;like&#x27;:1 ...&#125; </span></span><br><span class="line">vocab = &#123;word[<span class="number">0</span>]: idx <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(most_common_words)&#125;</span><br></pre></td></tr></table></figure><h2 id="Convert-Sentences-to-Tensors">Convert Sentences to Tensors</h2><p>With the <code>vocab</code> dictionary at hand, we can now convert sentences into an index format based on this dictionary. For example:</p><ul><li>Original sentence: <code>I like apple</code></li><li>Converted into index format: <code>[100, 3923, 123]</code></li></ul><p>But what if we encounter a word that we don’t understand or is not included in the dictionary?</p><ul><li>Here, we also need a <code>placeholder_index</code>.</li><li>When a word in our sentence is not in the <code>vocab</code> dictionary, we convert that word to the <code>placeholder_index</code>.</li><li>We set this as 5000, representing an unrecognizable word. For example:<ul><li>Original sentence: <code>I like jifw8evjk</code></li><li>Converted into index format: <code>[100, 3923, 5000]</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convert words to indexes, and use the placeholder index 5000 for words that are not in the vocabulary table </span></span><br><span class="line">placeholder_index = <span class="number">5000</span></span><br><span class="line"><span class="comment"># Store the result of the entire dataset converted to index </span></span><br><span class="line">indexed_dataset = []</span><br><span class="line"><span class="comment"># Iterate over the entire dataset</span></span><br><span class="line"><span class="keyword">for</span> tweet <span class="keyword">in</span> train_dataset:</span><br><span class="line">    <span class="comment"># Build an empty list to store the results of the current sentence (e.g. I like apple -&gt; [100, 3923, 123])</span></span><br><span class="line">    indexed_words = []</span><br><span class="line">    <span class="comment"># Use spacy to split the sentence into words</span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> nlp(tweet): </span><br><span class="line">        <span class="comment"># filter out the punctuation and stop words and space</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token.is_punct <span class="keyword">and</span> <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> <span class="keyword">not</span> token.is_space: </span><br><span class="line">            word = token.text </span><br><span class="line">            <span class="comment"># If the word is in the top 5000 words in the vocab, convert it to its index</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> vocab:</span><br><span class="line">                indexed_words.append(vocab[word])</span><br><span class="line">            <span class="comment"># Otherwise, convert it to the index of the placeholder token</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                indexed_words.append(placeholder_index) </span><br><span class="line">    indexed_dataset.append(indexed_words)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the converted data </span></span><br><span class="line"><span class="built_in">print</span>(indexed_dataset)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[[2013, 3615, 269, 3616, 3617, 1426, 717, 86], [1069, 339, 2014, 2015, 44, 2016], ...] </span></span><br></pre></td></tr></table></figure><p>Base on the above, we can wrap the above code into a function, which will be convenient for us to convert the sentence into an index format later during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for sentence to sequence </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_sentence_sequence</span>(<span class="params">seq, to_ix</span>):</span><br><span class="line">    idx = []</span><br><span class="line">    <span class="comment"># use spacy to tokenize the sentence </span></span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> nlp(seq):</span><br><span class="line">        <span class="comment"># filter out the punctuation and stop words and space </span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> token.is_punct <span class="keyword">and</span> <span class="keyword">not</span> token.is_stop <span class="keyword">and</span> <span class="keyword">not</span> token.is_space:</span><br><span class="line">            word = token.text</span><br><span class="line">            <span class="comment"># if the token is in the top 5000 words in the vocab, add its index to the list</span></span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> to_ix:</span><br><span class="line">                idx.append(to_ix[word])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># else add the index of the placeholder token</span></span><br><span class="line">                idx.append(placeholder_index)</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idx, dtype=torch.long) <span class="comment"># list convert to tensor </span></span><br></pre></td></tr></table></figure><h2 id="將標籤轉換成-tensor">將標籤轉換成 tensor</h2><p>接下來，我們要處理標籤，標籤也需要轉換成向量，這樣 model 的 ouput 才可以與 正確解答 做比較：</p><ul><li>通常我們預期 model 的 output 會長這樣：<code>[0.1, 0.2, 0.3, 0.4]</code><ul><li>分別代表 <code>&#123;0: 'anger', 1: 'joy', 2: 'optimism', 3: 'sadness'&#125;</code>的機率</li></ul></li><li>當解答是 <code>anger</code> 時，我們希望 model 的 output 越接近 <code>[1, 0, 0, 0]</code> 越好<ul><li>也就是說，我們需要把 label 進行 one-hot-encoding 轉換成向量的形式，才可以進行比較</li><li>為了可以把「模型產生的結果」 <code>[0.1, 0.2, 0.3, 0.4]</code> 和 「正確解答」<code>[1,0,0,0]</code> 放入 loss function 中計算 loss</li><li>因此我們需要一個函式，把標籤轉換成向量的形式，這個函示就是 <code>one_hot_encode</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># val is the index of the label (e.g. 2); to_ix is the dictionary of the label (e.g. &#123;0:&#x27;angry&#x27;, 1:&#x27;happy&#x27;&#125;)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_encode</span>(<span class="params">val, to_ix</span>): </span><br><span class="line">    <span class="comment"># create an empty list to store the result</span></span><br><span class="line">    result = [] </span><br><span class="line">    <span class="comment"># iterate over the dictionary of the label</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> to_ix.items(): </span><br><span class="line">        <span class="comment"># if the index of the label is the same as the index of the dictionary, we found the correct label</span></span><br><span class="line">        <span class="keyword">if</span> val == k: </span><br><span class="line">            <span class="comment"># append 1 to the list</span></span><br><span class="line">            result.append(<span class="number">1</span>)  </span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># append 0 to the list if the index is not the same</span></span><br><span class="line">            result.append(<span class="number">0</span>)  </span><br><span class="line">    <span class="keyword">return</span> torch.tensor(result, dtype=torch.float32) <span class="comment"># convert list to tensor</span></span><br></pre></td></tr></table></figure><p>After the above, we can wrap the above code into a function, which will be convenient for us to convert the sentence into an index format later during training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Because the label is a number, we need to convert it to a vector  </span></span><br><span class="line">mapping = <span class="built_in">dict</span>(<span class="built_in">zip</span>(mapping_pd[<span class="number">0</span>], mapping_pd[<span class="number">1</span>])) <span class="comment"># Return  &#123;0:&#x27;angry&#x27;, 1:&#x27;happy&#x27;, 2:&#x27;optimism&#x27;, 3:&#x27;sadness&#x27;&#125; </span></span><br><span class="line"><span class="built_in">print</span>(mapping)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;ans=2; vector=<span class="subst">&#123;one_hot_encode(<span class="number">2</span>, tag_to_ix)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result: We successfully converted 2 into the vector <code>[0, 0, 1, 0]</code>!</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;anger&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;joy&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;optimism&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;sadness&#x27;</span>&#125;</span><br><span class="line">ans=<span class="number">2</span>; vector=tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure><h1 id="Task-4-Train-the-Model-and-Calculate-Accuracy">Task 4: Train the Model and Calculate Accuracy</h1><div class="note info flat"><ol start="4"><li><strong>Train the Model and Calculate Accuracy</strong>: Evaluate the accuracy on the test set. (Note: If the training takes too long, try to use only a fraction of the training data.)</li></ol></div><h2 id="Try-Your-Hand">Try Your Hand</h2><p>Before starting to train the model, <strong>we need to first understand what our model’s input and output look like</strong>. Let’s see what the model predicts before it’s trained!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># See what the scores are before training</span></span><br><span class="line"><span class="comment"># Here we don&#x27;t need to train, so the code is wrapped in torch.no_grad()</span></span><br><span class="line"><span class="comment"># Take the first sentence as an example </span></span><br><span class="line">sentence_idx = <span class="number">1</span></span><br><span class="line"><span class="comment"># Print out：My roommate: it&#x27;s okay that we can&#x27;t spell because we have autocorrect. #terrible #firstworldprobs </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;First Sentense = <span class="subst">&#123;train_dataset[sentence_idx]&#125;</span>&#x27;</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># Convert the first sentence into index format, and convert it to tensor </span></span><br><span class="line">    inputs = prepare_sentence_sequence(train_dataset[sentence_idx], word_to_ix)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Sentense to tensor = <span class="subst">&#123;inputs&#125;</span>&#x27;</span>) <span class="comment"># 印出：tensor([1070,  340, 2015, 2016,   45, 2017])</span></span><br><span class="line">    <span class="comment"># Then convert the answer to tensor </span></span><br><span class="line">    labels = one_hot_encode(train_label_pd[<span class="number">0</span>][sentence_idx], tag_to_ix)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Sentense of result to tensor = <span class="subst">&#123;labels&#125;</span>&#x27;</span>) <span class="comment"># 印出：tensor([1., 0., 0., 0.])</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Send the inputs to the model and get the model&#x27;s prediction </span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;tag_scores = <span class="subst">&#123;outputs&#125;</span>&#x27;</span>) <span class="comment"># Print：tensor([[-1.3280, -1.4272, -1.4998, -1.3026]])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take the maximum probability value and take out the index </span></span><br><span class="line">    _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;preds = <span class="subst">&#123;preds&#125;</span>&#x27;</span>) <span class="comment"># Print out： preds = tensor([3])  </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Take out the index of the maximum probability value </span></span><br><span class="line">    result_idx = torch.argmax(outputs).item()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;result = <span class="subst">&#123;result_idx&#125;</span>, ans = <span class="subst">&#123;train_label_pd[<span class="number">0</span>][sentence_idx]&#125;</span>&#x27;</span>) <span class="comment"># 印出：result = 3, ans = 0 </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate loss to see the difference between output and label. Here output[0] is because we found that output has one more layer </span></span><br><span class="line">    loss = loss_function(outputs[<span class="number">0</span>], labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss = <span class="subst">&#123;loss&#125;</span>&#x27;</span>) </span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">First Sentense = My roommate: it<span class="string">&#x27;s okay that we can&#x27;</span>t spell because we have autocorrect. <span class="comment">#terrible #firstworldprobs </span></span><br><span class="line">Sentense to tensor = tensor([<span class="number">1070</span>,  <span class="number">340</span>, <span class="number">2015</span>, <span class="number">2016</span>,   <span class="number">45</span>, <span class="number">2017</span>])</span><br><span class="line">Sentense of result to tensor = tensor([<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">tag_scores = tensor([[-<span class="number">1.3280</span>, -<span class="number">1.4272</span>, -<span class="number">1.4998</span>, -<span class="number">1.3026</span>]])</span><br><span class="line">loss = <span class="number">1.32795250415802</span></span><br><span class="line">preds = tensor([<span class="number">3</span>])</span><br><span class="line">result = <span class="number">3</span>, ans = <span class="number">0</span></span><br></pre></td></tr></table></figure><p>Looks like it’s running pretty smoothly, right?<br>Here we go!</p><h1 id="Task-4-Train-the-Model-and-Calculate-Accuracy-2">Task 4: Train the Model and Calculate Accuracy</h1><div class="note info flat"><ol start="4"><li><strong>Train the Model and Calculate Accuracy</strong>: Evaluate the accuracy on the test set. (Note: If the training takes too long, try to use only a fraction of the training data.)</li></ol></div><h2 id="Try-Your-Hand-2">Try Your Hand</h2><p>Before starting to train the model, <strong>we need to first understand what our model’s input and output look like</strong>. Let’s see what the model predicts before it’s trained!</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">29</span> </span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.2157</span> Acc: <span class="number">0.4642</span> Time elapsed: <span class="number">25</span> sec. </span><br><span class="line">test Loss: <span class="number">1.2095</span> Acc: <span class="number">0.4553</span> Time elapsed: <span class="number">32</span> sec. </span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.1019</span> Acc: <span class="number">0.5333</span> Time elapsed: <span class="number">58</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1816</span> Acc: <span class="number">0.4708</span> Time elapsed: <span class="number">65</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.0151</span> Acc: <span class="number">0.5812</span> Time elapsed: <span class="number">92</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1603</span> Acc: <span class="number">0.4898</span> Time elapsed: <span class="number">99</span> sec.</span><br><span class="line">...</span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">17</span>m <span class="number">5</span>s </span><br><span class="line">Best val Acc: <span class="number">0.599578</span> # </span><br></pre></td></tr></table></figure><div class="note info flat"><p>Does the above code look familiar? Yes, it does! If you have followed this article <a href="https://shannonhung.github.io/posts/flower102-transfer-learning.html">Flower102 Dataset - Training with Transfer Learning + Batch Normalization for CNN</a> it uses the same kind of training. , the same kind of training is used.<br>It is possible to observe both the training results and the testing results to see if there is any overfitting.<br>Even if there is overfitting, this method can still preserve the best model.</p></div><p>So let’s get started with the <code>train_model</code> function, and I’ll indicate where we need to change it by <code>!!!! </code> to indicate where we need to make changes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, criterion, optimizer, scheduler, num_epochs=<span class="number">1</span></span>):</span><br><span class="line">    <span class="comment"># The time when training starts</span></span><br><span class="line">    since = time.time()</span><br><span class="line">    <span class="comment"># Create a temporary folder to store the best model</span></span><br><span class="line">    <span class="keyword">with</span> TemporaryDirectory() <span class="keyword">as</span> tempdir:</span><br><span class="line">        <span class="comment"># The path where the best model is stored</span></span><br><span class="line">        best_model_params_path = os.path.join(tempdir, <span class="string">&#x27;best_model_params.pt&#x27;</span>)</span><br><span class="line">        <span class="comment"># Initially store the best model</span></span><br><span class="line">        torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line">        <span class="comment"># The current best accuracy, which will be updated if a better accuracy is found</span></span><br><span class="line">        best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Start training for n epochs</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs - <span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Each epoch has a training and validation phase</span></span><br><span class="line">            <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]:</span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                    model.train()</span><br><span class="line">                <span class="keyword">else</span>: </span><br><span class="line">                    model.<span class="built_in">eval</span>()</span><br><span class="line">                </span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line">                running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Iterate over data.</span></span><br><span class="line">                <span class="keyword">for</span> <span class="built_in">input</span>, label <span class="keyword">in</span> <span class="built_in">zip</span>(dataloaders[phase], resultloaders[phase]):</span><br><span class="line">                    <span class="comment"># ===== !!! Here !!! ====== </span></span><br><span class="line">                    <span class="comment"># Here we will use the functions created in Task 2+3 to convert sentences to indices and labels to vectors</span></span><br><span class="line">                    <span class="comment"># e.g., tensor([1070,  340, 2015, 2016,   45, 2017])</span></span><br><span class="line">                    inputs_vector = prepare_sentence_sequence(<span class="built_in">input</span>, word_to_ix) </span><br><span class="line">                    <span class="comment"># e.g., tensor([1., 0., 0., 0.])</span></span><br><span class="line">                    labels_vector = one_hot_encode(label, tag_to_ix) </span><br><span class="line">                    <span class="comment"># ===== !!! End !!! ====== </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># zero the parameter gradients </span></span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># forward</span></span><br><span class="line">                    <span class="comment"># track history if only in train</span></span><br><span class="line">                    <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                        <span class="comment"># Similar to the earlier test</span></span><br><span class="line">                        <span class="comment"># Get the predicted result tensor for each emotion</span></span><br><span class="line">                        outputs = model(inputs_vector) <span class="comment"># (e.g., tensor([[-1.3948, -1.4476, -1.3804, -1.3261]]))</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># ===== !!! Here !!! ====== </span></span><br><span class="line">                        <span class="comment"># Get the index of the maximum value</span></span><br><span class="line">                        pred = torch.argmax(outputs).item() <span class="comment"># (e.g., 2)</span></span><br><span class="line">                        <span class="comment"># For the outer layer, only need to calculate the loss between the inner layer [-1.3948, -1.4476, -1.3804, -1.3261] and [0, 0, 1, 0]</span></span><br><span class="line">                        loss = criterion(outputs[<span class="number">0</span>], labels_vector) <span class="comment">#</span></span><br><span class="line">                        <span class="comment"># ===== !!! End !!! ====== </span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                        <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                            loss.backward()</span><br><span class="line">                            optimizer.step()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># statistics</span></span><br><span class="line">                    running_loss += loss.item()</span><br><span class="line">                    <span class="keyword">if</span> pred == label:</span><br><span class="line">                        running_corrects += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                    scheduler.step()</span><br><span class="line">                <span class="comment"># Calculate the loss and accuracy for each epoch</span></span><br><span class="line">                epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">                epoch_acc = running_corrects / dataset_sizes[phase]</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;phase&#125;</span> Loss: <span class="subst">&#123;epoch_loss:<span class="number">.4</span>f&#125;</span> Acc: <span class="subst">&#123;epoch_acc:<span class="number">.4</span>f&#125;</span> Time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - since))&#125;</span> sec.&#x27;</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># If a better accuracy is found, save the model</span></span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;test&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                    best_acc = epoch_acc</span><br><span class="line">                    torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">        time_elapsed = time.time() - since</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Training complete in <span class="subst">&#123;time_elapsed // <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>m <span class="subst">&#123;time_elapsed % <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Best val Acc: <span class="subst">&#123;best_acc:4f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load best model weights then proceed to the next epoch</span></span><br><span class="line">        model.load_state_dict(torch.load(best_model_params_path))</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>You will find that there are not many places to change… at most:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">        <span class="comment"># the input and label conversion </span></span><br><span class="line">        inputs_vector = prepare_sentence_sequence(<span class="built_in">input</span>, word_to_ix) </span><br><span class="line">        labels_vector = one_hot_encode(label, tag_to_ix) </span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># Then get the prediction result tensor for each emotion </span></span><br><span class="line">        pred = torch.argmax(outputs).item()</span><br><span class="line">        <span class="comment"># Calculate the loss </span></span><br><span class="line">        loss = criterion(outputs[<span class="number">0</span>], labels_vector)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>Now we can start training the model!</p><h2 id="Training">Training</h2><p>Let’s first prepare the dataset for training:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before we do that, let&#x27;s prepare the dataset for the model to use </span></span><br><span class="line">dataloaders = &#123;<span class="string">&#x27;train&#x27;</span>: train_dataset, <span class="string">&#x27;test&#x27;</span>: test_dataset&#125;</span><br><span class="line">resultloaders = &#123;<span class="string">&#x27;train&#x27;</span>: train_label_pd[<span class="number">0</span>].tolist(), <span class="string">&#x27;test&#x27;</span>: test_label_pd[<span class="number">0</span>].tolist()&#125;</span><br><span class="line">dataset_sizes = &#123;x: <span class="built_in">len</span>(dataloaders[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]&#125;</span><br></pre></td></tr></table></figure><p>Firstly, let’s build the LSTM model!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the model </span></span><br><span class="line"><span class="comment"># vocab_size need to add 1 because if there are words in the sentence that are not in the vocab, use 5000 to replace them, so you need to add 1 </span></span><br><span class="line">model_LSTM = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, <span class="built_in">len</span>(word_to_ix)+<span class="number">1</span>, <span class="built_in">len</span>(tag_to_ix), dropout=<span class="number">0.5</span>)</span><br><span class="line">loss_function_LSTM = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_LSTM = optim.SGD(model_LSTM.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">exp_lr_scheduler_LSTM = lr_scheduler.StepLR(optimizer_LSTM, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training </span></span><br><span class="line">modelLSTM = train_model(model_LSTM, loss_function_LSTM, optimizer_LSTM, exp_lr_scheduler_LSTM, num_epochs=<span class="number">30</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">2</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">0.9885</span> Acc: <span class="number">0.5840</span> Time elapsed: <span class="number">97</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1279</span> Acc: <span class="number">0.5236</span> Time elapsed: <span class="number">104</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">0.8893</span> Acc: <span class="number">0.6371</span> Time elapsed: <span class="number">132</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1053</span> Acc: <span class="number">0.5369</span> Time elapsed: <span class="number">139</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">0.7683</span> Acc: <span class="number">0.7003</span> Time elapsed: <span class="number">168</span> sec.</span><br><span class="line">test Loss: <span class="number">1.0772</span> Acc: <span class="number">0.5658</span> Time elapsed: <span class="number">175</span> sec.</span><br><span class="line">...</span><br><span class="line">test Loss: <span class="number">1.1330</span> Acc: <span class="number">0.6059</span> Time elapsed: <span class="number">1040</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">17</span>m <span class="number">20</span>s</span><br><span class="line">Best val Acc: <span class="number">0.610134</span></span><br></pre></td></tr></table></figure><p>Then let’s build the GRU model!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vocab_size need to add 1 because if there are words in the sentence that are not in the vocab, use 5000 to replace them, so you need to add 1 </span></span><br><span class="line">modelGRU = GRUTagger(EMBEDDING_DIM, HIDDEN_DIM, <span class="built_in">len</span>(word_to_ix)+<span class="number">1</span>, <span class="built_in">len</span>(tag_to_ix), dropout=<span class="number">0.5</span>)</span><br><span class="line">loss_function_gru = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_gru = optim.SGD(modelGRU.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">exp_lr_scheduler_gru = lr_scheduler.StepLR(optimizer_gru, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training </span></span><br><span class="line">modelGRU = train_model(modelGRU, loss_function_gru, optimizer_gru, exp_lr_scheduler_gru, num_epochs=<span class="number">30</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">3</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">0.8445</span> Acc: <span class="number">0.6702</span> Time elapsed: <span class="number">131</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1211</span> Acc: <span class="number">0.5327</span> Time elapsed: <span class="number">138</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">29</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">0.6843</span> Acc: <span class="number">0.7393</span> Time elapsed: <span class="number">166</span> sec.</span><br><span class="line">test Loss: <span class="number">1.1305</span> Acc: <span class="number">0.5707</span> Time elapsed: <span class="number">173</span> sec.</span><br><span class="line">...</span><br><span class="line">test Loss: <span class="number">1.3237</span> Acc: <span class="number">0.6073</span> Time elapsed: <span class="number">1003</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">16</span>m <span class="number">43</span>s</span><br><span class="line">Best val Acc: <span class="number">0.608726</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Mechine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mechine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection</title>
      <link href="/en/posts/coco-object-diagnoise/"/>
      <url>/en/posts/coco-object-diagnoise/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction">Introduction</h1><p>Recently I took an AI course, the main content is the following topics:</p><ol><li>Learn about Coco dataset</li><li>User pre-trained version of Faster R-CNN to predict the bounding box</li><li>Calculate IoU</li></ol><h1 id="Homework-Requirement">Homework Requirement</h1><ol><li><strong>Download the Coco Collection</strong>*: download the files “2017 Val images [5/1GB]” and “2017 Train/Val annotations [241MB]” from the Coco page.<br>Download from Coco page. You can load them into your notebook using the pycocotools library.</li><li><strong>Randomly select ten from the dataset</strong>: 10 images are randomly selected from this dataset.</li><li><strong>Predict the box using the pre-trained model FasterR-CNN</strong>: use a pre-trained version of the Faster R-CNN (Resnet50 backbone) to predict the bounding box of the object on the 10 images. of the bounding box. Only regions with scores greater than 0.8 are retained.</li><li><strong>isualize the model together with the answer visualization</strong>*: Visualize the predicted bounding boxes and label together with the ground truth bounding<br>boxes and label. Show all 10 pairs of images side by side in the jupyter notebook.</li><li><strong>Use another pre-trained model Mobilnet</strong>: Repeat the above steps using the Mobilenet backbone of the Faster R-CNN.</li><li><strong>Calculate IoU Compare Models</strong>: Which backbone provides better results? Calculate the IoU for both methods.</li></ol><h1 id="Task-1-Downloading-the-COCO-Dataset">Task 1: Downloading the COCO Dataset</h1><div class="note info flat"><p><strong>Task 1</strong></p><ol><li><strong>Download the COCO Dataset</strong>: Obtain the files “2017 Val images [5/1GB]” and “2017 Train/Val annotations [241MB]” from the Coco page. Utilize the pycocotools library to import them into your notebook.</li></ol></div><p>You can follow this guide to proceed with the download: <a href="https://jason-chen-1992.weebly.com/home/coco-dataset">Download COCO Dataset</a><br><img src="https://i.imgur.com/BieHtLG.png" alt=""></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── annotations <span class="comment"># These are annotation files</span></span><br><span class="line">│   ├── captions_train2017.json</span><br><span class="line">│   ├── captions_val2017.json</span><br><span class="line">│   ├── instances_train2017.json</span><br><span class="line">│   ├── instances_val2017.json</span><br><span class="line">│   ├── person_keypoints_train2017.json</span><br><span class="line">│   └── person_keypoints_val2017.json</span><br><span class="line">└── val2017 <span class="comment"># This is the image set </span></span><br><span class="line">    ├── 000000000139.jpg</span><br><span class="line">    ├── 000000000285.jpg</span><br><span class="line">    ├── 000000000632.jpg</span><br><span class="line">    ├── 000000000724.jpg</span><br><span class="line">    ├── 000000000776.jpg</span><br><span class="line">    ├── 000000000785.jpg</span><br><span class="line">    ├── 000000000802.jpg</span><br><span class="line">    ... </span><br></pre></td></tr></table></figure><ul><li>Download these two files as shown in the image.</li><li>After downloading, the folder structure upon extraction will resemble the one above.</li></ul><h1 id="Task-2-Randomly-Select-Ten-Images">Task 2: Randomly Select Ten Images</h1><div class="note info flat"><p><strong>Task 2</strong><br>2. <strong>Randomly Select Ten Images from the Dataset</strong>: Pick 10 images randomly from this dataset.</p></div><p>Here, we’ll primarily do a few things:</p><ul><li>Import necessary libraries.</li><li>Set up the COCO API to allow it to access relevant information from our dataset, such as bounding box positions, label locations, and image information.</li><li>Visualize images and perform annotations.</li><li>Randomly select ten images.</li></ul><p>Let’s begin by importing the necessary libraries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># CNN </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"></span><br><span class="line"><span class="comment"># others</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># torchvision</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset </span></span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><h2 id="Setting-up-the-COCO-API">Setting up the COCO API</h2><p>COCO provides an API to access datasets. By providing it with a JSON file, we can easily retrieve the necessary information such as images, labels, bounding boxes, and more.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify dataset location</span></span><br><span class="line">cocoRoot = <span class="string">&quot;../../Data/Coco/&quot;</span></span><br><span class="line">dataType = <span class="string">&quot;val2017&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set annotation file location</span></span><br><span class="line">annFile = os.path.join(cocoRoot, <span class="string">f&#x27;annotations/instances_<span class="subst">&#123;dataType&#125;</span>.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Annotation file: <span class="subst">&#123;annFile&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># # initialize COCO api for instance annotations</span></span><br><span class="line">coco=COCO(annFile)</span><br><span class="line">coco </span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Annotation file: ../../Data/Coco/annotations/instances_val2017.json</span><br><span class="line"><span class="comment">-- Indicates successful annotation file read</span></span><br><span class="line">loading annotations into memory...</span><br><span class="line">Done (t=<span class="number">0.35</span>s)</span><br><span class="line">creating index...</span><br><span class="line">index created!</span><br></pre></td></tr></table></figure><h2 id="Annotation-Visualization">Annotation Visualization</h2><p>To ensure familiarity with the COCO-provided API, here’s an exercise focusing on the following:</p><ul><li>Obtaining image info by ID</li><li>Retrieving annotation info by ID</li><li>Learning to draw bounding boxes and labels on images</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Rectangle</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a function that, given an image ID, plots the image with bounding boxes and labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_image_with_annotations</span>(<span class="params">coco, cocoRoot, dataType, imgId, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Get image information</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgId)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Get image location for visualization</span></span><br><span class="line">    imPath = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])    </span><br><span class="line">    <span class="comment"># Read the image</span></span><br><span class="line">    im = cv2.imread(imPath)</span><br><span class="line">    <span class="comment"># Convert color space: OpenCV defaults to BGR, but matplotlib to RGB, so conversion is needed</span></span><br><span class="line">    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find all annotations (bounding boxes) for the image</span></span><br><span class="line">    annIds = coco.getAnnIds(imgIds=imgInfo[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">    <span class="comment"># Load all annotation information: bounding box coordinates, labels, accuracies</span></span><br><span class="line">    anns = coco.loadAnns(annIds)</span><br><span class="line">    all_labels = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Extract bounding box coordinates, labels, accuracies</span></span><br><span class="line">    <span class="keyword">for</span> ann <span class="keyword">in</span> anns:</span><br><span class="line">        <span class="comment"># Specifically select information related to the bounding box: returns (x, y) of the lower-left corner, width, height</span></span><br><span class="line">        x, y, w, h = ann[<span class="string">&#x27;bbox&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get label text information: load category name by category ID</span></span><br><span class="line">        label = coco.loadCats(ann[<span class="string">&#x27;category_id&#x27;</span>])[<span class="number">0</span>][<span class="string">&quot;name&quot;</span>]</span><br><span class="line">        all_labels.add(label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw bounding boxes using provided coordinates</span></span><br><span class="line">        rect = Rectangle((x, y), w, h, linewidth=<span class="number">2</span>, edgecolor=<span class="string">&#x27;r&#x27;</span>, facecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Draw the image: if sorting of images is needed, ax parameter specifies the position</span></span><br><span class="line">        <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            plt.gca().add_patch(rect) </span><br><span class="line">            plt.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ax.add_patch(rect)</span><br><span class="line">            ax.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the image with a title</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        plt.imshow(im)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">f&#x27;Annotations: <span class="subst">&#123;all_labels&#125;</span>&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_title(<span class="string">f&#x27;Annotations: <span class="subst">&#123;all_labels&#125;</span>&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>, loc=<span class="string">&#x27;center&#x27;</span>, pad=<span class="number">20</span>)</span><br><span class="line">        ax.imshow(im)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the tenth image</span></span><br><span class="line">imgIds = coco.getImgIds()</span><br><span class="line">imgId = imgIds[<span class="number">10</span>]</span><br><span class="line"><span class="comment"># Plot the image with annotations</span></span><br><span class="line">plot_image_with_annotations(coco, cocoRoot, dataType, imgId)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><p><img src="https://i.imgur.com/C0nZWY9.png" alt=""></p><h2 id="Randomly-Select-Ten-Images">Randomly Select Ten Images</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_select</span>(<span class="params">coco, cocoRoot, dataType, num_images=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># Get IDs for all images</span></span><br><span class="line">    imgIds = coco.getImgIds()</span><br><span class="line">    <span class="comment"># Randomly select num_images IDs from this set</span></span><br><span class="line">    selected_imgIds = random.sample(imgIds, num_images)</span><br><span class="line">    <span class="comment"># Call the plot_image_with_annotations function for each selected ID</span></span><br><span class="line">    <span class="keyword">for</span> imgId <span class="keyword">in</span> selected_imgIds:</span><br><span class="line">        <span class="comment"># Plot images based on their IDs</span></span><br><span class="line">        plot_image_with_annotations(coco, cocoRoot, dataType, imgId)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Print out all selected IDs</span></span><br><span class="line">    <span class="keyword">return</span> selected_imgIds</span><br><span class="line">    </span><br><span class="line">valid_ids = random_select(coco, cocoRoot, dataType, num_images=<span class="number">10</span>)</span><br><span class="line">valid_ids</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><p><img src="https://i.imgur.com/TUiI82h.png" alt=""></p><h1 id="Task-3-5-FasterR-CNN-v-s-Mobilnet">Task 3+5: FasterR-CNN v.s Mobilnet</h1><div class="note info flat"><p><strong>Task 3 &amp; 5</strong><br>3. <strong>Predicting bboxes using the pre-trained model FasterR-CNN</strong>：Use a pre-trained version of Faster R-CNN (Resnet50 backbone) to predict the bounding box<br>of objects on the 10 images. Only keep regions that have a score &gt; 0.8.<br>5. <strong>Using another pre-trained model Mobilnet</strong>：Repeat the steps from above using a Mobilenet backbone for the Faster R-CNN.</p></div><h2 id="Using-pre-train-model">Using pre-train model</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using pre-train model (FasterR-CNN)</span></span><br><span class="line">model_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=<span class="string">&quot;FasterRCNN_ResNet50_FPN_Weights.DEFAULT&quot;</span>)</span><br><span class="line">model_res.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># using pre-train model  (Mobilenet)</span></span><br><span class="line">model_mobile = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights)</span><br><span class="line">model_mobile.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h2 id="Convert-image-to-tensor">Convert image to tensor</h2><p>We need to be able to take the image out of the picture based on the position of the image. Then we need to convert the image read from the book into a tensor before we can put it into the model for prediction. So we made two functions:</p><ul><li>One is to read the picture</li><li>One is to convert the picture to a tensor.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">imgIdx</span>):</span><br><span class="line">    <span class="comment"># Get image information</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgIdx)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Get image location for visualization</span></span><br><span class="line">    imPath = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])    </span><br><span class="line">    <span class="comment"># Read the image path </span></span><br><span class="line">    <span class="built_in">print</span>(imPath)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># Read the image </span></span><br><span class="line">        <span class="keyword">return</span> Image.<span class="built_in">open</span>(imPath)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the picture to a tensor </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pil2tensor</span>(<span class="params">pil_image</span>):</span><br><span class="line">    <span class="comment"># Use unsqueeze(0) because the model still contains the batch size dimension, a total of four dimensions (batch_size, channel-RGB, height, width)</span></span><br><span class="line">    <span class="comment"># But the picture has only one picture without batch size, the picture is converted to tensor will only have three dimensions (channel-RGB, height, width), so we need to add a dimensions </span></span><br><span class="line">    <span class="comment"># /255 is because the input of the model is a number between 0 and 1, and the value of the picture is 0~255, so it needs to be divided by 255 for normalization </span></span><br><span class="line">    <span class="keyword">return</span> torchvision.transforms.PILToTensor()(pil_image).unsqueeze(<span class="number">0</span>) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><h2 id="Training-the-model">Training the model</h2><p>After the pre-training model is loaded, we need to train the model. The training process is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the prediction result </span></span><br><span class="line">predictions_res = []</span><br><span class="line">predictions_mobile = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Recursively call each id, these ids are the 10 ids we randomly selected above</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> valid_ids:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">    <span class="comment"># transform to tensor from PIL image</span></span><br><span class="line">    img_as_tensor = pil2tensor(load_image(i))</span><br><span class="line">    <span class="comment"># put the tensor to resnet model</span></span><br><span class="line">    prediction = model_res(img_as_tensor)</span><br><span class="line">    <span class="comment"># Save the prediction result: the prediction result is a dictionary, which contains the predicted bounding box, label, and accuracy </span></span><br><span class="line">    predictions_res.append(prediction)</span><br><span class="line">    <span class="comment"># put the tensor to mobilenet model</span></span><br><span class="line">    prediction = model_mobile(img_as_tensor)</span><br><span class="line">    <span class="comment"># Save the prediction result: the prediction result is a dictionary, which contains the predicted bounding box, label, and accuracy</span></span><br><span class="line">    predictions_mobile.append(prediction)</span><br></pre></td></tr></table></figure><h2 id="Only-select-the-prediction-results-0-8">Only select the prediction results &gt; 0.8</h2><p>After the model is trained, we need to select the prediction results that are greater than 0.8. The reason is that the model will predict a lot of bounding boxes, but we only need the bounding boxes with high accuracy. So we need to filter out the bounding boxes with low accuracy. The code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">filter_valid_boxes</span>(<span class="params">predictions, threshold=<span class="number">0.8</span></span>):</span><br><span class="line">    <span class="comment"># Used to store the filtered prediction results </span></span><br><span class="line">    valid_boxes_list = []</span><br><span class="line">    <span class="comment"># Recursively call each prediction result </span></span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        valid_boxes_for_this_prediction = []</span><br><span class="line">        <span class="comment"># Recursively call each bounding box </span></span><br><span class="line">        <span class="keyword">for</span> box, label, score <span class="keyword">in</span> <span class="built_in">zip</span>(prediction[<span class="number">0</span>][<span class="string">&quot;boxes&quot;</span>], prediction[<span class="number">0</span>][<span class="string">&quot;labels&quot;</span>], prediction[<span class="number">0</span>][<span class="string">&quot;scores&quot;</span>]):</span><br><span class="line">            <span class="comment"># Only keep the predicted bounding box with accuracy greater than threshold </span></span><br><span class="line">            <span class="keyword">if</span> score &gt;= threshold: </span><br><span class="line">                <span class="comment"># Save the predicted bounding box, label, and accuracy </span></span><br><span class="line">                valid_boxes_for_this_prediction.append((box, label, score))</span><br><span class="line">        <span class="comment"># If none of the predicted boxes in this picture have an accuracy greater than threshold, store an empty list </span></span><br><span class="line">        valid_boxes_list.append(valid_boxes_for_this_prediction)</span><br><span class="line">    <span class="comment"># Return the filtered prediction result </span></span><br><span class="line">    <span class="keyword">return</span> valid_boxes_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set threshold to 0.8 and get the prediction results of resnet and mobilenet </span></span><br><span class="line">valid_boxes_res = filter_valid_boxes(predictions_res, threshold=<span class="number">0.8</span>)</span><br><span class="line">valid_boxes_mobile = filter_valid_boxes(predictions_mobile, threshold=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-4-6-Visualization-IoU">Task 4+6: Visualization + IoU</h1><div class="note info flat"><p><strong>Tasks 4 &amp; 6</strong></p><ol><li><strong>Visualize the model together with the solution</strong>: Visualize the predicted bounding boxes and label together with the ground truth bounding</li><li><strong>CalculateIoU to compare models</strong>: Which backbone delivers the better results? Calculate the IoU for both approaches.</li></ol></div><p>There are a few important points in visual dialog, the steps are as follows:</p><ul><li>We need to know the id of the image first, and get the annotation information based on the id, then we can Calculate the IoU.</li><li>We take the annotation information and the model information to conduct the IoU Calculate.</li><li>We read the location of the image in the computer, and according to the path of the image, we draw the image through plt first.</li><li>We read the location of the picture in the computer, and based on the path of the picture, we draw the picture through plt first, and then based on the picture, we can draw the prediction box and label on it, as well as the average value of the IoU.</li></ul><p>The following program is the procedure described above, we will draw the results of both models and calculate the average of the IoU.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Can put the results of different models into this function, and it will return the average value of IoU </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_annotated_results</span>(<span class="params">imgId, valid_boxes, model_name, color=<span class="string">&#x27;g&#x27;</span>, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Load the image</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgId)[<span class="number">0</span>]</span><br><span class="line">    image_path = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])</span><br><span class="line">    image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the correct bounding box results </span></span><br><span class="line">    annIds = coco.getAnnIds(imgIds=imgInfo[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">    anns = coco.loadAnns(annIds)</span><br><span class="line">    bbox_tlist_anns = torch.tensor([ann[<span class="string">&quot;bbox&quot;</span>] <span class="keyword">for</span> ann <span class="keyword">in</span> anns]) <span class="comment"># tensor.shape[2,4]</span></span><br><span class="line">    <span class="comment"># because our bounding box is x,y,w,h which is the coordinate of the lower left corner of the box (x,y) + the length and width of the box </span></span><br><span class="line">    <span class="comment"># But torchvision Calculate the box_iou must give the coordinates of the lower left corner (x,y) and the coordinates of the upper right corner (x2,y2), so we need to Calculate (x2,y2) through (x+w, y+h) to get the coordinates of the upper right corner </span></span><br><span class="line">    <span class="comment"># x,y,w,h -&gt; x1,y1,x2,y2 = x,y,x+w,y+h </span></span><br><span class="line">    bbox_tlist_anns[:, <span class="number">2</span>] = bbox_tlist_anns[:, <span class="number">0</span>] + bbox_tlist_anns[:, <span class="number">2</span>]</span><br><span class="line">    bbox_tlist_anns[:, <span class="number">3</span>] = bbox_tlist_anns[:, <span class="number">1</span>] + bbox_tlist_anns[:, <span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From resultsvalid_boxes, we only need the box part, so we use (box, _, _)  </span></span><br><span class="line">    <span class="comment"># Use stack because we want to stack all the boxes together to become a tensor </span></span><br><span class="line">    bbox_tlist_model = torch.stack([box <span class="keyword">for</span> box, _, _ <span class="keyword">in</span> valid_boxes]) <span class="comment"># turn [4] to tensor.shape[2,4]</span></span><br><span class="line">    <span class="comment"># use box_iou 來Calculate IoU </span></span><br><span class="line">    iou = torchvision.ops.box_iou(bbox_tlist_anns, bbox_tlist_model) <span class="comment"># get IoU </span></span><br><span class="line">    <span class="comment"># Get the maximum value of each predicted box in ann, and then Calculate the average value of IoU </span></span><br><span class="line">    avg_iou = np.mean([t.cpu().detach().numpy().<span class="built_in">max</span>() <span class="keyword">for</span> t <span class="keyword">in</span> iou]) <span class="comment"># calculate the mean of IoU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># display image label </span></span><br><span class="line">    all_labels = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Start drawing the prediction box </span></span><br><span class="line">    <span class="keyword">for</span> boxes <span class="keyword">in</span> valid_boxes:</span><br><span class="line">        <span class="comment"># Get the information of the prediction box, including box, label, and accuracy </span></span><br><span class="line">        box, label, score = boxes</span><br><span class="line">        <span class="comment"># Get the text information of the label: load category name by category ID</span></span><br><span class="line">        label = coco.loadCats(label.item())[<span class="number">0</span>][<span class="string">&quot;name&quot;</span>]</span><br><span class="line">        <span class="comment"># Save the label for later display </span></span><br><span class="line">        all_labels.add(label)</span><br><span class="line">        <span class="comment"># Because the results returned by the model are two coordinates, the lower left corner and the upper right corner, so we need to convert them into x,y,w,h form and put them into Rectangle </span></span><br><span class="line">        x, y, x2, y2 = box.detach().numpy() <span class="comment"># x,y,w,h -&gt; x,y,x2-x,y2-y</span></span><br><span class="line">        rect = Rectangle((x, y), x2 - x, y2 - y, linewidth=<span class="number">2</span>, edgecolor=color, facecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw the picture: if you need to sort the picture, you can specify where to draw the picture through ax </span></span><br><span class="line">        <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># gca can get the current axes, if not, it will automatically create one, and then draw the prediction box through add_patch </span></span><br><span class="line">            plt.gca().add_patch(rect) </span><br><span class="line">            <span class="comment"># Draw the label on the prediction box </span></span><br><span class="line">            plt.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=color)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># add_patch can add a patch to the current axes, and then draw the prediction box on the ax </span></span><br><span class="line">            ax.add_patch(rect)</span><br><span class="line">            <span class="comment"># Draw the label on the prediction box </span></span><br><span class="line">            ax.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=color)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># display image and give it a title, the title is the label that appears in this picture, and the average value of IoU </span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>: <span class="subst">&#123;all_labels&#125;</span> \n IoU: <span class="subst">&#123;avg_iou:<span class="number">.4</span>f&#125;</span>&#x27;</span>, color=color)</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_title(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>: <span class="subst">&#123;all_labels&#125;</span> \n I0U: <span class="subst">&#123;avg_iou:<span class="number">.4</span>f&#125;</span>&#x27;</span>, color=color)</span><br><span class="line">        ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg_iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res_iou = []</span><br><span class="line">mobile_iou = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Recursively call each id, where id is one of the 10 random ids we selected above</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(valid_ids)):</span><br><span class="line">    <span class="comment"># Create a 1x3 grid of images, each image sized 15x5</span></span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Draw the truth image and display it in the center</span></span><br><span class="line">    plot_image_with_annotations(coco, cocoRoot, dataType, valid_ids[i], ax=axs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Draw the predicted results from two different models on the left and right sides respectively, and return the IoU</span></span><br><span class="line">    i_mobil_iou = display_annotated_results(valid_ids[i], valid_boxes_mobile[i], <span class="string">&quot;mobile&quot;</span>, color=<span class="string">&#x27;g&#x27;</span>, ax=axs[<span class="number">0</span>])</span><br><span class="line">    i_res_iou = display_annotated_results(valid_ids[i], valid_boxes_res[i], <span class="string">&quot;ResNet&quot;</span>, color=<span class="string">&#x27;b&#x27;</span>, ax=axs[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the IoU of each image to assess the overall performance of the model</span></span><br><span class="line">    mobile_iou.append(i_mobil_iou)</span><br><span class="line">    res_iou.append(i_res_iou)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Organize the layout</span></span><br><span class="line">    plt.tight_layout()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the mean of the IoU list</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ResNet: Avg.&quot;</span>, np.mean(res_iou), <span class="string">&quot;; each IoU:&quot;</span>, res_iou)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MobileNet: Avg.&quot;</span>, np.mean(mobile_iou), <span class="string">&quot;; each IoU:&quot;</span>, mobile_iou)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><p><img src="https://i.imgur.com/LjCVWdY.png" alt=""></p><h1 id="Supplement-IoU">Supplement: IoU</h1><ul><li>Ref: <a href="https://blog.csdn.net/IAMoldpan/article/details/78799857">https://blog.csdn.net/IAMoldpan/article/details/78799857</a></li><li>Ref: <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></li></ul><div class="note info flat"><p>IoU (Intersection over Union) is a metric used to evaluate object detection algorithms. It is defined as the <code>intersection area</code> of the <code>predicted box</code> and the <code>true box</code> divided by their <code>union area</code>. The value ranges between 0 and 1, where a higher value indicates a greater overlap between the predicted and true boxes, signifying more accurate predictions.</p></div><p><img src="https://i.imgur.com/VzMudvr.png" alt=""><br><img src="https://i.imgur.com/OKroIoL.png" alt=""><br>Source: <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></p><div class="note info flat"><p><strong>From the example above, you might be wondering, what exactly does the following code segment do?</strong></p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.ops.box_iou(bbox_tlist_anns, bbox_tlist_model) </span><br></pre></td></tr></table></figure><ul><li>Essentially, it calculates the Intersection over Union (IoU) between all predicted bounding boxes of the ground truth and all predicted bounding boxes of the model. This process returns a tensor with the shape (number of ground truth bounding boxes, number of model’s predicted bounding boxes). Refer to the images below.<br><img src="https://i.imgur.com/kQ6IVMY.png" alt=""><br><img src="https://i.imgur.com/lnjmgeD.png" alt=""></li></ul><p><strong>Here, we only need to obtain the maximum IoU for each ground truth bounding box and calculate the average value, so we use the following code</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After obtaining the maximum value for each predicted box of the annotation (see supplementary IoU for details), calculate the average IoU</span></span><br><span class="line">avg_iou = np.mean([t.cpu().detach().numpy().<span class="built_in">max</span>() <span class="keyword">for</span> t <span class="keyword">in</span> iou]) <span class="comment"># calculate the mean of IoU</span></span><br></pre></td></tr></table></figure><div class="note warning flat"><p>You might be curious whether using functions like <code>max()</code>, <code>mean()</code>, <code>sum()</code> will affect our results?</p></div><p><img src="https://i.imgur.com/lnQtu1r.png" alt=""><br><strong>As we can see from the above image</strong></p><ul><li>Using <code>sum()</code>, you may find that the value can exceed 1, which is not a reasonable range for IoU values.</li><li>Using <code>max()</code>, it chooses, for each ground truth bounding box, the closest predicted bounding box from the model as its IoU. Then, we can obtain the maximum IoU values for <code>all predicted bounding boxes</code> of the ground truth and calculate the average to determine the overall IoU.</li><li>Using <code>mean()</code> poses a problem as the IoU calculation will never be 1. This is because it considers the IoUs of other bounding boxes, which lowers the overall IoU. For instance, if the ground truth has two bounding boxes <code>[A1,A2]</code>, and the model also predicts two <code>[B1,B2]</code>, it’s clear that B1 predicts A1, and B2 predicts A2, and the model predicts accurately. However, using mean will incorrectly consider B1 as A2 and B2 as A1, which is wrong, and these pairs have low IoUs. Thus, using <code>mean()</code> in this way will unjustly lower the IoU, making it unreasonable.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Mechine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mechine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN</title>
      <link href="/en/posts/flower102-transfer-learning/"/>
      <url>/en/posts/flower102-transfer-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Preface">Preface</h1><p>I recently took an ai course, this is the fourth assignment and the main topics taught are the following.</p><ol><li>selecting a dataset and training a model on it.</li><li>migration learning - fine tuning.</li><li>batch normalization in CNN.</li></ol><p>The main references are the following websites: 1.</p><ol><li><a href="https://pytorch.org/vision/stable/generated/torchvision.datasets.Flowers102.html#torchvision.datasets.Flowers102">Flower102 dataset</a></li><li><a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Migration Learning</a></li><li><a href="https://pytorch.org/vision/stable/datasets.html">Pytorch dataset</a></li><li><a href="https://pytorch.org/vision/stable/models.html">Migration Learning Model</a></li><li><a href="/posts/ML.html#Transfer-Learning">Shannon’s Transfer Learning Blog</a></li><li><a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18">Resnet18</a></li></ol><h1 id="Assignment-Requirements">Assignment Requirements</h1><p>Tasks</p><ol><li><strong>Choose a dataset</strong>*: Look at torchvision <a href="https://pytorch.org/vision/stable/datasets.html">Pytorch’s dataset</a> and decide which dataset you want to use (excluding<br>CIFAR, ImageNet, FashionMNIST).</li><li><strong>Print images and profile sizes</strong>: show some sample images of the dataset in your notebook and print the size of the dataset.</li><li><strong>Construct a CNN using batch normalization</strong>: design a CNN to make predictions on the dataset. Use a similar architecture as last time, but this time<br>also includes a batch normalization layer.</li><li><strong>Train a model using a dataset and print out the accuracy of the test</strong>: train a model on a dataset and measure the accuracy on retained test data.</li><li><strong>Use ResNet18 for Migration Learning</strong>: now use migration learning to use a pre-trained ResNet18 on the dataset as follows:</li><li><strong>Without changing the trained weights of other people’s models</strong>: ResNet18 is used as a fixed feature extractor.</li><li><strong>Fine-tuning using RestNet</strong> : ResNet18 is fine-tuned on the training data.</li><li><strong>Fine-tuning using EfficientNet_B5</strong>: Repeat step 4 but now use EfficientNet_B5 instead of RestNet18.<br><strong>Compare these different methods and print out the accuracy</strong>: Compare the accuracy of the different methods on the test data and print out the training time for each method.<br>Training time for each method.</li></ol><h1 id="Task-0-Importing-Packages">Task 0 - Importing Packages</h1><p>Let’s start by importing the required package: # Task 0 - import package</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CNN </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"></span><br><span class="line"><span class="comment"># others</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset </span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> Flowers102</span><br><span class="line"></span><br><span class="line"><span class="comment"># label </span></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure><h1 id="Task-1-Selecting-a-DataSet">Task 1 - Selecting a DataSet</h1><ul><li>Ref: <a href="https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/">Why [0.485, 0.456, 0.406] for Normalization</a></li></ul><div class="note info flat"><p><strong>Select a DataSet</strong>: Check out the torchvision <a href="https://pytorch.org/vision/stable/datasets.html">DataSet of Pytorch</a> and decide one dataset that you want to use (no<br>CIFAR, no ImageNet, no FashionMNIST).</p></div><p>In order to experience Transfer Learning and train it quickly, we use flower102 here. We use flower102 as our dataset. Since flower102 doesn’t provide Chinese labels, most of my searching on the web is done by reading the <code>.json</code> or <code>.txt</code> files that are already written, which describes each label index in Chinese.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify the data you want to download, the path and btach size, and the amount of training to do at once.</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">data_dir = <span class="string">&#x27;... /... /Data/flowers-102&#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build the classes_name of the dataset</span></span><br><span class="line">json_data = <span class="string">&#x27;&#123;&quot;21&quot;: &quot;fire lily&quot;, &quot;3&quot;: &quot;canterbury bells&quot;, &quot;45&quot;: &quot;bolero deep blue&quot;, &quot;1&quot;: &quot;pink primrose&quot;, &quot;34&quot;: &quot;mexican aster&quot;, &quot;27&quot;: &quot;prince of wales feathers&quot;, &quot;7&quot;: &quot;moon orchid&quot;, &quot;16&quot;: &quot;globe-flower&quot;, &quot;25&quot;: &quot;grape hyacinth&quot;, &quot;26&quot;: &quot;corn poppy&quot;, &quot;79&quot;: &quot;toad lily&quot;, &quot;39&quot;: &quot;siam tulip&quot;, &quot;24&quot;: &quot;red ginger&quot;, &quot;67&quot;: &quot;spring crocus&quot;, &quot;35&quot;: &quot;alpine sea holly&quot;, &quot;32&quot;: &quot;garden phlox&quot;, &quot;10&quot;: &quot;globe thistle&quot;, &quot;6&quot;: &quot;tiger lily&quot;, &quot;93&quot;: &quot;ball moss&quot;, &quot;33&quot;: &quot;love in the mist&quot;, &quot;9&quot;: &quot;monkshood&quot;, &quot;102&quot;: &quot;blackberry lily&quot;, &quot;14&quot;: &quot;spear thistle&quot;, &quot;19&quot;: &quot;balloon flower&quot;, &quot;100&quot;: &quot;blanket flower&quot;, &quot;13&quot;: &quot;king protea&quot;, &quot;49&quot;: &quot;oxeye daisy&quot;, &quot;15&quot;: &quot;yellow iris&quot;, &quot;61&quot;: &quot;cautleya spicata&quot;, &quot;31&quot;: &quot;carnation&quot;, &quot;64&quot;: &quot;silverbush&quot;, &quot;68&quot;: &quot;bearded iris&quot;, &quot;63&quot;: &quot;black-eyed susan&quot;, &quot;69&quot;: &quot;windflower&quot;, &quot;62&quot;: &quot;japanese anemone&quot;, &quot;20&quot;: &quot;giant white arum lily&quot;, &quot;38&quot;: &quot;great masterwort&quot;, &quot;4&quot;: &quot;sweet pea&quot;, &quot;86&quot;: &quot;tree mallow&quot;, &quot;101&quot;: &quot;trumpet creeper&quot;, &quot;42&quot;: &quot;daffodil&quot;, &quot;22&quot;: &quot;pincushion flower&quot;, &quot;2&quot;: &quot;hard-leaved pocket orchid&quot;, &quot;54&quot;: &quot;sunflower&quot;, &quot;66&quot;: &quot;osteospermum&quot;, &quot;70&quot;: &quot;tree poppy&quot;, &quot;85&quot;: &quot;desert-rose&quot;, &quot;99&quot;: &quot;bromelia&quot;, &quot;87&quot;: &quot;magnolia&quot;, &quot;5&quot;: &quot;english marigold&quot;, &quot;92&quot;: &quot;bee balm&quot;, &quot;28&quot;: &quot;stemless gentian&quot;, &quot;97&quot;: &quot;mallow&quot;, &quot;57&quot;: &quot;gaura&quot;, &quot;40&quot;: &quot;lenten rose&quot;, &quot;47&quot;: &quot;marigold&quot;, &quot;59&quot;: &quot;orange dahlia&quot;, &quot;48&quot;: &quot;buttercup&quot;, &quot;55&quot;: &quot;pelargonium&quot;, &quot;36&quot;: &quot;ruby-lipped cattleya&quot;, &quot;91&quot;: &quot;hippeastrum&quot;, &quot;29&quot;: &quot;artichoke&quot;, &quot;71&quot;: &quot;gazania&quot;, &quot;90&quot;: &quot;canna lily&quot;, &quot;18&quot;: &quot;peruvian lily&quot;, &quot;98&quot;: &quot;mexican petunia&quot;, &quot;8&quot;: &quot;bird of paradise&quot;, &quot;30&quot;: &quot;sweet william&quot;, &quot;17&quot;: &quot;purple coneflower&quot;, &quot;52&quot;: &quot;wild pansy&quot;, &quot;84&quot;: &quot;columbine&quot;, &quot;12&quot;: &quot;colt\&#x27;s foot&quot;, &quot;11&quot;: &quot;snapdragon&quot;, &quot;96&quot;: &quot;camellia&quot;, &quot;23&quot;: &quot;fritillary&quot;, &quot;50&quot;: &quot;common dandelion&quot;, &quot;44&quot;: &quot;poinsettia&quot;, &quot;53&quot;: &quot;primula&quot;, &quot;72&quot;: &quot;azalea&quot;, &quot;65&quot;: &quot;californian poppy&quot;, &quot;80&quot;: &quot;anthurium&quot;, &quot;76&quot;: &quot;morning glory&quot;, &quot;37&quot;: &quot;cape flower&quot;, &quot;56&quot;: &quot;bishop of llandaff&quot;, &quot;60&quot;: &quot;pink-yellow dahlia&quot;, &quot;82&quot;: &quot;clematis&quot;, &quot;58&quot;: &quot;geranium&quot;, &quot;75&quot;: &quot;thorn apple&quot;, &quot;41&quot;: &quot;barbeton daisy&quot;, &quot;95&quot;: &quot;bougainvillea&quot;, &quot;43&quot;: &quot;sword lily&quot;, &quot;83&quot;: &quot;hibiscus&quot;, &quot;78&quot;: &quot;lotus lotus&quot;, &quot;88&quot;: &quot;cyclamen&quot;, &quot;94&quot;: &quot;foxglove&quot;, &quot;81&quot;: &quot;frangipani&quot;, &quot;74&quot;: &quot;rose&quot;, &quot;89&quot;: &quot;watercress&quot;, &quot;73&quot;: &quot;water lily&quot;, &quot;46&quot;: &quot;wallflower&quot;, &quot;77&quot;: &quot;passion flower&quot;, &quot;51&quot;: &quot;petunia&quot;&#125;&#x27;</span></span><br><span class="line"><span class="comment"># load data </span></span><br><span class="line">cat_to_name = json.loads(json_data)</span><br><span class="line"><span class="comment"># Turn the key into an int, because the label of the dataset starts from 0. But this json starts from 1, so we have to -1 </span></span><br><span class="line">cat_to_name = &#123;<span class="built_in">int</span>(k)-<span class="number">1</span>:v <span class="keyword">for</span> k,v <span class="keyword">in</span> cat_to_name.items()&#125;</span><br><span class="line"><span class="comment"># sort and print </span></span><br><span class="line">class_names = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(cat_to_name.items()))</span><br><span class="line"><span class="built_in">print</span>(class_names)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;pink primrose&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;hard-leaved pocket orchid&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;canterbury bells&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;sweet pea&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;english marigold&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;tiger lily&#x27;</span>, <span class="number">6</span>: <span class="string">&#x27;moon orchid&#x27;</span>, <span class="number">7</span>: <span class="string">&#x27;bird of paradise&#x27;</span>, <span class="number">8</span>: <span class="string">&#x27;monkshood&#x27;</span>, <span class="number">9</span>: <span class="string">&#x27;globe thistle&#x27;</span>, <span class="number">10</span>: <span class="string">&#x27;snapdragon&#x27;</span>, <span class="number">11</span>: <span class="string">&quot;colt&#x27;s foot&quot;</span>, <span class="number">12</span>: <span class="string">&#x27;king protea&#x27;</span>, <span class="number">13</span>: <span class="string">&#x27;spear thistle&#x27;</span>, <span class="number">14</span>: <span class="string">&#x27;yellow iris&#x27;</span>, <span class="number">15</span>: <span class="string">&#x27;globe-flower&#x27;</span>, <span class="number">16</span>: <span class="string">&#x27;purple coneflower&#x27;</span>, <span class="number">17</span>: <span class="string">&#x27;peruvian lily&#x27;</span>, <span class="number">18</span>: <span class="string">&#x27;balloon flower&#x27;</span>, <span class="number">19</span>: <span class="string">&#x27;giant white arum lily&#x27;</span>, <span class="number">20</span>: <span class="string">&#x27;fire lily&#x27;</span>, <span class="number">21</span>: <span class="string">&#x27;pincushion flower&#x27;</span>, <span class="number">22</span>: <span class="string">&#x27;fritillary&#x27;</span>, <span class="number">23</span>: <span class="string">&#x27;red ginger&#x27;</span>, <span class="number">24</span>: <span class="string">&#x27;grape hyacinth&#x27;</span>, <span class="number">25</span>: <span class="string">&#x27;corn poppy&#x27;</span>, <span class="number">26</span>: <span class="string">&#x27;prince of wales feathers&#x27;</span>, <span class="number">27</span>: <span class="string">&#x27;stemless gentian&#x27;</span>, <span class="number">28</span>: <span class="string">&#x27;artichoke&#x27;</span>, <span class="number">29</span>: <span class="string">&#x27;sweet william&#x27;</span>, <span class="number">30</span>: <span class="string">&#x27;carnation&#x27;</span>, <span class="number">31</span>: <span class="string">&#x27;garden phlox&#x27;</span>, <span class="number">32</span>: <span class="string">&#x27;love in the mist&#x27;</span>, <span class="number">33</span>: <span class="string">&#x27;mexican aster&#x27;</span>, <span class="number">34</span>: <span class="string">&#x27;alpine sea holly&#x27;</span>, <span class="number">35</span>: <span class="string">&#x27;ruby-lipped cattleya&#x27;</span>, <span class="number">36</span>: <span class="string">&#x27;cape flower&#x27;</span>, <span class="number">37</span>: <span class="string">&#x27;great masterwort&#x27;</span>, <span class="number">38</span>: <span class="string">&#x27;siam tulip&#x27;</span>, <span class="number">39</span>: <span class="string">&#x27;lenten rose&#x27;</span>, <span class="number">40</span>: <span class="string">&#x27;barbeton daisy&#x27;</span>, <span class="number">41</span>: <span class="string">&#x27;daffodil&#x27;</span>, <span class="number">42</span>: <span class="string">&#x27;sword lily&#x27;</span>, <span class="number">43</span>: <span class="string">&#x27;poinsettia&#x27;</span>, <span class="number">44</span>: <span class="string">&#x27;bolero deep blue&#x27;</span>, <span class="number">45</span>: <span class="string">&#x27;wallflower&#x27;</span>, <span class="number">46</span>: <span class="string">&#x27;marigold&#x27;</span>, <span class="number">47</span>: <span class="string">&#x27;buttercup&#x27;</span>, <span class="number">48</span>: <span class="string">&#x27;oxeye daisy&#x27;</span>, <span class="number">49</span>: <span class="string">&#x27;common dandelion&#x27;</span>, <span class="number">50</span>: <span class="string">&#x27;petunia&#x27;</span>, <span class="number">51</span>: <span class="string">&#x27;wild pansy&#x27;</span>, <span class="number">52</span>: <span class="string">&#x27;primula&#x27;</span>, <span class="number">53</span>: <span class="string">&#x27;sunflower&#x27;</span>, <span class="number">54</span>: <span class="string">&#x27;pelargonium&#x27;</span>, <span class="number">55</span>: <span class="string">&#x27;bishop of llandaff&#x27;</span>, <span class="number">56</span>: <span class="string">&#x27;gaura&#x27;</span>, <span class="number">57</span>: <span class="string">&#x27;geranium&#x27;</span>, <span class="number">58</span>: <span class="string">&#x27;orange dahlia&#x27;</span>, <span class="number">59</span>: <span class="string">&#x27;pink-yellow dahlia&#x27;</span>, <span class="number">60</span>: <span class="string">&#x27;cautleya spicata&#x27;</span>, <span class="number">61</span>: <span class="string">&#x27;japanese anemone&#x27;</span>, <span class="number">62</span>: <span class="string">&#x27;black-eyed susan&#x27;</span>, <span class="number">63</span>: <span class="string">&#x27;silverbush&#x27;</span>, <span class="number">64</span>: <span class="string">&#x27;californian poppy&#x27;</span>, <span class="number">65</span>: <span class="string">&#x27;osteospermum&#x27;</span>, <span class="number">66</span>: <span class="string">&#x27;spring crocus&#x27;</span>, <span class="number">67</span>: <span class="string">&#x27;bearded iris&#x27;</span>, <span class="number">68</span>: <span class="string">&#x27;windflower&#x27;</span>, <span class="number">69</span>: <span class="string">&#x27;tree poppy&#x27;</span>, <span class="number">70</span>: <span class="string">&#x27;gazania&#x27;</span>, <span class="number">71</span>: <span class="string">&#x27;azalea&#x27;</span>, <span class="number">72</span>: <span class="string">&#x27;water lily&#x27;</span>, <span class="number">73</span>: <span class="string">&#x27;rose&#x27;</span>, <span class="number">74</span>: <span class="string">&#x27;thorn apple&#x27;</span>, <span class="number">75</span>: <span class="string">&#x27;morning glory&#x27;</span>, <span class="number">76</span>: <span class="string">&#x27;passion flower&#x27;</span>, <span class="number">77</span>: <span class="string">&#x27;lotus lotus&#x27;</span>, <span class="number">78</span>: <span class="string">&#x27;toad lily&#x27;</span>, <span class="number">79</span>: <span class="string">&#x27;anthurium&#x27;</span>, <span class="number">80</span>: <span class="string">&#x27;frangipani&#x27;</span>, <span class="number">81</span>: <span class="string">&#x27;clematis&#x27;</span>, <span class="number">82</span>: <span class="string">&#x27;hibiscus&#x27;</span>, <span class="number">83</span>: <span class="string">&#x27;columbine&#x27;</span>, <span class="number">84</span>: <span class="string">&#x27;desert-rose&#x27;</span>, <span class="number">85</span>: <span class="string">&#x27;tree mallow&#x27;</span>, <span class="number">86</span>: <span class="string">&#x27;magnolia&#x27;</span>, <span class="number">87</span>: <span class="string">&#x27;cyclamen&#x27;</span>, <span class="number">88</span>: <span class="string">&#x27;watercress&#x27;</span>, <span class="number">89</span>: <span class="string">&#x27;canna lily&#x27;</span>, <span class="number">90</span>: <span class="string">&#x27;hippeastrum&#x27;</span>, <span class="number">91</span>: <span class="string">&#x27;bee balm&#x27;</span>, <span class="number">92</span>: <span class="string">&#x27;ball moss&#x27;</span>, <span class="number">93</span>: <span class="string">&#x27;foxglove&#x27;</span>, <span class="number">94</span>: <span class="string">&#x27;bougainvillea&#x27;</span>, <span class="number">95</span>: <span class="string">&#x27;camellia&#x27;</span>, <span class="number">96</span>: <span class="string">&#x27;mallow&#x27;</span>, <span class="number">97</span>: <span class="string">&#x27;mexican petunia&#x27;</span>, <span class="number">98</span>: <span class="string">&#x27;bromelia&#x27;</span>, <span class="number">99</span>: <span class="string">&#x27;blanket flower&#x27;</span>, <span class="number">100</span>: <span class="string">&#x27;trumpet creeper&#x27;</span>, <span class="number">101</span>: <span class="string">&#x27;blackberry lily&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p>Here I mainly refer to the official website <a href="hhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning</a> to change the writing style to the dataSet I want, and start to download the file:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">        <span class="comment"># First, the image is randomly cropped and then resized. A random rectangular area is chosen and the image is cropped.</span></span><br><span class="line">        <span class="comment"># The cropped image is then resized to the specified size of 224x224 pixels.</span></span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        <span class="comment"># Set the probability of image flipping, usually a number from 0 to 1, for example, 0.5, which means there&#x27;s a 50% chance of flipping the image. Default value is 0.5</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        <span class="comment"># Convert the image into a Tensor</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        <span class="comment"># Normalize the image values using numerical normalization, where the first parameter is mean, and the second parameter is the standard deviation (std)</span></span><br><span class="line">        <span class="comment"># The reason for setting [0.485, 0.456, 0.406] can be referred from: https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/</span></span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">&#x27;val&#x27;</span>: transforms.Compose([</span><br><span class="line">        <span class="comment"># This doesn&#x27;t randomly select an area but directly resizes the entire image to fit the specified size.</span></span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        <span class="comment"># Keep the central part of the image, then resize to meet the specified size.</span></span><br><span class="line">        <span class="comment"># Used for validation or test data to ensure that the test images have similar features, and don&#x27;t have the randomness like RandomResizedCrop.</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># We download the training data into the data_dir/train folder, and use the data_transforms[&quot;train&quot;] function for data transformation.</span></span><br><span class="line">train_datasets = Flowers102(root=data_dir+<span class="string">&quot;/train&quot;</span>, split=<span class="string">&quot;train&quot;</span>, download=<span class="literal">True</span>, transform=data_transforms[<span class="string">&quot;train&quot;</span>])</span><br><span class="line"><span class="comment"># We download the validation data into the data_dir/val folder, and use the data_transforms[&quot;val&quot;] function for data transformation.</span></span><br><span class="line">val_datasets = Flowers102(root=data_dir+<span class="string">&quot;/val&quot;</span>, split=<span class="string">&quot;val&quot;</span>, download=<span class="literal">True</span>, transform=data_transforms[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Specify to download the flowers102 dataset, downloading both train and val datasets.</span></span><br><span class="line">image_datasets = &#123;x: Flowers102(root=data_dir, split=x, download=<span class="literal">True</span>, transform=data_transforms[x])</span><br><span class="line">                    <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert to DataLoader format, and specify batch_size</span></span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;device: &quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;image_datasets function call: &quot;</span>, <span class="built_in">dir</span>(image_datasets[<span class="string">&quot;train&quot;</span>]))</span><br></pre></td></tr></table></figure><div class="note info flat"><p>This completes the first Task, which is to download the dataset we want.</p></div><h1 id="Task-2-Printing-out-images-and-profile-sizes">Task 2 - Printing out images and profile sizes</h1><div class="note info flat"><ol start="2"><li><em><strong>Print images and profile size</strong></em>: display some sample images of the dataset in the notebook and print the dataset size.</li></ol></div><p>Referring to the official website <a href="hhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning</a> for the writeup, we first create the</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">inp, title=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Display image for Tensor.&quot;&quot;&quot;</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;train&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.item() takes the value of the tensor, usually a number, and finds the English equivalent of that number in the class_names dic</span></span><br><span class="line">imshow(out, title=[class_names[x.item()] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(inputs.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dataset_sizes: &quot;</span>,dataset_sizes)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><p><img src="https://i.imgur.com/dPGyFvN.png" alt=""></p><h1 id="Task-3-4-CNN-Batch-Normalization">Task 3 &amp; 4 - CNN + Batch Normalization</h1><div class="note info flat"><ol start="3"><li><strong>Construct a CNN using Batch Normalization</strong>: Design a CNN to predict on the dataset. Use a similar architecture like last time, but this time also include batch normalization layers.</li><li>**Train the model on the dataset and measure the accuracy on hold out test data.</li></ol></div><p><strong>According to Prof. Hongyi Li in Transfer Learning, he mentioned…</strong><br>Usually, <code>Batch Normalization</code> is performed before <code>Activation Function</code>, you can refer to this <a href="/posts/ML.html#Feature-Normalization">section</a> if you are interested. <code>Batch Normalization</code> is simply to run <code>feature normalization</code> in the same way as <code>Batch</code>.</p><p><strong>Why do we need to do feature normalization?  Why do we do feature normalization?</strong><br>It is to let different features have similar value ranges, so that when the model performs Gradient Descent, the effect of w1 and w2 on the loss will not be too big, and they have similar value ranges, so that they can affect the loss evenly, instead of a certain w1 affecting the loss much more than w2.</p><blockquote><p>Instead of a w1 having a much larger effect on loss than w2, the effect will be something like the following.</p></blockquote><p>! <a href="https://i.imgur.com/RB51XXy.png">Origin</a></p><h2 id="Build-the-Network">Build the Network</h2><div class="note warning flat"><p>Note that depending on the size of the dataset and the number of hidden layers, you have to make two adjustments!</p><ol><li>In the fully connection layer, the input is determined by the number of times your hidden layer performs <code>max-pooling</code> and <code>convolution</code>. 2.</li><li>Then you have to adjust the number of outputs in the last output layer according to the number of categories in your dataset.</li></ol><p>Please note the part of the code labeled with the comment arrow <code>&lt;====</code>.</p></div><p>So our CNN architecture is as follows, <code>You can decide whether you want to run a dropout to unpack the annotations or not</code>.<br>But in my case, I tested that the dropout didn’t result in higher accuracy.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NewNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NewNet, self).__init__()</span><br><span class="line">        <span class="comment"># Layer 1: 3x3 kernel, depth = 32, 224-3+1=222 =&gt; 222x222 pixel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        <span class="comment"># self.dropout1 = nn.Dropout(0.5) # Apply dropout as needed</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 2: Max pooling with 2x2 kernel, 222/2=111 =&gt; 111x111 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 3: 3x3 kernel, depth = 64, 111-3+1=109 =&gt; 109x109 pixel</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        <span class="comment"># self.dropout2 = nn.Dropout(0.5) # Apply dropout as needed</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 4: Max pooling with 2x2 kernel, 109/2=54 =&gt; 54x54 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 5: 3x3 kernel, depth = 128, 54-3+1=52 =&gt; 52x52 pixel</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        <span class="comment"># self.dropout3 = nn.Dropout(0.5) # Apply dropout as needed</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 6: Max pooling with 2x2 kernel, 52/2=26 =&gt; 26x26 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Final input is 512, pixel is 26*26 =&gt; 128*26*26</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">26</span> * <span class="number">26</span>, <span class="number">2048</span>) <span class="comment"># &lt;==== Adjust according to the hidden layer, 128 * 26 * 26</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">512</span>, <span class="number">102</span>) <span class="comment"># &lt;==== 102 according to the number of classes in the dataset</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># We put the batch normalization before the activation function. </span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout1(x) # Apply dropout as needed</span></span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout2(x) # Apply dropout as needed</span></span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout3(x) # Apply dropout as needed</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">26</span> * <span class="number">26</span>) <span class="comment"># &lt;==== Adjust according to the hidden layer, 128 * 26 * 26</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.relu(self.fc3(x))</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = NewNet()</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>and specify the optimizer and loss function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h2 id="Create-a-Training-Func">Create a Training Func</h2><p>We need to create a funcntion to execute the training model as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch, start_time</span>):</span><br><span class="line">    net.train()</span><br><span class="line">    cur_count = <span class="number">0</span> </span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloaders[<span class="string">&quot;train&quot;</span>], <span class="number">0</span>):</span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)</span><br><span class="line">        inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        outputs.to(device)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.to(device)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch&#125;</span>, <span class="subst">&#123;batch_idx + <span class="number">1</span>:5d&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">100</span>:<span class="number">.3</span>f&#125;</span> time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time))&#125;</span> sec.&#x27;</span>)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure><h2 id="Build-Testing-Func">Build Testing Func</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(): </span><br><span class="line">    net.<span class="built_in">eval</span>()  <span class="comment"># set model to evaluation mode</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    class_correct = [<span class="number">0</span>] * <span class="built_in">len</span>(class_names)  </span><br><span class="line">    class_total = [<span class="number">0</span>] * <span class="built_in">len</span>(class_names)  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dataloaders[<span class="string">&quot;val&quot;</span>]:</span><br><span class="line">            images, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">            outputs = net(images) </span><br><span class="line"></span><br><span class="line">            <span class="comment"># select top 3 predictions</span></span><br><span class="line">            _, predicted = torch.topk(outputs, <span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check if predicted labels are in true labels</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">                total += <span class="number">1</span></span><br><span class="line">                class_total[labels[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> labels[i] <span class="keyword">in</span> predicted[i]:</span><br><span class="line">                    correct += <span class="number">1</span></span><br><span class="line">                    class_correct[labels[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    class_accuracies = [class_correct[i] / class_total[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(class_names))]</span><br><span class="line">    accuracy = correct / total</span><br><span class="line">    <span class="keyword">return</span> accuracy, class_accuracies</span><br></pre></td></tr></table></figure><h2 id="Run-Training">Run Training</h2><p>In order for us to see the status of the training during the training process, we print the status of the training every 100 batches and the status of the test every 5 epochs.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">100</span> </span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line">accuracy, class_accuracies = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test data (top-1): <span class="subst">&#123;<span class="number">100</span> * accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_epochs - <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;============ Epoch: <span class="subst">&#123;epoch&#125;</span> ==========&quot;</span>)</span><br><span class="line">    train(epoch, start_time)</span><br><span class="line">    <span class="comment"># every 5 epoch is completed, we will perform validation on the test set </span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        accuracy, class_accuracies = test()</span><br><span class="line">        <span class="comment"># print accuracies</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test data (top-1): <span class="subst">&#123;<span class="number">100</span> * accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Finished Training. Total elapsed time: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>, <span class="number">1</span>)&#125;</span> min&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Accuracy on test data (top<span class="number">-1</span>): <span class="number">0.0019</span>%</span><br><span class="line">============ Epoch: <span class="number">0</span> ==========</span><br><span class="line">[<span class="number">0</span>,   <span class="number">100</span>] loss: <span class="number">4.984</span> <span class="built_in">time</span> elapsed: <span class="number">40</span> sec.</span><br><span class="line">[<span class="number">0</span>,   <span class="number">200</span>] loss: <span class="number">4.876</span> <span class="built_in">time</span> elapsed: <span class="number">47</span> sec.</span><br><span class="line">...</span><br><span class="line">Accuracy on test data (top<span class="number">-1</span>): <span class="number">35.59</span>%</span><br><span class="line">Finished Training. Total elapsed <span class="built_in">time</span>: <span class="number">67</span> <span class="built_in">min</span></span><br></pre></td></tr></table></figure><h1 id="Task-5-4-Transfer-Learning：Resnet18">Task 5 &amp; 4 - Transfer Learning：Resnet18</h1><div class="note info flat"><ol start="4"><li><strong>Train a model using a dataset and print out the accuracy of the test</strong>: train a model on a dataset and measure the accuracy on retained test data.</li><li><strong>Use ResNet18 for Migration Learning</strong>: now use migration learning to use a pre-trained ResNet18 on the dataset as follows:<br><strong>Without changing the trained weights of other people’s models</strong>: ResNet18 is used as a fixed feature extractor.<ol><li><strong>Fix the parameters</strong>: Fix the parameters of the ResNet18 model and only train the last layer.</li><li><strong>Fine-tuning</strong>: Fine-tune the ResNet18 model.</li></ol></li></ol></div><h2 id="Build-up-Trainning-Testing-Func">Build up Trainning &amp; Testing Func</h2><p>Refer to the official website <a href="hhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning</a> for the writeup, we first create the</p><figure class="highlight python"><figcaption><span>def train_model(model, criterion, optimizer, scheduler, num_epochs</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set the start time for logging to monitor the training time for each epoch</span></span><br><span class="line">since = time.time()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a temporary folder to store the best model parameters</span></span><br><span class="line"><span class="keyword">with</span> TemporaryDirectory() <span class="keyword">as</span> tempdir:</span><br><span class="line">    best_model_params_path = os.path.join(tempdir, <span class="string">&#x27;best_model_params.pt&#x27;</span>)</span><br><span class="line">    <span class="comment"># Haven&#x27;t started training yet, but we first save the current model</span></span><br><span class="line">    torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line">    best_acc = <span class="number">0.0</span>  <span class="comment"># Set the current best accuracy to 0, it will be updated if a higher value is found to determine the best model</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs - <span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># After training each epoch, proceed with validation</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">            <span class="comment"># Determine whether it&#x27;s training or validation phase</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.<span class="built_in">eval</span>()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Iterate over data.</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                <span class="comment"># Move to GPU</span></span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Zero the parameter gradients</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Forward propagation</span></span><br><span class="line">                <span class="comment"># track history if only in train</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment"># Select the number with the highest prediction as the label</span></span><br><span class="line">                    loss = criterion(outputs, labels) <span class="comment"># Calculate the difference between the answer and prediction</span></span><br><span class="line"></span><br><span class="line">                    <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                    <span class="comment"># Perform backward propagation</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Since batch_size is 4, multiply loss by 4 to get the loss for a batch</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># Calculate how many are correct in a batch</span></span><br><span class="line">                running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Adjust the learning rate only during training</span></span><br><span class="line">            <span class="comment"># scheduler is a learning rate (lr) adjuster used to modify the lr value during model training</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># After an entire epoch of training, calculate the loss and accuracy for that epoch</span></span><br><span class="line">            <span class="comment"># Avg. loss = total loss / size of the entire dataset</span></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            <span class="comment"># Avg. Acc = total number of correct answers / size of the entire dataset</span></span><br><span class="line">            epoch_acc = running_corrects.<span class="built_in">float</span>() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;phase&#125;</span> Loss: <span class="subst">&#123;epoch_loss:<span class="number">.4</span>f&#125;</span> Acc: <span class="subst">&#123;epoch_acc:<span class="number">.4</span>f&#125;</span> Time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - since))&#125;</span> sec.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># If during validation, and accuracy is found to be better than the current best, save the model parameters</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;val&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                <span class="comment"># Update the current best accuracy</span></span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                <span class="comment"># Deep copy the model</span></span><br><span class="line">                torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Training complete in <span class="subst">&#123;time_elapsed // <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>m <span class="subst">&#123;time_elapsed % <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Best val Acc: <span class="subst">&#123;best_acc:4f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Load the best model weights</span></span><br><span class="line">    <span class="comment"># Take out the best model so far to continue with the next epoch of training</span></span><br><span class="line">    model.load_state_dict(torch.load(best_model_params_path))</span><br><span class="line"><span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h2 id="Use-Transfer-Learning">Use Transfer Learning</h2><p>According to the teacher’s request, I want to use resnet18 for Transfer Learning, currently according to the [official description](<a href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18">https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18</a>. html#torchvision.models.ResNet18_Weights), <code>resent18</code> is <code>IMAGENET1K_V1</code> by default if we don’t give the parameter, in order to make it clear which model’s parameter we are using, we still give the parameter.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_ftrs is the number of input features for the last layer. </span></span><br><span class="line"><span class="comment"># Retrieve the number of input features for the last layer</span></span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 102.</span></span><br><span class="line"><span class="comment"># model_ft.fc is the final layer of the model, used for classification.</span></span><br><span class="line"><span class="comment"># Creating the final layer ourselves, setting the input number as num_ftrs, and output number as 102 (since there are 102 classes in this case)</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The loss function uses CrossEntropyLoss </span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line"><span class="comment"># Optimizer uses SGD with learning rate = 0.001, momentum = 0.9</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line"><span class="comment"># Multiply the learning rate by 0.1 every 7 epochs to decay the lr</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/rygK4KS.png" alt=""></p><div class="note warning flat"><p><strong>Why need to adjust lr?</strong><br>Adjusting the learning rate every certain epoch is a common learning rate adjustment strategy called learning rate decay or learning rate scheduling. The effect of this is to:</p><ol><li><p>Improve model stability: During training, ``using a relatively large learning rate at the beginning helps to converge quickly.‘’ However, when training <code>near the optimal solution, a larger learning rate may cause the model to oscillate or over-adjust near the optimal solution</code>. By periodically decreasing the learning rate, the model will be more stable and closer to the optimal solution in the later stages of training.</p></li><li><p>Preventing overfitting: `Periodically decreasing the learning rate helps prevent the model from overfitting on the training set.’ When the learning rate is reduced, the model adjusts its parameters more carefully and is less likely to fall into the noise in the training set.</p></li></ol><p>In practice, the specific settings of the learning rate tuning strategy (e.g., the values of <code>step_size</code> and <code>gamma</code>) are usually adjusted based on trials and experience to achieve optimal performance. Typically, the settings of these parameters depend on the size of your dataset, the model architecture, the difficulty of the problem, and other factors.</p></div><h2 id="Start-Training">Start Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result: Accuracy on test data (top-1): 89.41%</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.4280</span> Acc: <span class="number">0.0657</span> Time elapsed: <span class="number">33</span> sec.</span><br><span class="line">val Loss: <span class="number">2.9901</span> Acc: <span class="number">0.3118</span> Time elapsed: <span class="number">58</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.3046</span> Acc: <span class="number">0.2353</span> Time elapsed: <span class="number">87</span> sec.</span><br><span class="line">val Loss: <span class="number">1.6604</span> Acc: <span class="number">0.5941</span> Time elapsed: <span class="number">112</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.5080</span> Acc: <span class="number">0.4029</span> Time elapsed: <span class="number">141</span> sec.</span><br><span class="line">val Loss: <span class="number">1.2243</span> Acc: <span class="number">0.6951</span> Time elapsed: <span class="number">166</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.9871</span> Acc: <span class="number">0.5196</span> Time elapsed: <span class="number">195</span> sec.</span><br><span class="line">val Loss: <span class="number">0.9578</span> Acc: <span class="number">0.7216</span> Time elapsed: <span class="number">219</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.5865</span> Acc: <span class="number">0.6225</span> Time elapsed: <span class="number">249</span> sec.</span><br><span class="line">val Loss: <span class="number">0.6911</span> Acc: <span class="number">0.8108</span> Time elapsed: <span class="number">273</span> sec.</span><br><span class="line">...</span><br><span class="line">val Loss: <span class="number">0.3919</span> Acc: <span class="number">0.8912</span> Time elapsed: <span class="number">1313</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">21</span>m <span class="number">53</span>s</span><br><span class="line">Best val Acc: <span class="number">0.894118</span></span><br></pre></td></tr></table></figure><h2 id="Using-ResNet18-as-a-fixed-feature-extractor">Using ResNet18 as a fixed feature extractor</h2><p>Due to the requirements of the homework, ResNet18 is required to be used as a fixed feature extractor, so we need to set all the parameters to be untrainable, and only the parameters of the last layer can be trained. <strong>Simply put, don’t change the weights of other people’s models.</strong> The only thing we need to change is to set each parameter of the model’s <code>requires_grad</code> to False. This way, we can use ResNet18 as a fixed feature extractor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!! Add these two lines to set requires_grad to False, so that the parameters will not be updated </span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We start to train </span></span><br><span class="line">model_conv_SGD = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result: Accuracy on test data: 79.11%</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.6979</span> Acc: <span class="number">0.0176</span> Time elapsed: <span class="number">24</span> sec.</span><br><span class="line">val Loss: <span class="number">3.9863</span> Acc: <span class="number">0.1235</span> Time elapsed: <span class="number">48</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.0589</span> Acc: <span class="number">0.1137</span> Time elapsed: <span class="number">72</span> sec.</span><br><span class="line">val Loss: <span class="number">3.1125</span> Acc: <span class="number">0.3608</span> Time elapsed: <span class="number">95</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.4935</span> Acc: <span class="number">0.2304</span> Time elapsed: <span class="number">119</span> sec.</span><br><span class="line">val Loss: <span class="number">2.5003</span> Acc: <span class="number">0.4912</span> Time elapsed: <span class="number">142</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.1030</span> Acc: <span class="number">0.3422</span> Time elapsed: <span class="number">165</span> sec.</span><br><span class="line">val Loss: <span class="number">2.1583</span> Acc: <span class="number">0.5510</span> Time elapsed: <span class="number">189</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.7367</span> Acc: <span class="number">0.4402</span> Time elapsed: <span class="number">212</span> sec.</span><br><span class="line">val Loss: <span class="number">1.7064</span> Acc: <span class="number">0.6304</span> Time elapsed: <span class="number">236</span> sec.</span><br><span class="line">...</span><br><span class="line">val Loss: <span class="number">1.0910</span> Acc: <span class="number">0.7824</span> Time elapsed: <span class="number">1179</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">19</span>m <span class="number">39</span>s</span><br><span class="line">Best val Acc: <span class="number">0.791176</span></span><br></pre></td></tr></table></figure><h1 id="Task-6-4-Transfer-Learning：EfficientNet-B5">Task 6 &amp; 4 - Transfer Learning：EfficientNet_B5</h1><div class="note info flat"><ol start="4"><li><em><strong>Train a model using a dataset and print out the accuracy of the test</strong></em>: train a model on a dataset and measure the accuracy on retained test data.</li><li><em><strong>Fine-tuning using RestNet</strong></em> : ResNet18 is fine-tuned on the training data.</li></ol></div><p>We need to install the <code>efficientnet_pytorch</code> package first:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install efficientnet_pytorch</span><br></pre></td></tr></table></figure><p>Then we can import the package and use it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> efficientnet_pytorch <span class="keyword">import</span> EfficientNet</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained EfficientNet-B5 model</span></span><br><span class="line">model_ft = EfficientNet.from_pretrained(<span class="string">&#x27;efficientnet-b5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We obtain the number of input features for the last layer</span></span><br><span class="line">num_ftrs = model_ft._fc.in_features</span><br><span class="line"><span class="comment"># build a new layer, the input number is num_ftrs, and the output number is 102 (because there are 102 classes in this case)</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line"><span class="comment"># Put the model on the GPU</span></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start training </span></span><br><span class="line">model_ft_effb5 = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result: Accuracy on test data: 82.15%</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">6.2860</span> Acc: <span class="number">0.0157</span> Time elapsed: <span class="number">156</span> sec.</span><br><span class="line">val Loss: <span class="number">5.6637</span> Acc: <span class="number">0.0382</span> Time elapsed: <span class="number">200</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.8955</span> Acc: <span class="number">0.1039</span> Time elapsed: <span class="number">322</span> sec.</span><br><span class="line">val Loss: <span class="number">4.5101</span> Acc: <span class="number">0.2392</span> Time elapsed: <span class="number">365</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.8566</span> Acc: <span class="number">0.2422</span> Time elapsed: <span class="number">485</span> sec.</span><br><span class="line">val Loss: <span class="number">3.6194</span> Acc: <span class="number">0.4265</span> Time elapsed: <span class="number">529</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.0979</span> Acc: <span class="number">0.3637</span> Time elapsed: <span class="number">653</span> sec.</span><br><span class="line">val Loss: <span class="number">2.8613</span> Acc: <span class="number">0.5539</span> Time elapsed: <span class="number">696</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.4323</span> Acc: <span class="number">0.4725</span> Time elapsed: <span class="number">818</span> sec.</span><br><span class="line">val Loss: <span class="number">2.2894</span> Acc: <span class="number">0.6725</span> Time elapsed: <span class="number">863</span> sec.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Epoch <span class="number">24</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.3168</span> Acc: <span class="number">0.7343</span> Time elapsed: <span class="number">4769</span> sec.</span><br><span class="line">val Loss: <span class="number">1.3167</span> Acc: <span class="number">0.8167</span> Time elapsed: <span class="number">4812</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">80</span>m <span class="number">12</span>s</span><br><span class="line">Best val Acc: <span class="number">0.821569</span></span><br></pre></td></tr></table></figure><h1 id="Task-7-Discussion">Task 7 - Discussion</h1><div class="note info flat"><p><strong>Compare these different methods and print out the accuracy</strong>: Compare the accuracy of the different methods on the test data and print out the training time for each method.</p></div><p>From the results of the above experiments, we can see that the accuracy of the model is as follows:</p><ul><li><a href="#Task-3-4-CNN-Batch-Normalization">Use the CNN build by ourselves</a></li><li>[Using Transfer Learning Resnet18](#Task-5-4-Transfer Learning: Resnet18)</li><li>[Using Transfer Learning EfficientNet-B5](#Task-6-4-Transfer Learning: EfficientNet-B5)</li></ul><p>Their data are as follows:</p><table><thead><tr><th>Model</th><th>Accuracy</th><th>Training Time</th><th>Result</th></tr></thead><tbody><tr><td>Self-built CNN</td><td><code>35%</code></td><td><code>more than 1 hour</code></td><td>Worst</td></tr><tr><td>Resnet18</td><td><strong>89.41%</strong></td><td>21 minutes</td><td>highest accuracy</td></tr><tr><td>Resnet18 (fixed feature extractor)</td><td>79.11%</td><td><strong>19 minutes</strong></td><td>Shortest</td></tr><tr><td>EfficientNet-B5</td><td>82.15%</td><td>80 mins</td><td>PuPu</td></tr></tbody></table><div class="note warning flat"><p><strong>Conclusion</strong></p><ul><li>If we use Transfer Learning, we can obviously feel that the accuracy rate is significantly improved and the training time is greatly reduced.</li><li>Moreover, in the current case, the accuracy is better without fixed model parameters, although the matching time is longer because of the gradient descent.</li></ul></div>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Mechine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mechine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard</title>
      <link href="/en/posts/pytorch-CNN-TensorBoard/"/>
      <url>/en/posts/pytorch-CNN-TensorBoard/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction">Introduction</h1><p>I recently enrolled in an AI course, and this is the third assignment. It mainly refers to the following websites:</p><ul><li>Teaching how to build a CNN using PyTorch: <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html">Pytorch Tutorial</a></li><li>Teaching how to use TensorBoard with PyTorch: <a href="https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html">Pytorch TensorBoard Tutorial</a></li><li>Tutorial on using TensorBoard in CoLab: <a href="https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks">TensorBoard in CoLab Tutorial</a></li></ul><p>The main purpose of this article is to understand CNNs, try to build a deeper network, use GPU to improve efficiency, and finally display the results of Loss and mispredicted results on TensorBoard.</p><h1 id="Environment-Setup-and-Homework-Requirements">Environment Setup and Homework Requirements</h1><blockquote><p>Environment setup:</p><ul><li>Python 3.10.9</li><li>Pytorch 2.0.1</li></ul></blockquote><h1 id="Homework-Requirements">Homework Requirements</h1><p>Task:</p><ol><li><strong>First build a CNN</strong>: Train the same network as in the PyTorch CNN tutorial.</li><li><strong>Build a CNN that meets the following requirements</strong>: Change the network architecture as follows and train the network:<ol><li>Conv layer with 3x3 kernel and depth = 8, ReLu activation</li><li>Conv layer with 3x3 kernel and depth = 16, ReLu activation</li><li>Max pooling with 2x2 kernel</li><li>Conv layer with 3x3 kernel and depth = 32, ReLu activation</li><li>Conv layer with 3x3 kernel and depth = 64, ReLu activation</li><li>Max pooling with 2x2 kernel</li><li>Fully connected with 4096 nodes, ReLu activation</li><li>Fully connected with 1000 nodes, ReLu activation</li><li>Fully connected with 10 nodes, no activation</li></ol></li><li><strong>Use GPU and compare with CPU results</strong>: Run the training on the GPU and compare the training time to CPU.</li><li><strong>Log Training Loss to TensorBoard</strong>: Log the training loss in TensorBoard.</li><li><strong>Modify the criterion for correctness to include predictions in the top three outputs</strong>: Change the test metric as follows: A prediction is considered „correct“ if the true label is within the top three outputs of the network. Print the accuracy on the test data (with respect to this new definition).</li><li><strong>Randomly select five examples of incorrect predictions and display them on TensorBoard</strong>: Randomly take 5 examples on which the network was wrong on the test data (according to the new definition of correct) and plot them to TensorBoard together with the true label.</li><li><strong>Display TensorBoard in the notebook</strong>: Show the TensorBoard widget at the end of your notebook.</li></ol><ul><li><strong>Bonus</strong>: See if you can improve results by using a deeper network (or another architecture).</li></ul><h1 id="Preliminary-Preparation">Preliminary Preparation</h1><ol><li>First, load the necessary packages</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Store the TensorBoard results in ./board/assignment_3</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./board/result&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Download the training and testing datasets into a `/data` folder created in the current directory. To normalize, set the mean to 0.5 and the standard deviation to 0.5, indicating that the image range is between [0, 1] and is converted to [-1, 1].</span></span><br><span class="line">```python</span><br><span class="line"><span class="comment"># Define image transformation, converting images to tensors and normalizing them</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># Convert images to PyTorch tensors</span></span><br><span class="line">    <span class="comment"># Since each pixel has three channels (red, green, blue) and channel values are typically in the [0, 1] range.</span></span><br><span class="line">    <span class="comment"># We normalize these three channels to bring their range to [-1, 1].</span></span><br><span class="line">    <span class="comment"># Since the mean of [0,1] is 0.5, setting mean to 0.5 means subtracting the mean of 0.5 to shift the original mean from 0.5 to 0</span></span><br><span class="line">    <span class="comment"># Since the standard deviation of [0,1] is 0.5, setting std to 0.5 means dividing by the standard deviation of 0.5 to change the original standard deviation from 0.5 to 1</span></span><br><span class="line">    <span class="comment"># Finally, as the mean becomes 0 and standard deviation is 1, we obtain the range [-1,1]</span></span><br><span class="line">    transforms.Normalize(mean=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), std=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))  <span class="comment"># Normalize image data</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define batch size for training</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Download and load the CIFAR-10 training dataset</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,  <span class="comment"># Root directory for storing data</span></span><br><span class="line">    train=<span class="literal">True</span>,  <span class="comment"># Load training data</span></span><br><span class="line">    download=<span class="literal">True</span>,  <span class="comment"># Download data (if not already downloaded)</span></span><br><span class="line">    transform=transform  <span class="comment"># Apply previously defined image transformation</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataLoader for the training data for batch processing and data loading</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">    trainset,</span><br><span class="line">    batch_size=batch_size,  <span class="comment"># Set the size of each batch</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,  <span class="comment"># Randomly shuffle data to increase the randomness of training</span></span><br><span class="line">    num_workers=<span class="number">2</span>  <span class="comment"># Use multiple worker processes to speed up data loading</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download and load the CIFAR-10 test dataset, with the same data transformation and data loading settings</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,  <span class="comment"># Load test data</span></span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataLoader for the test data</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">    testset,</span><br><span class="line">    batch_size=batch_size,</span><br><span class="line">    shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Place all class names in a tuple</span></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="Task-1-2-Build-a-CNN">Task 1+2 Build a CNN</h1><div class="note info flat"><ol><li><strong>First build a CNN</strong>: Train the same network as in the PyTorch CNN tutorial.</li><li><strong>Build a CNN that meets the following requirements</strong>: Change the network architecture as follows and train the network.</li></ol></div><h2 id="Build-a-CNN">Build a CNN</h2><p><strong>Task 2. Build a CNN that meets the following requirements</strong>: Change the network architecture as follows and train the network:</p><ol><li>Conv layer with 3x3 kernel and depth = 8, ReLu activation</li><li>Conv layer with 3x3 kernel and depth = 16, ReLu activation</li><li>Max pooling with 2x2 kernel</li><li>Conv layer with 3x3 kernel and depth = 32, ReLu activation</li><li>Conv layer with 3x3 kernel and depth = 64, ReLu activation</li><li>Max pooling with 2x2 kernel</li><li>Fully connected with 4096 nodes, ReLu activation</li><li>Fully connected with 1000 nodes, ReLu activation</li><li>Fully connected with 10 nodes, no activation</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NewNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NewNet, self).__init__()</span><br><span class="line">        <span class="comment"># The input is the depth of the previous layer, which is the color 3 RGB, 32*32 pixel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">8</span>, <span class="number">3</span>)  <span class="comment"># Layer 1: 3x3 kernel and depth = 8</span></span><br><span class="line">        <span class="comment"># The input is the depth of the previous layer, which is the requested 8, 32-3+1=30, thus 30*30 pixel</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">3</span>)  <span class="comment"># Layer 2: 3x3 kernel and depth = 16</span></span><br><span class="line">        <span class="comment"># 30/2=15, thus 15*15 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># Layer 3: Max pooling with 2x2 kernel</span></span><br><span class="line">        <span class="comment"># The input is the depth of the previous layer, which is the requested 16, 15-3+1=13, thus 13*13 pixel</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, <span class="number">3</span>)  <span class="comment"># Layer 4: 3x3 kernel and depth = 32</span></span><br><span class="line">        <span class="comment"># The input is the depth of the previous layer, which is the requested 32, 13-3+1=11, thus 11*11 pixel</span></span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)  <span class="comment"># Layer 5: 3x3 kernel and depth = 64</span></span><br><span class="line">        <span class="comment"># Another Max pooling is needed, 11/2=5, thus 5*5 pixel</span></span><br><span class="line">        <span class="comment"># Therefore, the final input is 64, and the pixel is 5*5, hence 64*5*5</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">4096</span>)  <span class="comment"># Layer 6: Fully connected with 4096 nodes</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4096</span>, <span class="number">1000</span>)  <span class="comment"># Layer 7: Fully connected with 1000 nodes</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">1000</span>, <span class="number">10</span>)  <span class="comment"># Layer 8: Fully connected with 10 nodes</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))  <span class="comment"># ReLu activation</span></span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>)  <span class="comment"># Flatten the tensor</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h1 id="Task-3-4-GPU-and-Loss-on-TensorBoard">Task 3 + 4 GPU and Loss on TensorBoard</h1><div class="note info flat"><ol start="3"><li><strong>Use GPU and Compare CPU Results</strong>: Run the training on the GPU and compare the training time to CPU.</li><li><strong>Log Training Loss on TensorBoard</strong>: Log the training loss in TensorBoard.</li></ol></div><h2 id="Accelerate-Network-Using-GPU">Accelerate Network Using GPU</h2><p>Since I am using a Mac, I input <code>mps</code>, but if you are using a Windows system, please input <code>cuda</code>.<br>Initialize function and optimizer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># device = torch.device(&quot;cuda&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;)</span></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net = NewNet()</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s use a Classification Cross-Entropy loss and SGD with momentum.</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h2 id="Build-Training-Model">Build Training Model</h2><p>Start writing the training model, and log the results to TensorBoard.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        <span class="comment"># inputs, labels = data, put the inputs and labels on the device (cpu or gpu) </span></span><br><span class="line">        inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        outputs.to(device)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.to(device)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">200</span> == <span class="number">199</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, <span class="subst">&#123;i + <span class="number">1</span>:5d&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">200</span>:<span class="number">.3</span>f&#125;</span> time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>)&#125;</span> min&#x27;</span>)</span><br><span class="line">            <span class="comment"># ...log the running loss</span></span><br><span class="line">            <span class="comment"># put the loss into tensorBoard, because it only writes every 200, so / 200 is the real loss </span></span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;training loss&#x27;</span>,</span><br><span class="line">                            running_loss / <span class="number">200</span>,</span><br><span class="line">                            epoch * <span class="built_in">len</span>(trainloader) + i)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate the total time for training, and round it to 1 decimal place </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Finished Training. Total elapsed time: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>, <span class="number">1</span>)&#125;</span> min&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,  <span class="number">3600</span>] loss: <span class="number">1.977</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">3800</span>] loss: <span class="number">2.021</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4000</span>] loss: <span class="number">1.933</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4200</span>] loss: <span class="number">1.922</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4400</span>] loss: <span class="number">1.902</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4600</span>] loss: <span class="number">1.836</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">4800</span>] loss: <span class="number">1.788</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>,  <span class="number">5000</span>] loss: <span class="number">1.818</span> <span class="built_in">time</span> elapsed: <span class="number">1</span> <span class="built_in">min</span></span><br><span class="line">...</span><br><span class="line">[<span class="number">1</span>, <span class="number">12000</span>] loss: <span class="number">1.479</span> <span class="built_in">time</span> elapsed: <span class="number">2</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">12200</span>] loss: <span class="number">1.469</span> <span class="built_in">time</span> elapsed: <span class="number">2</span> <span class="built_in">min</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">12400</span>] loss: <span class="number">1.485</span> <span class="built_in">time</span> elapsed: <span class="number">2</span> <span class="built_in">min</span></span><br><span class="line">Finished Training. Total elapsed <span class="built_in">time</span>: <span class="number">2.2</span> <span class="built_in">min</span></span><br></pre></td></tr></table></figure><p>Then you can write a cpu to compare the time</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use CPU </span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">net.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs; data is a list of [inputs, labels]</span></span><br><span class="line">        <span class="comment"># inputs, labels = data, put the inputs and labels on the device (cpu or gpu)</span></span><br><span class="line">        inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        outputs.to(device)</span><br><span class="line"></span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.to(device) </span><br><span class="line">        loss.backward() </span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">200</span> == <span class="number">199</span>:    <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, <span class="subst">&#123;i + <span class="number">1</span>:5d&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">2000</span>:<span class="number">.3</span>f&#125;</span> time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>)&#125;</span> min&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Finished Training. Total elapsed time: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>, <span class="number">1</span>)&#125;</span> min&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/GW04dzw.png" alt=""></p><h2 id="Save-Training-Results">Save Training Results</h2><p>I am currently saving the model in <code>./model/cifar_net.pth</code>, and then reading it back later, so that I don’t have to retrain next time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PATH = <span class="string">&#x27;./model/cifar_net.pth&#x27;</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line">net = NewNet()</span><br><span class="line">net.load_state_dict(torch.load(PATH)) <span class="comment"># load the weights from the saved file</span></span><br></pre></td></tr></table></figure><h2 id="Evaluate-the-Model-Using-Test-Data">Evaluate the Model Using Test Data</h2><div class="note info flat"><p><strong>Task 5. Modify the Criterion for Correctness to Include Predictions in the Top Three Outputs</strong>: Change the test metric as follows: A prediction is considered „correct“ if the true label is within the top three outputs of the network. Print the accuracy on the test data (with respect to this new definition).</p></div><p>According to the assignment requirements, we need to do the following:</p><ol><li>TODO 1 Adjust the definition of accuracy to consider a prediction correct if the answer is among the top three outputs.</li><li>TODO 2 Print the accuracy, here I print out the accuracy “for each category” and “overall accuracy.”</li><li>TODO 3 Since we need to record the wrong images, outputs, and labels, we first record them and then randomly select five wrong ones for later use.</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">class_correct = [<span class="number">0</span>] * <span class="built_in">len</span>(classes)  <span class="comment"># Used to record the number of correct predictions for each class</span></span><br><span class="line">class_total = [<span class="number">0</span>] * <span class="built_in">len</span>(classes)    <span class="comment"># Used to record the total number of samples for each class</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Used to store all the misclassified images, outputs, and labels</span></span><br><span class="line">all_errors = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Since we are not training, we don&#x27;t need to compute gradients of the outputs</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data </span><br><span class="line">        outputs = net(images) </span><br><span class="line"></span><br><span class="line">        <span class="comment"># We use _, predicted because we don&#x27;t need the values, but we need the indices of the results</span></span><br><span class="line">        _, predicted = torch.topk(outputs, <span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Since testloader is a batch, we need to loop through individual samples (in this case, 4 samples)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">            <span class="comment"># Example Print out =&gt; Predicted: tensor([3, 5, 2]) Actual: 3 Correct: True</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Predicted: <span class="subst">&#123;predicted[i]&#125;</span> Actual: <span class="subst">&#123;labels[i]&#125;</span> \t Correct: <span class="subst">&#123;labels[i] <span class="keyword">in</span> predicted[i]&#125;</span>&#x27;</span>)</span><br><span class="line">            total += <span class="number">1</span></span><br><span class="line">            <span class="comment"># Increment class_total for the class with key labels[i]</span></span><br><span class="line">            class_total[labels[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Check if labels[i] is in predicted[i] since labels has four values, we use i to access them</span></span><br><span class="line">            <span class="keyword">if</span> labels[i] <span class="keyword">in</span> predicted[i]:</span><br><span class="line">                correct += <span class="number">1</span></span><br><span class="line">                class_correct[labels[i]] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># Record the misclassified image, output, and label</span></span><br><span class="line">                all_errors.append((images[i], outputs[i], labels[i]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the accuracy for each class</span></span><br><span class="line">class_accuracies = [class_correct[i] / class_total[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate and print the new overall accuracy</span></span><br><span class="line">accuracy = correct / total</span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Predicted: tensor([3, 5, 2]) Actual: 3  Correct: True</span><br><span class="line">Predicted: tensor([8, 0, 1]) Actual: 8  Correct: True</span><br><span class="line">Predicted: tensor([8, 1, 9]) Actual: 8  Correct: True</span><br><span class="line">Predicted: tensor([8, 0, 1]) Actual: 0  Correct: True</span><br><span class="line">Predicted: tensor([4, 2, 6]) Actual: 6  Correct: True</span><br><span class="line">Predicted: tensor([6, 3, 5]) Actual: 6  Correct: True</span><br><span class="line">Predicted: tensor([1, 9, 5]) Actual: 1  Correct: True</span><br><span class="line">Predicted: tensor([2, 6, 4]) Actual: 6  Correct: True</span><br><span class="line">Predicted: tensor([3, 5, 2]) Actual: 3  Correct: True</span><br><span class="line">Predicted: tensor([1, 8, 9]) Actual: 1  Correct: True</span><br><span class="line">...</span><br><span class="line">Predicted: tensor([5, 7, 2]) Actual: 5  Correct: True</span><br><span class="line">Predicted: tensor([4, 2, 3]) Actual: 1  Correct: False</span><br><span class="line">Predicted: tensor([7, 4, 2]) Actual: 7  Correct: True</span><br></pre></td></tr></table></figure><h2 id="Print-Accuracy">Print Accuracy</h2><p>Now we can print the accuracy. We can see that the accuracy is 0.1 because we have only 10 classes, so the random guessing accuracy is 0.1.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test data (top-3): <span class="subst">&#123;<span class="number">100</span> * accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print each class accuracy</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(classes)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy for class <span class="subst">&#123;classes[i]&#125;</span>: <span class="subst">&#123;<span class="number">100</span> * class_accuracies[i]:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print the number of misclassified images</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Total misclassified images: <span class="subst">&#123;<span class="built_in">len</span>(all_errors)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Result</p></blockquote><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Accuracy on test data (top-3): 91.60%</span><br><span class="line">Accuracy for class plane: 89.50%</span><br><span class="line">Accuracy for class car: 96.30%</span><br><span class="line">Accuracy for class bird: 85.20%</span><br><span class="line">Accuracy for class cat: 91.80%</span><br><span class="line">Accuracy for class deer: 92.40%</span><br><span class="line">Accuracy for class dog: 91.40%</span><br><span class="line">Accuracy for class frog: 88.30%</span><br><span class="line">Accuracy for class horse: 91.50%</span><br><span class="line">Accuracy for class ship: 96.20%</span><br><span class="line">Accuracy for class truck: 93.40%</span><br><span class="line"></span><br><span class="line">Total misclassified images: 840</span><br></pre></td></tr></table></figure><h1 id="Task-6-Random-5-errors-img">Task 6 Random 5 errors img</h1><div class="note info flat"><p><strong>Task 6. Randomly select five examples that were incorrectly predicted by the model and display them in TensorBoard</strong>:<br>Randomly take 5 examples on which the network was wrong on the test data (according to the new definition of correct) and plot them to TensorBoard together with the true label.</p></div><h2 id="Setting-up-the-Image-Transformation-Function">Setting up the Image Transformation Function</h2><p>In order to display the images later, we need to create a function for displaying images. Since the output of torchvision datasets is PILImage images with a range of [0, 1], we need to convert them to tensors with a normalized range of [-1, 1]. If we want to display the images, we need to perform the reverse normalization to go from the normalized range [-1, 1] back to [0, 1]. We can achieve this using the formula <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>x</mi><mn>2</mn></mfrac><mo>+</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\frac{x}{2} + 0.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0404em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.5</span></span></span></span>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Functions to show an image</span></span><br><span class="line"><span class="comment"># If one_channel is True, the function assumes the input image is single-channel (usually grayscale) and displays it using a grayscale colormap.</span></span><br><span class="line"><span class="comment"># If one_channel is False, the function assumes the input image is three-channel (usually color) and displays it using a color colormap.</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplotlib_imshow</span>(<span class="params">img, one_channel=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> one_channel:</span><br><span class="line">        img = img.mean(dim=<span class="number">0</span>)</span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    <span class="keyword">if</span> one_channel: </span><br><span class="line">        plt.imshow(npimg, cmap=<span class="string">&quot;Greys&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Because the function used by matplotlib expects input as (height_1, depth_2, width_0)</span></span><br><span class="line">        <span class="comment"># while npimg defaults to (width_0, height_1, depth=RGB color_2)</span></span><br><span class="line">        <span class="comment"># We need to use np.transpose to change the channel order from (width_0, height_1, depth=RGB color_2) to (height_1, depth_2, width_0)</span></span><br><span class="line">        plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br></pre></td></tr></table></figure><h2 id="Randomly-Select-5-Errors">Randomly Select 5 Errors</h2><p>We have just collected all the images, predictions, and labels for errors. According to the task requirements, we need to randomly select 5 images with incorrect predictions and print them out.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_classes_preds</span>(<span class="params">all_errors</span>):</span><br><span class="line">    <span class="comment"># Randomly select five misclassified images</span></span><br><span class="line">    random_errors = random.sample(all_errors, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a large matplotlib figure, the figsize parameter is used to specify the width and height of the figure in inches</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">12</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, (image, output, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(random_errors):</span><br><span class="line">        <span class="comment"># Parameters indicate: number of rows, number of columns, subplot index (starting from 1, placing five images in a 12-inch by 18-inch figure)</span></span><br><span class="line">        <span class="comment"># xticks and yticks are used to set coordinate parameters; if you don&#x27;t want to display coordinates, you can set them as empty lists</span></span><br><span class="line">        ax = fig.add_subplot(<span class="number">1</span>, <span class="number">5</span>, idx+<span class="number">1</span>, xticks=[], yticks=[])</span><br><span class="line">        <span class="comment"># Display the color image</span></span><br><span class="line">        matplotlib_imshow(image, one_channel=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Since output is a 1x10 tensor, we use dim=0 to extract the top 3 columns</span></span><br><span class="line">        preds = torch.topk(output, <span class="number">3</span>, dim=<span class="number">0</span>).indices  <span class="comment"># See the explanation regarding dim in the notes</span></span><br><span class="line">        pred_classes = [classes[p] <span class="keyword">for</span> p <span class="keyword">in</span> preds] <span class="comment"># Convert indices to class names and store them in a list (there will be three strings)</span></span><br><span class="line">        <span class="comment"># Set the title for the current image, displaying the predicted class and the actual class</span></span><br><span class="line">        ax.set_title(<span class="string">&quot;\n(label: &#123;0&#125;)\n(&#123;1&#125;)&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            classes[label],</span><br><span class="line">            <span class="string">&quot;, &quot;</span>.join(pred_classes)),</span><br><span class="line">            color=<span class="string">&quot;red&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Finally, return the entire set of prepared figures (there will be a total of 5 images)</span></span><br><span class="line">    <span class="keyword">return</span> fig</span><br><span class="line"></span><br><span class="line"><span class="comment"># Call the function</span></span><br><span class="line">plot_classes_preds(all_errors)</span><br></pre></td></tr></table></figure><blockquote><p>Result<br><img src="https://i.imgur.com/OY6EvGW.png" alt=""></p></blockquote><h2 id="Put-the-image-into-tensorBoard">Put the image into tensorBoard</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># put on tensorBoard</span></span><br><span class="line">fig = plot_classes_preds(all_errors)</span><br><span class="line">writer.add_figure(<span class="string">&quot;predictions vs. actuals&quot;</span>, fig)</span><br></pre></td></tr></table></figure><h1 id="Task-7-Show-tensorBoard-on-notebook">Task 7 Show tensorBoard on notebook</h1><div class="note info flat"><ol start="7"><li><strong>Display TensorBoard in the notebook</strong>: Show the TensorBoard widget at the end of your notebook.</li></ol></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Displaying TensorBoard in the notebook</span></span><br><span class="line">%load_ext tensorboard <span class="comment"># This line of code loads the TensorBoard extension so that you can run TensorBoard within the notebook.</span></span><br><span class="line">%tensorboard --logdir board <span class="comment"># This line of code starts TensorBoard and points it to your log folder.</span></span><br></pre></td></tr></table></figure><h1 id="Supplementary-Information">Supplementary Information</h1><h2 id="Normalization-vs-Standardization">Normalization vs. Standardization</h2><ul><li>Ref: <a href="https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/preprocessing-data-%E6%95%B8%E6%93%9A%E7%89%B9%E5%BE%B5%E6%A8%99%E6%BA%96%E5%8C%96%E5%92%8C%E6%AD%B8%E4%B8%80%E5%8C%96-9bd3e5a8f2fc">Preprocessing Data: Data Feature Normalization and Standardization</a></li></ul><blockquote><p><strong>Normalization vs. Standardization: What’s the Difference?</strong></p></blockquote><ul><li><code>Normalization</code>: Scaling data proportionally to fit into a small specific range, such as [0, 1] or [-1, 1].<ul><li>Formula: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{x_i - min(x_i)}{max(x_i) - min(x_i)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">x</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">min</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">min</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li></ul></li><li><code>Standardization</code>: Scaling data proportionally to fit into a distribution with a mean of 0 and a standard deviation of 1, so extreme values may not fall within [0, 1].<ul><li>Formula: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>μ</mi></mrow><mrow><mi>s</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{x_i - \mu}{sd(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3744em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8544em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">d</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">μ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li></ul></li></ul><blockquote><p><strong>Common Standards for Both</strong></p></blockquote><ul><li>Both techniques scale individual features (columns) and not the feature vectors of individual samples (rows).</li></ul><blockquote><p><strong>Why Normalize Data?</strong></p></blockquote><ol><li><strong>Improved Precision</strong>: Many machine learning algorithms are based on objective functions that assume all features have zero mean and the same order of magnitude for variances. If the variance of a feature is orders of magnitude larger than that of other features, it will dominate the learning algorithm and prevent it from learning correctly. Therefore, normalization is done to <strong>make different dimensions of features comparable</strong>, significantly improving the classifier’s accuracy.</li><li><strong>Faster Convergence</strong>: After normalization, the process of finding the optimal solution is noticeably smoother, making it easier to converge to the optimal solution.</li></ol><h2 id="dim">dim?</h2><p>The <code>dim</code> parameter in PyTorch determines the dimension along which ranking and obtaining the maximum values occur. Let’s explain the differences using an example:</p><ul><li>If you set <code>dim=0</code>, it will look at the maximum values for the entire column.</li><li>If you set <code>dim=1</code>, it will look at the maximum values for the entire row.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an example tensor</span></span><br><span class="line">output = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.9</span>, <span class="number">0.5</span>, <span class="number">0.3</span>],</span><br><span class="line">                       [<span class="number">0.4</span>, <span class="number">0.1</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.2</span>],</span><br><span class="line">                       [<span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the top 3 maximum values and their indices per column</span></span><br><span class="line">top_values_col, top_indices_col = torch.topk(output, <span class="number">3</span>, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the top 3 maximum values and their indices per row</span></span><br><span class="line">top_values_row, top_indices_row = torch.topk(output, <span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Output tensor:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Top 3 values and indices per column:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(top_values_col)</span><br><span class="line"><span class="built_in">print</span>(top_indices_col)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Top 3 values and indices per row:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(top_values_row)</span><br><span class="line"><span class="built_in">print</span>(top_indices_row)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Output tensor:</span><br><span class="line">tensor([[<span class="number">0.2000</span>, <span class="number">0.6000</span>, <span class="number">0.9000</span>, <span class="number">0.5000</span>, <span class="number">0.3000</span>],</span><br><span class="line">        [<span class="number">0.4000</span>, <span class="number">0.1000</span>, <span class="number">0.8000</span>, <span class="number">0.7000</span>, <span class="number">0.2000</span>],</span><br><span class="line">        [<span class="number">0.5000</span>, <span class="number">0.8000</span>, <span class="number">0.9000</span>, <span class="number">0.1000</span>, <span class="number">0.2000</span>]])</span><br><span class="line">Top <span class="number">3</span> values <span class="keyword">and</span> indices per column:</span><br><span class="line"><span class="comment"># Already sorted from largest to smallest</span></span><br><span class="line">tensor([[<span class="number">0.5000</span>, <span class="number">0.8000</span>, <span class="number">0.9000</span>, <span class="number">0.7000</span>, <span class="number">0.3000</span>],</span><br><span class="line">        [<span class="number">0.4000</span>, <span class="number">0.6000</span>, <span class="number">0.9000</span>, <span class="number">0.5000</span>, <span class="number">0.2000</span>],</span><br><span class="line">        [<span class="number">0.2000</span>, <span class="number">0.1000</span>, <span class="number">0.8000</span>, <span class="number">0.1000</span>, <span class="number">0.2000</span>]])</span><br><span class="line"><span class="comment"># Printing the indices in each column from largest to smallest. For example, in the first column (0.2, 0.4, 0.5), 0.5 is the largest,</span></span><br><span class="line"><span class="comment"># so the indices in order are 2, 1, 0. This is why you see [[2...], [1...], [0...]] in the output.</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], </span><br><span class="line">        [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">Top <span class="number">3</span> values <span class="keyword">and</span> indices per row:</span><br><span class="line"><span class="comment"># Already sorted from largest to smallest</span></span><br><span class="line">tensor([[<span class="number">0.9000</span>, <span class="number">0.6000</span>, <span class="number">0.5000</span>],</span><br><span class="line">        [<span class="number">0.8000</span>, <span class="number">0.7000</span>, <span class="number">0.4000</span>],</span><br><span class="line">        [<span class="number">0.9000</span>, <span class="number">0.8000</span>, <span class="number">0.5000</span>]])</span><br><span class="line"><span class="comment"># Printing the indices in each row from largest to smallest. For example, in the first row (0.2, 0.6, 0.9, 0.5, 0.3),</span></span><br><span class="line"><span class="comment"># 0.9 is the largest, so the indices in order are 2, 1, 3. This is why you see [[2, 1, 3] in the output.</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Mechine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mechine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting</title>
      <link href="/en/posts/pytorch-titanic-nn/"/>
      <url>/en/posts/pytorch-titanic-nn/</url>
      
        <content type="html"><![CDATA[<h1 id="Reference">Reference</h1><ul><li>Ref: <a href="https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8">A Detailed Explanation of the Titanic Dataset Structure</a></li></ul><h1 id="Introduction">Introduction</h1><p>Recently, I enrolled in an AI course that included an assignment to build a Neural Network and use the Titanic Dataset for training. The task was to implement overfitting by increasing hidden layers and neurons and then mitigate overfitting using dropout or other methods.</p><p>This article documents the process of completing the assignment.</p><h1 id="Environment-Setup-and-Assignment-Requirements">Environment Setup and Assignment Requirements</h1><blockquote><p>Environment Setup:</p><ul><li>Python 3.10.9</li><li>PyTorch 2.0.1</li></ul></blockquote><h2 id="Assignment-Requirements">Assignment Requirements</h2><ol><li>Write a custom <strong>dataset class for the Titanic data</strong> (see the data folder on <a href="https://github.com/pabair/ki-lab-ss23">GitHub</a>). Use only the features: “Pclass,” “Age,” “SibSp,” “Parch,” “Fare,” “Sex,” and “Embarked.” <strong>Preprocess the features</strong> accordingly in that class (scaling, one-hot-encoding, etc.), and split the data into train and validation data (80% and 20%). The constructor of that class should look like this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titanic_train = TitanicDataSet(<span class="string">&#x27;titanic.csv&#x27;</span>, train=<span class="literal">True</span>)</span><br><span class="line">titanic_val = TitanicDataSet(<span class="string">&#x27;titanic.csv&#x27;</span>, train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure></li><li>Build a neural network with <strong>one hidden layer of size 3</strong> that predicts the survival of the passengers. Use a <strong>BCE loss</strong> (Hint: you need a <strong>sigmoid activation</strong> in the output layer). Use a data loader to train in batches of <strong>size 16</strong> and shuffle the data.</li><li><strong>Evaluate the performance</strong> of the model on the validation data using accuracy as a metric.</li><li><strong>Create the following plot</strong> that was introduced in the lecture.<ul><li><img src="https://i.imgur.com/yPNS7vc.png?x300" alt=""></li></ul></li><li>Increase the complexity of the network by <strong>adding more layers and neurons</strong> and see if you can overfit on the training data.</li><li>Try to remove overfitting by introducing a <strong><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">dropout</a> layer</strong>.</li></ol><h2 id="In-Simple-Terms">In Simple Terms</h2><p>In simple terms, we will satisfy the above requirements through the following four steps:</p><ol><li><p><strong>Data Preprocessing</strong></p><ul><li><code>Task 1</code>: Build a class and import Titanic data.</li><li><code>Task 1</code>: Select specific columns as training features.</li><li><code>Task 1</code>: Data preprocessing (scaling, one-hot encoding, etc.) to convert non-numeric columns like “Sex” or “Embarked” into numeric values.</li><li><code>Task 1</code>: Split the data into train and validation data (80% and 20%).</li><li><code>Task 1</code>: Create a class and import the data.</li></ul></li><li><p><strong>Build a Neural Network</strong></p><ul><li><code>Task 2</code>: Build a three-layer network (1 input layer + 1 hidden layer + 1 output layer).</li><li><code>Task 2</code>: The size of the first hidden layer is 3.</li><li><code>Task 2</code>: Use BCE loss as the loss function.</li><li><code>Task 2</code>: Use sigmoid activation as the output layer’s activation function.</li></ul></li><li><p><strong>Model Training</strong></p><ul><li><code>Task 3</code>: Start training the model and record accuracy at each step.</li></ul></li><li><p><strong>Generate Results</strong></p><ul><li><code>Task 4</code>: Generate results and create plots.</li></ul></li><li><p><strong>Create Overfitting</strong></p><ul><li><code>Task 5</code>: Increase hidden layers and neurons to induce overfitting.</li></ul></li><li><p><strong>Use Dropout</strong></p><ul><li><code>Task 6</code>: Use dropout or other methods to mitigate the impact of overfitting.</li></ul></li></ol><h1 id="Step-1-Data-Preprocessing">Step 1: Data Preprocessing</h1><div class="note info flat"><p>Let’s start with <strong>data preprocessing</strong>:</p><ul><li><code>Task 1</code>: Build a class and import Titanic data.</li><li><code>Task 1</code>: Select specific columns as training features.</li><li><code>Task 1</code>: Data preprocessing (scaling, one-hot encoding, etc.) to convert non-numeric columns like “Sex” or “Embarked” into numeric values.</li><li><code>Task 1</code>: Split the data into train data and validation data (80% and 20%).</li><li><code>Task 1</code>: Create a class and import the data.</li></ul></div><h2 id="1-1-Data-Preprocessing">1.1 Data Preprocessing</h2><ol><li><p>First, we’ll import all the necessary packages.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># data process </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot </span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># neural network </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure></li><li><p>Before we start, I’d like to place all the parameters we’ll use at the top for easier modification:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shared variables</span></span><br><span class="line">D_in, D_out = <span class="number">10</span>, <span class="number">1</span> </span><br><span class="line">num_epochs = <span class="number">250</span> </span><br><span class="line">log_interval = <span class="number">100</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size: the amount of data for each training iteration</span></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate: Since we will create two different networks, we set two different learning rates</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">multi_learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hidden layers</span></span><br><span class="line">multi_num_layers = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hidden neurons: Since we will create two different networks, we set two different numbers of hidden neurons</span></span><br><span class="line">neurons = <span class="number">3</span> </span><br><span class="line">multi_neurons = <span class="number">1024</span> </span><br></pre></td></tr></table></figure></li><li><p>Next, we’ll create a class based on the requirements and load the Titanic data, returning features:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialization function for loading and preprocessing data</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, train=<span class="literal">True</span>, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># The train parameter indicates whether it&#x27;s training data or test data</span></span><br><span class="line">        self.train = train</span><br><span class="line">        <span class="comment"># The transform parameter is used to define a transformation function if needed</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create MinMaxScaler and OneHotEncoder for data preprocessing</span></span><br><span class="line">        minmax_scaler = MinMaxScaler()</span><br><span class="line">        onehot_enc = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Read the Titanic data from the CSV file</span></span><br><span class="line">        titanic = pd.read_csv(root_dir)</span><br><span class="line">        <span class="comment"># Select specific columns from the data</span></span><br><span class="line">        titanic = titanic[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Embarked&quot;</span>, <span class="string">&quot;Survived&quot;</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fill missing values in the &quot;Age&quot; column with the mean and drop rows with missing values</span></span><br><span class="line">        titanic[<span class="string">&quot;Age&quot;</span>] = titanic[<span class="string">&quot;Age&quot;</span>].fillna(titanic[<span class="string">&quot;Age&quot;</span>].mean())</span><br><span class="line">        titanic = titanic.dropna()</span><br><span class="line">        titanic = titanic.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the data into categorical features, numerical features, and labels</span></span><br><span class="line">        categorical_features = titanic[titanic.select_dtypes(include=[<span class="string">&#x27;object&#x27;</span>]).columns.tolist()]</span><br><span class="line">        numerical_features = titanic[titanic.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>]).columns].drop(<span class="string">&#x27;Survived&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">        label_features = titanic[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize numerical features (MinMax scaling)</span></span><br><span class="line">        numerical_features_arr = minmax_scaler.fit_transform(numerical_features)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one-hot encoding on categorical features</span></span><br><span class="line">        categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Merge the normalized numerical features and one-hot encoded categorical features into one dataset</span></span><br><span class="line">        combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)</span><br><span class="line">        combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=<span class="number">1</span>)</span><br><span class="line">        combined_features = pd.concat([combined_features, label_features], axis=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the dataset into training and test sets</span></span><br><span class="line">        train_data, test_data = train_test_split(combined_features, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Choose the data to use based on training or testing mode</span></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            self.data = train_data</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data = test_data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the length of the dataset</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Function for training the neural network, returns features and labels</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Get the data from the self.data DataFrame for the idx-th row</span></span><br><span class="line">        sample = self.data.iloc[idx]</span><br><span class="line">        <span class="comment"># Convert a data structure to a PyTorch tensor and specify the data type as float</span></span><br><span class="line">        features = torch.FloatTensor(sample[:-<span class="number">1</span>])</span><br><span class="line">        label = torch.FloatTensor([sample[<span class="string">&#x27;Survived&#x27;</span>]])</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            features = self.transform(features)</span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the entire dataset as a DataFrame</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data</span><br></pre></td></tr></table></figure></li><li><p>After writing the function, you can use the following commands to test it:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">titanic_train = TitanicDataset(<span class="string">&#x27;./data/titanic.csv&#x27;</span>, train=<span class="literal">True</span>)</span><br><span class="line">titanic_val = TitanicDataset(<span class="string">&#x27;./data/titanic.csv&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;val_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_val))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;total_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_train) + <span class="built_in">len</span>(titanic_val))</span><br><span class="line"><span class="comment"># Output </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">train_dataset len: 711</span></span><br><span class="line"><span class="string">val_dataset len: 178</span></span><br><span class="line"><span class="string">total_dataset len: 889</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p>可以透過以下程式，列印出以下結果： Or using the following code to print the following results:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic_val.getData()</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://i.imgur.com/d2C0wTF.png" alt=""></p><h1 id="Step2-Building-the-Neural-Network">Step2. Building the Neural Network</h1><ol><li>Next, we will construct the following Neural Network, primarily performing the following tasks:<ul><li><code>__init__</code>: Create a three-layer network (1 input layer + 1 hidden layer + 1 output layer).<ul><li><code>D_in</code>: Neurons size of the input layer.</li><li><code>H</code>: Neurons size of the hidden layer.</li><li><code>D_out</code>: Neurons size of the output layer.</li></ul></li><li><code>forward</code>: The place for performing the forward pass, primarily performing the linear transformation of the first layer with the <code>relu</code> activation function, the linear transformation of the second layer with the <code>sigmoid</code> activation function, and finally returning the prediction.</li></ul> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as member variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TwoLayerNet, self).__init__()</span><br><span class="line">        <span class="comment"># the weight and bias of linear1 will be initialized </span></span><br><span class="line">        <span class="comment"># you can access them by self.linear1.weight and self.linear1.bias</span></span><br><span class="line">        self.linear1 = nn.Linear(D_in, H) <span class="comment"># this will create weight, bias for linear1</span></span><br><span class="line">        self.linear2 = nn.Linear(H, D_out) <span class="comment"># this will create weight, bias for linear2</span></span><br><span class="line">        self.sigmoid = nn.Sigmoid() <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.</span></span><br><span class="line"><span class="string">        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h_relu = F.relu(self.linear1(x))</span><br><span class="line">        y_pred = self.sigmoid(self.linear2(h_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></li><li>Before training the model, we need to build the neural network. The following code sets the batch size to 16, which means we train with 16 samples at a time. We train on all 889 data points in a single epoch. The input layer has 10 neurons, the hidden layer has 3 neurons, the output layer has 1 neuron, and the learning rate is set to 0.001. We train for a total of 500 epochs.<ul><li>We create the network to construct the neural network.</li><li>We use Adam as the optimizer for gradient descent updates.</li><li>We use Binary Cross-Entropy Loss as the loss function.</li></ul> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">16</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">log_interval = <span class="number">100</span> <span class="comment"># Print the training status every log_interval epoch</span></span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(D_in, H, D_out)  <span class="comment"># H=3 for one hidden layer with 3 neurons</span></span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr)</span><br><span class="line">criterion = nn.BCELoss() <span class="comment"># Define the loss function as Binary Cross-Entropy Loss</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="Step3-Model-Training">Step3. Model Training</h1><ol><li><p>You can start by creating lists to keep track of the loss and accuracy for each training loop (epoch) of the neural network model during the training process:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_losses = []  <span class="comment"># Save the loss value of each training loop (epoch) of the neural network model during the training process</span></span><br><span class="line">train_counter = []  <span class="comment"># Save the number of images for training so far</span></span><br><span class="line">test_losses = []   <span class="comment"># Save the loss value of each test loop (epoch) of the neural network model during the training process</span></span><br><span class="line">test_counter = [i * <span class="built_in">len</span>(titanic_train) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs + <span class="number">1</span>)]  <span class="comment"># how many data for training so far</span></span><br></pre></td></tr></table></figure></li><li><p>Create a training function with the main purpose of training the model using the train dataset:</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):  <span class="comment"># Indicates the current epoch being run</span></span><br><span class="line">    network.train()  <span class="comment"># Use the network created in the previous step for training</span></span><br><span class="line">    correct = <span class="number">0</span>  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">    cur_count = <span class="number">0</span>  <span class="comment"># Record how many data points have been trained so far</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># Clear the gradient to start fresh for each batch because the gradient is updated after each batch</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        output = network(data)  <span class="comment"># Feed the data into the network for forward propagation</span></span><br><span class="line">        loss = criterion(output, target)  <span class="comment"># Calculate the loss</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># Since the answers are either 0 or 1, we need to set a threshold where &gt;= 0.5 is 1 and &lt; 0.5 is 0</span></span><br><span class="line">        correct += (pred == target).<span class="built_in">sum</span>().item()  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)  <span class="comment"># Record how many data points have been trained so far</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        loss.backward()  <span class="comment"># Calculate the gradient of the loss</span></span><br><span class="line">        optimizer.step()  <span class="comment"># Update the gradient</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:  <span class="comment"># Print the training status every log_interval</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;\t Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch,</span><br><span class="line">                cur_count,</span><br><span class="line">                <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * cur_count / <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                loss.item(),</span><br><span class="line">                correct, <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * correct / <span class="built_in">len</span>(train_dataloader.dataset))</span><br><span class="line">            )</span><br><span class="line">            train_losses.append(loss.item())</span><br><span class="line">            train_counter.append((batch_idx * <span class="number">16</span>) + ((epoch - <span class="number">1</span>) * <span class="built_in">len</span>(train_dataloader.dataset)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the current accuracy</span></span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(train_dataloader.dataset)</span><br></pre></td></tr></table></figure></li><li><p>Constructing a test function, the main purpose is to test the trained model through validation dataset to see how accurate the model is when it is trained to detect unknown data.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    network.<span class="built_in">eval</span>()  <span class="comment"># Use the network created in the previous step and indicate that it&#x27;s for evaluation</span></span><br><span class="line">    test_loss = <span class="number">0</span>  <span class="comment"># Record the current loss</span></span><br><span class="line">    correct = <span class="number">0</span>  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># We don&#x27;t need to calculate gradients for evaluation, so we can use torch.no_grad() for speedup</span></span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_dataloader:  <span class="comment"># Get data through the test_dataloader</span></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            output = network(data)  <span class="comment"># Feed the data into the trained network for forward propagation</span></span><br><span class="line">            test_loss += criterion(output, target).item()  <span class="comment"># Calculate the loss</span></span><br><span class="line">            <span class="comment"># Accuracy</span></span><br><span class="line">            pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># 0.5 is the threshold</span></span><br><span class="line">            correct += (pred == target).<span class="built_in">sum</span>().item()  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_dataloader.dataset)  <span class="comment"># Calculate the average loss</span></span><br><span class="line">    test_losses.append(test_loss)  <span class="comment"># Add the current loss to the list</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss,</span><br><span class="line">        correct,</span><br><span class="line">        <span class="built_in">len</span>(test_dataloader.dataset),</span><br><span class="line">        <span class="number">1.</span> * correct / <span class="built_in">len</span>(test_dataloader.dataset))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(test_dataloader.dataset)  <span class="comment"># Return the current accuracy</span></span><br></pre></td></tr></table></figure></li><li><p>Finally, we can train the model according to the number of epochs, and check the training status by <code>test()</code> after each epoch.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test()</span><br><span class="line">train_accuracy_list = []</span><br><span class="line">test_accuracy_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>): <span class="comment"># Indicates the current epoch being run</span></span><br><span class="line">    train_accuracy_list.append(train(epoch)) <span class="comment"># After each epoch, we use the train() function to train the model</span></span><br><span class="line">    test_accuracy_list.append(test()) <span class="comment"># After each epoch, we use the test() function to test the model</span></span><br></pre></td></tr></table></figure></li></ol><h1 id="Step4-Generate-Results">Step4. Generate Results</h1><p>Finally, we can generate the results by using the following commands:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(train_accuracy_list, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.plot(test_accuracy_list, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="comment"># plt.ylim(0.5, 1)</span></span><br><span class="line">plt.legend([<span class="string">&#x27;Train Accuracy&#x27;</span>, <span class="string">&#x27;Test Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Train Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Test Accuracy&#x27;</span>], loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/8yHAk60.png" alt=""></p><h1 id="Step5-Make-Overfitting">Step5. Make Overfitting</h1><div class="note info flat"><p>Creating overfitting can mainly be achieved through a few methods:</p><ol><li><strong>Increasing the number of epochs</strong> can lead to some overfitting.</li><li><strong>Increasing the number of hidden layers</strong> or <strong>increasing the neuron size</strong> can also result in some overfitting.</li></ol></div><p>Since the task requires increasing the number of hidden layers and increasing the neuron size, let’s give it a try! The simplest way is to increase the number of hidden layers, increase the neuron size, and also increase the number of epochs to observe overfitting.</p><ol><li>Create a MultiLayerNet and increase the number of hidden layers and neuron size. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = <span class="number">128</span></span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, H)</span><br><span class="line">        self.linear1 = nn.Linear(H, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid() <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_relu = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        y_relu = F.relu(self.linear1(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear2(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear3(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear4(y_relu))</span><br><span class="line">        y_pred = self.sigmoid(self.output(y_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></li></ol><div class="note danger flat"><p>Note: If you simply add layers, you may not see much learning effect, and you will always see a flat line… and the accuracy won’t improve! Later, a classmate found that decreasing the number of neurons gradually would lead to better learning. So, we can set the neurons as 128, 64, 32, 16!<br>A classmate said: “It’s like an hourglass,” filtering out unimportant information step by step and leaving behind the important information!</p></div><ol><li><p>Create new <code>multi_train()</code> and <code>test_multi()</code> functions for the multi-network.</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_multi</span>(<span class="params">epoch</span>):</span><br><span class="line">    multi_network.train() <span class="comment"># Use the network created in the previous step for training</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    cur_count = <span class="number">0</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        multi_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward propagation</span></span><br><span class="line">        <span class="comment"># You will find that we use multi_network here for forward propagation</span></span><br><span class="line">        output = multi_network(data) </span><br><span class="line">        loss = multi_criterion(output, target) </span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># survival_rate is the threshold</span></span><br><span class="line">        correct += (pred == target).<span class="built_in">sum</span>().item()</span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backword propagation</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        multi_optimizer.step()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Muti Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;\t Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, </span><br><span class="line">                cur_count, </span><br><span class="line">                <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * cur_count / <span class="built_in">len</span>(train_dataloader.dataset), </span><br><span class="line">                loss.item(), </span><br><span class="line">                correct, <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * correct / <span class="built_in">len</span>(train_dataloader.dataset))</span><br><span class="line">            )</span><br><span class="line">            train_losses.append(loss.item())</span><br><span class="line">            train_counter.append((batch_idx*<span class="number">16</span>) + ((epoch-<span class="number">1</span>)*<span class="built_in">len</span>(train_dataloader.dataset)))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(train_dataloader.dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_multi</span>():</span><br><span class="line">    multi_network.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            output = multi_network(data)</span><br><span class="line">            test_loss += multi_criterion(output, target).item()</span><br><span class="line">            <span class="comment"># Accuracy</span></span><br><span class="line">            pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># 0.5 is the threshold</span></span><br><span class="line">            correct += (pred == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_dataloader.dataset)</span><br><span class="line">    test_losses.append(test_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nMulti Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, </span><br><span class="line">        correct, </span><br><span class="line">        <span class="built_in">len</span>(test_dataloader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_dataloader.dataset))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(test_dataloader.dataset)</span><br></pre></td></tr></table></figure></li><li><p>Retrain model</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_multi()</span><br><span class="line"></span><br><span class="line">multi_train_accuracy_list = []</span><br><span class="line">multi_test_accuracy_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">    multi_train_accuracy_list.append(train_multi(epoch))</span><br><span class="line">    multi_test_accuracy_list.append(test_multi())</span><br></pre></td></tr></table></figure></li><li><p>Draw the plot again: You can try to increase the number of epochs to 500, and you will see the overfitting phenomenon!</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(multi_train_accuracy_list, color=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line">plt.plot(multi_test_accuracy_list, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.ylim(<span class="number">0.5</span>, <span class="number">0.9</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train Accuracy&#x27;</span>, <span class="string">&#x27;Test Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Train Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Test Accuracy&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure></li></ol><p><img src="https://i.imgur.com/eVxVkM6.png" alt=""></p><h2 id="Advanced-Version">Advanced Version</h2><p>If you wish to dynamically adjust the number of neurons and hidden layers, you can use the following approach:</p><ul><li><code>neurons</code>: The initial number of neurons. If set to 1024, it will decrease from 1024 to 16, dividing by 2 each time, until the number of neurons is less than 16.</li><li><code>num_layers</code>: The number of hidden layers.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">neurons = <span class="number">1024</span> </span><br><span class="line">num_layers = <span class="number">5</span> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, D_out, neurons, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = neurons</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, neurons)</span><br><span class="line">        self.linears = nn.ModuleList()  <span class="comment"># Note that if you want to create multiple layers with for loops, you have to use nn.ModuleList() to create</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.linears.append(nn.Linear(neurons, <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>)))</span><br><span class="line">            neurons = <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>) </span><br><span class="line">        self.output = nn.Linear(neurons, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()  <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linears:</span><br><span class="line">            y = F.relu(layer(y))</span><br><span class="line">        y_pred = self.sigmoid(self.output(y))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure><h1 id="Step6-Use-Dropout">Step6. Use Dropout</h1><p>Here we can use dropout to avoid overfitting, mainly by randomly turning off some neurons during forward propagation, so as to avoid overfitting.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out, num_layers, dropout_prob</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, H)</span><br><span class="line">        self.linear1 = nn.Linear(H, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout_prob)  <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_relu = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        y_relu = F.relu(self.linear1(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear2(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear3(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear4(y_relu))</span><br><span class="line">        y_relu = self.dropout(y_relu)  <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">        y_pred = self.sigmoid(self.output(y_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>At this point, you will find that the overfitting phenomenon is not so serious! Here are the results when the epoch number is set to 200.</p><blockquote><p>Without Dropout<br><img src="https://i.imgur.com/BxmZNLL.png" alt=""></p></blockquote><blockquote><p>With Dropout<br><img src="https://i.imgur.com/wj5vFvt.png" alt=""></p></blockquote><h2 id="Advanced">Advanced</h2><p>The difference in the advanced version is that the number of dropout layers is the same as the number of hidden layers, and the number of dropout layers decreases with the number of hidden layers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, D_out, neurons, num_layers, dropout_prob=<span class="number">0.8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = neurons</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, neurons)</span><br><span class="line">        self.linears = nn.ModuleList()  </span><br><span class="line">        self.dropouts = nn.ModuleList() <span class="comment">#  ======&gt; dropout layer</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.linears.append(nn.Linear(neurons, <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>)))</span><br><span class="line">            self.dropouts.append(nn.Dropout(p=dropout_prob)) <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">            neurons = <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>) </span><br><span class="line">        self.output = nn.Linear(neurons, D_out) <span class="comment"># output layer</span></span><br><span class="line">        self.sigmoid = nn.Sigmoid()  <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        <span class="keyword">for</span> layer, dropout <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, self.dropouts):</span><br><span class="line">            y = F.relu(layer(y))</span><br><span class="line">            y = dropout(y) <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">        y_pred = self.sigmoid(self.output(y))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MAC OS -  PyTorch on Mac OS with GPU support</title>
      <link href="/en/posts/PyTorch-Mac-GPU/"/>
      <url>/en/posts/PyTorch-Mac-GPU/</url>
      
        <content type="html"><![CDATA[<h1 id="References">References</h1><ul><li><a href="https://blog.csdn.net/DaydreamHippo/article/details/128094886">Installing GPU-supported PyTorch and TensorFlow on Mac M1/M2</a></li><li><a href="https://developer.apple.com/metal/pytorch/">Accelerated PyTorch training on Mac</a></li></ul><h1 id="Enabling-GPU-on-Mac-OS-for-PyTorch">Enabling GPU on Mac OS for PyTorch</h1><ol><li>Since I personally reinstalled GPU-supported PyTorch based on Anaconda, you can check whether Conda is installed by using the command <code>conda --version</code>. If it is installed, the output should confirm its presence. If not, you can download it from the <a href="https://www.anaconda.com/products/distribution">Anaconda official website</a>.</li><li>(Optional) If you want to create a separate environment specifically for Python with GPU support, you can use the following command:</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create an environment named &#x27;torch-gpu&#x27; using Python version 3.10.9</span></span><br><span class="line">conda create -n torch-gpu python=3.10.9</span><br><span class="line"></span><br><span class="line"><span class="comment"># Activate the created environment</span></span><br><span class="line">conda activate torch-gpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># List all environments to verify the &#x27;torch-gpu&#x27; environment is created</span></span><br><span class="line">conda <span class="built_in">env</span> list</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check if the current Python version matches the Python version in the activated environment</span></span><br><span class="line">python --version</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="31"><li><p>According to the <a href="https://pytorch.org/">PyTorch official website</a>, choose the corresponding version and copy the installation command.<br><img src="https://i.imgur.com/i89otBb.png" alt=""></p></li><li><p>You can use a simple Python script to verify MPS (Multi-Process Service) support:</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">if</span> torch.backends.mps.is_available():</span><br><span class="line">    mps_device = torch.device(<span class="string">&quot;mps&quot;</span>)</span><br><span class="line">    x = torch.ones(<span class="number">1</span>, device=mps_device)</span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;MPS device not found.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Or</span></span><br><span class="line"><span class="built_in">print</span>(torch.backends.mps.is_available()) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(torch.backends.mps.is_built()) <span class="comment"># True</span></span><br></pre></td></tr></table></figure><div class="note warning flat"><p>It’s important to note that if you’re using MPS on macOS, you should specify it as follows: <code>device = torch.device(&quot;mps)</code>.</p></div>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
          <category> Mechine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mechine Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
