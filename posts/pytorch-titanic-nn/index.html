<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting | Shannon's Blog üêü Tech | Life | Travel</title><meta name="author" content="Shannon Hung"><meta name="copyright" content="Shannon Hung"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Reference  Ref: A Detailed Explanation of the Titanic Dataset Structure  Introduction Recently, I enrolled in an AI course that included an assignment to build a Neural Network and use the Titanic Dat">
<meta property="og:type" content="article">
<meta property="og:title" content="Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting">
<meta property="og:url" content="https://shannonhung.github.io/en/posts/pytorch-titanic-nn/">
<meta property="og:site_name" content="Shannon&#39;s Blog üêü Tech | Life | Travel">
<meta property="og:description" content="Reference  Ref: A Detailed Explanation of the Titanic Dataset Structure  Introduction Recently, I enrolled in an AI course that included an assignment to build a Neural Network and use the Titanic Dat">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://shannonhung.github.io/en/img/cover/mac_coding.jpeg">
<meta property="article:published_time" content="2023-10-11T13:25:04.000Z">
<meta property="article:modified_time" content="2023-12-09T23:38:02.294Z">
<meta property="article:author" content="Shannon Hung">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shannonhung.github.io/en/img/cover/mac_coding.jpeg"><link rel="shortcut icon" href="/en/img/shannon-icon.png"><link rel="canonical" href="https://shannonhung.github.io/en/posts/pytorch-titanic-nn/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IAmwAuWZP3fXPtoYru7VJBancFMT2BkhN15HC2iea1o"/><link rel="stylesheet" href="/en/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XBNKVVH2P4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XBNKVVH2P4');
</script><script>const GLOBAL_CONFIG = {
  root: '/en/',
  algolia: undefined,
  localSearch: {"path":"/en/search.xml","preload":false,"top_n_per_article":-1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":3,"translateDelay":0,"msgToTraditionalChinese":"ÁπÅ","msgToSimplifiedChinese":"Á∞°"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-10 07:38:02'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/en/css/style.css"><link rel="stylesheet" href="/en/css/background.css"><link rel="shortcut icon" href="#"/></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/en/img/loading-icon.gif" data-original="/en/img/dudu-me.png" onerror="onerror=null;src='/en/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home Page</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categroies</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About Me</span></a></div><div class="menus_item"><a class="site-page" href="/en/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> Find Posts</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/tags/"><i class="fa-fw fas fa-tags"></i><span> By Tags</span></a></li><li><a class="site-page child" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> By Posts</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://shannonhung.github.io/"><i class="fa-fw fas fa-c"></i><span> ‰∏≠Êñá</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/en/img/cover/mac_coding.jpeg')"><nav id="nav"><span id="blog-info"><a href="/en/" title="Shannon's Blog üêü Tech | Life | Travel"><span class="site-name">Shannon's Blog üêü Tech | Life | Travel</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home Page</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categroies</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About Me</span></a></div><div class="menus_item"><a class="site-page" href="/en/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> Find Posts</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/tags/"><i class="fa-fw fas fa-tags"></i><span> By Tags</span></a></li><li><a class="site-page child" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> By Posts</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://shannonhung.github.io/"><i class="fa-fw fas fa-c"></i><span> ‰∏≠Êñá</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-10-11T13:25:04.000Z" title="Created 2023-10-11 21:25:04">2023-10-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-12-09T23:38:02.294Z" title="Updated 2023-12-10 07:38:02">2023-12-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Code/">Code</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Code/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>21mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Titanic Dataset - Building a Neural Network with PyTorch + Testing for Overfitting"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/en/posts/pytorch-titanic-nn/#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Reference">Reference</h1>
<ul>
<li>Ref: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8">A Detailed Explanation of the Titanic Dataset Structure</a></li>
</ul>
<h1 id="Introduction">Introduction</h1>
<p>Recently, I enrolled in an AI course that included an assignment to build a Neural Network and use the Titanic Dataset for training. The task was to implement overfitting by increasing hidden layers and neurons and then mitigate overfitting using dropout or other methods.</p>
<p>This article documents the process of completing the assignment.</p>
<h1 id="Environment-Setup-and-Assignment-Requirements">Environment Setup and Assignment Requirements</h1>
<blockquote>
<p>Environment Setup:</p>
<ul>
<li>Python 3.10.9</li>
<li>PyTorch 2.0.1</li>
</ul>
</blockquote>
<h2 id="Assignment-Requirements">Assignment Requirements</h2>
<ol>
<li>Write a custom <strong>dataset class for the Titanic data</strong> (see the data folder on <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/pabair/ki-lab-ss23">GitHub</a>). Use only the features: ‚ÄúPclass,‚Äù ‚ÄúAge,‚Äù ‚ÄúSibSp,‚Äù ‚ÄúParch,‚Äù ‚ÄúFare,‚Äù ‚ÄúSex,‚Äù and ‚ÄúEmbarked.‚Äù <strong>Preprocess the features</strong> accordingly in that class (scaling, one-hot-encoding, etc.), and split the data into train and validation data (80% and 20%). The constructor of that class should look like this: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titanic_train = TitanicDataSet(<span class="string">&#x27;titanic.csv&#x27;</span>, train=<span class="literal">True</span>)</span><br><span class="line">titanic_val = TitanicDataSet(<span class="string">&#x27;titanic.csv&#x27;</span>, train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
</li>
<li>Build a neural network with <strong>one hidden layer of size 3</strong> that predicts the survival of the passengers. Use a <strong>BCE loss</strong> (Hint: you need a <strong>sigmoid activation</strong> in the output layer). Use a data loader to train in batches of <strong>size 16</strong> and shuffle the data.</li>
<li><strong>Evaluate the performance</strong> of the model on the validation data using accuracy as a metric.</li>
<li><strong>Create the following plot</strong> that was introduced in the lecture.
<ul>
<li><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/yPNS7vc.png?x300" alt=""></li>
</ul>
</li>
<li>Increase the complexity of the network by <strong>adding more layers and neurons</strong> and see if you can overfit on the training data.</li>
<li>Try to remove overfitting by introducing a <strong><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html">dropout</a> layer</strong>.</li>
</ol>
<h2 id="In-Simple-Terms">In Simple Terms</h2>
<p>In simple terms, we will satisfy the above requirements through the following four steps:</p>
<ol>
<li>
<p><strong>Data Preprocessing</strong></p>
<ul>
<li><code>Task 1</code>: Build a class and import Titanic data.</li>
<li><code>Task 1</code>: Select specific columns as training features.</li>
<li><code>Task 1</code>: Data preprocessing (scaling, one-hot encoding, etc.) to convert non-numeric columns like ‚ÄúSex‚Äù or ‚ÄúEmbarked‚Äù into numeric values.</li>
<li><code>Task 1</code>: Split the data into train and validation data (80% and 20%).</li>
<li><code>Task 1</code>: Create a class and import the data.</li>
</ul>
</li>
<li>
<p><strong>Build a Neural Network</strong></p>
<ul>
<li><code>Task 2</code>: Build a three-layer network (1 input layer + 1 hidden layer + 1 output layer).</li>
<li><code>Task 2</code>: The size of the first hidden layer is 3.</li>
<li><code>Task 2</code>: Use BCE loss as the loss function.</li>
<li><code>Task 2</code>: Use sigmoid activation as the output layer‚Äôs activation function.</li>
</ul>
</li>
<li>
<p><strong>Model Training</strong></p>
<ul>
<li><code>Task 3</code>: Start training the model and record accuracy at each step.</li>
</ul>
</li>
<li>
<p><strong>Generate Results</strong></p>
<ul>
<li><code>Task 4</code>: Generate results and create plots.</li>
</ul>
</li>
<li>
<p><strong>Create Overfitting</strong></p>
<ul>
<li><code>Task 5</code>: Increase hidden layers and neurons to induce overfitting.</li>
</ul>
</li>
<li>
<p><strong>Use Dropout</strong></p>
<ul>
<li><code>Task 6</code>: Use dropout or other methods to mitigate the impact of overfitting.</li>
</ul>
</li>
</ol>
<h1 id="Step-1-Data-Preprocessing">Step 1: Data Preprocessing</h1>
<div class="note info flat"><p>Let‚Äôs start with <strong>data preprocessing</strong>:</p>
<ul>
<li><code>Task 1</code>: Build a class and import Titanic data.</li>
<li><code>Task 1</code>: Select specific columns as training features.</li>
<li><code>Task 1</code>: Data preprocessing (scaling, one-hot encoding, etc.) to convert non-numeric columns like ‚ÄúSex‚Äù or ‚ÄúEmbarked‚Äù into numeric values.</li>
<li><code>Task 1</code>: Split the data into train data and validation data (80% and 20%).</li>
<li><code>Task 1</code>: Create a class and import the data.</li>
</ul>
</div>
<h2 id="1-1-Data-Preprocessing">1.1 Data Preprocessing</h2>
<ol>
<li>
<p>First, we‚Äôll import all the necessary packages.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># data process </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io, transform</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot </span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># neural network </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># preprocessing</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Before we start, I‚Äôd like to place all the parameters we‚Äôll use at the top for easier modification:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shared variables</span></span><br><span class="line">D_in, D_out = <span class="number">10</span>, <span class="number">1</span> </span><br><span class="line">num_epochs = <span class="number">250</span> </span><br><span class="line">log_interval = <span class="number">100</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># Batch size: the amount of data for each training iteration</span></span><br><span class="line">batch_size = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate: Since we will create two different networks, we set two different learning rates</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">multi_learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hidden layers</span></span><br><span class="line">multi_num_layers = <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hidden neurons: Since we will create two different networks, we set two different numbers of hidden neurons</span></span><br><span class="line">neurons = <span class="number">3</span> </span><br><span class="line">multi_neurons = <span class="number">1024</span> </span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Next, we‚Äôll create a class based on the requirements and load the Titanic data, returning features:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TitanicDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialization function for loading and preprocessing data</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, train=<span class="literal">True</span>, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># The train parameter indicates whether it&#x27;s training data or test data</span></span><br><span class="line">        self.train = train</span><br><span class="line">        <span class="comment"># The transform parameter is used to define a transformation function if needed</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create MinMaxScaler and OneHotEncoder for data preprocessing</span></span><br><span class="line">        minmax_scaler = MinMaxScaler()</span><br><span class="line">        onehot_enc = OneHotEncoder()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Read the Titanic data from the CSV file</span></span><br><span class="line">        titanic = pd.read_csv(root_dir)</span><br><span class="line">        <span class="comment"># Select specific columns from the data</span></span><br><span class="line">        titanic = titanic[[<span class="string">&quot;Pclass&quot;</span>, <span class="string">&quot;Age&quot;</span>, <span class="string">&quot;SibSp&quot;</span>, <span class="string">&quot;Parch&quot;</span>, <span class="string">&quot;Fare&quot;</span>, <span class="string">&quot;Sex&quot;</span>, <span class="string">&quot;Embarked&quot;</span>, <span class="string">&quot;Survived&quot;</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fill missing values in the &quot;Age&quot; column with the mean and drop rows with missing values</span></span><br><span class="line">        titanic[<span class="string">&quot;Age&quot;</span>] = titanic[<span class="string">&quot;Age&quot;</span>].fillna(titanic[<span class="string">&quot;Age&quot;</span>].mean())</span><br><span class="line">        titanic = titanic.dropna()</span><br><span class="line">        titanic = titanic.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the data into categorical features, numerical features, and labels</span></span><br><span class="line">        categorical_features = titanic[titanic.select_dtypes(include=[<span class="string">&#x27;object&#x27;</span>]).columns.tolist()]</span><br><span class="line">        numerical_features = titanic[titanic.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>]).columns].drop(<span class="string">&#x27;Survived&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">        label_features = titanic[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Normalize numerical features (MinMax scaling)</span></span><br><span class="line">        numerical_features_arr = minmax_scaler.fit_transform(numerical_features)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Perform one-hot encoding on categorical features</span></span><br><span class="line">        categorical_features_arr = onehot_enc.fit_transform(categorical_features).toarray()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Merge the normalized numerical features and one-hot encoded categorical features into one dataset</span></span><br><span class="line">        combined_features = pd.DataFrame(data=numerical_features_arr, columns=numerical_features.columns)</span><br><span class="line">        combined_features = pd.concat([combined_features, pd.DataFrame(data=categorical_features_arr)], axis=<span class="number">1</span>)</span><br><span class="line">        combined_features = pd.concat([combined_features, label_features], axis=<span class="number">1</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split the dataset into training and test sets</span></span><br><span class="line">        train_data, test_data = train_test_split(combined_features, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Choose the data to use based on training or testing mode</span></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            self.data = train_data</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data = test_data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the length of the dataset</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Function for training the neural network, returns features and labels</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="comment"># Get the data from the self.data DataFrame for the idx-th row</span></span><br><span class="line">        sample = self.data.iloc[idx]</span><br><span class="line">        <span class="comment"># Convert a data structure to a PyTorch tensor and specify the data type as float</span></span><br><span class="line">        features = torch.FloatTensor(sample[:-<span class="number">1</span>])</span><br><span class="line">        label = torch.FloatTensor([sample[<span class="string">&#x27;Survived&#x27;</span>]])</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            features = self.transform(features)</span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the entire dataset as a DataFrame</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getData</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>After writing the function, you can use the following commands to test it:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">titanic_train = TitanicDataset(<span class="string">&#x27;./data/titanic.csv&#x27;</span>, train=<span class="literal">True</span>)</span><br><span class="line">titanic_val = TitanicDataset(<span class="string">&#x27;./data/titanic.csv&#x27;</span>, train=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;val_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_val))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;total_dataset len:&#x27;</span>, <span class="built_in">len</span>(titanic_train) + <span class="built_in">len</span>(titanic_val))</span><br><span class="line"><span class="comment"># Output </span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">train_dataset len: 711</span></span><br><span class="line"><span class="string">val_dataset len: 178</span></span><br><span class="line"><span class="string">total_dataset len: 889</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>ÂèØ‰ª•ÈÄèÈÅé‰ª•‰∏ãÁ®ãÂºèÔºåÂàóÂç∞Âá∫‰ª•‰∏ãÁµêÊûúÔºö Or using the following code to print the following results:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic_val.getData()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/d2C0wTF.png" alt=""></p>
<h1 id="Step2-Building-the-Neural-Network">Step2. Building the Neural Network</h1>
<ol>
<li>Next, we will construct the following Neural Network, primarily performing the following tasks:
<ul>
<li><code>__init__</code>: Create a three-layer network (1 input layer + 1 hidden layer + 1 output layer).
<ul>
<li><code>D_in</code>: Neurons size of the input layer.</li>
<li><code>H</code>: Neurons size of the hidden layer.</li>
<li><code>D_out</code>: Neurons size of the output layer.</li>
</ul>
</li>
<li><code>forward</code>: The place for performing the forward pass, primarily performing the linear transformation of the first layer with the <code>relu</code> activation function, the linear transformation of the second layer with the <code>sigmoid</code> activation function, and finally returning the prediction.</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the constructor we instantiate two nn.Linear modules and assign them as member variables.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TwoLayerNet, self).__init__()</span><br><span class="line">        <span class="comment"># the weight and bias of linear1 will be initialized </span></span><br><span class="line">        <span class="comment"># you can access them by self.linear1.weight and self.linear1.bias</span></span><br><span class="line">        self.linear1 = nn.Linear(D_in, H) <span class="comment"># this will create weight, bias for linear1</span></span><br><span class="line">        self.linear2 = nn.Linear(H, D_out) <span class="comment"># this will create weight, bias for linear2</span></span><br><span class="line">        self.sigmoid = nn.Sigmoid() <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        In the forward function we accept a Tensor of input data and we must return a Tensor of output data.</span></span><br><span class="line"><span class="string">        We can use Modules defined in the constructor as well as arbitrary operators on Tensors.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        h_relu = F.relu(self.linear1(x))</span><br><span class="line">        y_pred = self.sigmoid(self.linear2(h_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
</li>
<li>Before training the model, we need to build the neural network. The following code sets the batch size to 16, which means we train with 16 samples at a time. We train on all 889 data points in a single epoch. The input layer has 10 neurons, the hidden layer has 3 neurons, the output layer has 1 neuron, and the learning rate is set to 0.001. We train for a total of 500 epochs.
<ul>
<li>We create the network to construct the neural network.</li>
<li>We use Adam as the optimizer for gradient descent updates.</li>
<li>We use Binary Cross-Entropy Loss as the loss function.</li>
</ul>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">N, D_in, H, D_out = <span class="number">16</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">1</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">log_interval = <span class="number">100</span> <span class="comment"># Print the training status every log_interval epoch</span></span><br><span class="line"></span><br><span class="line">network = TwoLayerNet(D_in, H, D_out)  <span class="comment"># H=3 for one hidden layer with 3 neurons</span></span><br><span class="line">optimizer = optim.Adam(network.parameters(), lr)</span><br><span class="line">criterion = nn.BCELoss() <span class="comment"># Define the loss function as Binary Cross-Entropy Loss</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Step3-Model-Training">Step3. Model Training</h1>
<ol>
<li>
<p>You can start by creating lists to keep track of the loss and accuracy for each training loop (epoch) of the neural network model during the training process:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_losses = []  <span class="comment"># Save the loss value of each training loop (epoch) of the neural network model during the training process</span></span><br><span class="line">train_counter = []  <span class="comment"># Save the number of images for training so far</span></span><br><span class="line">test_losses = []   <span class="comment"># Save the loss value of each test loop (epoch) of the neural network model during the training process</span></span><br><span class="line">test_counter = [i * <span class="built_in">len</span>(titanic_train) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs + <span class="number">1</span>)]  <span class="comment"># how many data for training so far</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Create a training function with the main purpose of training the model using the train dataset:</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):  <span class="comment"># Indicates the current epoch being run</span></span><br><span class="line">    network.train()  <span class="comment"># Use the network created in the previous step for training</span></span><br><span class="line">    correct = <span class="number">0</span>  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">    cur_count = <span class="number">0</span>  <span class="comment"># Record how many data points have been trained so far</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># Clear the gradient to start fresh for each batch because the gradient is updated after each batch</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        output = network(data)  <span class="comment"># Feed the data into the network for forward propagation</span></span><br><span class="line">        loss = criterion(output, target)  <span class="comment"># Calculate the loss</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># Since the answers are either 0 or 1, we need to set a threshold where &gt;= 0.5 is 1 and &lt; 0.5 is 0</span></span><br><span class="line">        correct += (pred == target).<span class="built_in">sum</span>().item()  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)  <span class="comment"># Record how many data points have been trained so far</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        loss.backward()  <span class="comment"># Calculate the gradient of the loss</span></span><br><span class="line">        optimizer.step()  <span class="comment"># Update the gradient</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:  <span class="comment"># Print the training status every log_interval</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;\t Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch,</span><br><span class="line">                cur_count,</span><br><span class="line">                <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * cur_count / <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                loss.item(),</span><br><span class="line">                correct, <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * correct / <span class="built_in">len</span>(train_dataloader.dataset))</span><br><span class="line">            )</span><br><span class="line">            train_losses.append(loss.item())</span><br><span class="line">            train_counter.append((batch_idx * <span class="number">16</span>) + ((epoch - <span class="number">1</span>) * <span class="built_in">len</span>(train_dataloader.dataset)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return the current accuracy</span></span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(train_dataloader.dataset)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Constructing a test function, the main purpose is to test the trained model through validation dataset to see how accurate the model is when it is trained to detect unknown data.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    network.<span class="built_in">eval</span>()  <span class="comment"># Use the network created in the previous step and indicate that it&#x27;s for evaluation</span></span><br><span class="line">    test_loss = <span class="number">0</span>  <span class="comment"># Record the current loss</span></span><br><span class="line">    correct = <span class="number">0</span>  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># We don&#x27;t need to calculate gradients for evaluation, so we can use torch.no_grad() for speedup</span></span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_dataloader:  <span class="comment"># Get data through the test_dataloader</span></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            output = network(data)  <span class="comment"># Feed the data into the trained network for forward propagation</span></span><br><span class="line">            test_loss += criterion(output, target).item()  <span class="comment"># Calculate the loss</span></span><br><span class="line">            <span class="comment"># Accuracy</span></span><br><span class="line">            pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># 0.5 is the threshold</span></span><br><span class="line">            correct += (pred == target).<span class="built_in">sum</span>().item()  <span class="comment"># Record the current number of correct predictions</span></span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_dataloader.dataset)  <span class="comment"># Calculate the average loss</span></span><br><span class="line">    test_losses.append(test_loss)  <span class="comment"># Add the current loss to the list</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss,</span><br><span class="line">        correct,</span><br><span class="line">        <span class="built_in">len</span>(test_dataloader.dataset),</span><br><span class="line">        <span class="number">1.</span> * correct / <span class="built_in">len</span>(test_dataloader.dataset))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(test_dataloader.dataset)  <span class="comment"># Return the current accuracy</span></span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Finally, we can train the model according to the number of epochs, and check the training status by <code>test()</code> after each epoch.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test()</span><br><span class="line">train_accuracy_list = []</span><br><span class="line">test_accuracy_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>): <span class="comment"># Indicates the current epoch being run</span></span><br><span class="line">    train_accuracy_list.append(train(epoch)) <span class="comment"># After each epoch, we use the train() function to train the model</span></span><br><span class="line">    test_accuracy_list.append(test()) <span class="comment"># After each epoch, we use the test() function to test the model</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Step4-Generate-Results">Step4. Generate Results</h1>
<p>Finally, we can generate the results by using the following commands:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(train_accuracy_list, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.plot(test_accuracy_list, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="comment"># plt.ylim(0.5, 1)</span></span><br><span class="line">plt.legend([<span class="string">&#x27;Train Accuracy&#x27;</span>, <span class="string">&#x27;Test Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Train Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Test Accuracy&#x27;</span>], loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/8yHAk60.png" alt=""></p>
<h1 id="Step5-Make-Overfitting">Step5. Make Overfitting</h1>
<div class="note info flat"><p>Creating overfitting can mainly be achieved through a few methods:</p>
<ol>
<li><strong>Increasing the number of epochs</strong> can lead to some overfitting.</li>
<li><strong>Increasing the number of hidden layers</strong> or <strong>increasing the neuron size</strong> can also result in some overfitting.</li>
</ol>
</div>
<p>Since the task requires increasing the number of hidden layers and increasing the neuron size, let‚Äôs give it a try! The simplest way is to increase the number of hidden layers, increase the neuron size, and also increase the number of epochs to observe overfitting.</p>
<ol>
<li>Create a MultiLayerNet and increase the number of hidden layers and neuron size. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = <span class="number">128</span></span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, H)</span><br><span class="line">        self.linear1 = nn.Linear(H, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid() <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_relu = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        y_relu = F.relu(self.linear1(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear2(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear3(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear4(y_relu))</span><br><span class="line">        y_pred = self.sigmoid(self.output(y_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
</li>
</ol>
<div class="note danger flat"><p>Note: If you simply add layers, you may not see much learning effect, and you will always see a flat line‚Ä¶ and the accuracy won‚Äôt improve! Later, a classmate found that decreasing the number of neurons gradually would lead to better learning. So, we can set the neurons as 128, 64, 32, 16!<br>
A classmate said: ‚ÄúIt‚Äôs like an hourglass,‚Äù filtering out unimportant information step by step and leaving behind the important information!</p>
</div>
<ol>
<li>
<p>Create new <code>multi_train()</code> and <code>test_multi()</code> functions for the multi-network.</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_multi</span>(<span class="params">epoch</span>):</span><br><span class="line">    multi_network.train() <span class="comment"># Use the network created in the previous step for training</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    cur_count = <span class="number">0</span> </span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader):</span><br><span class="line">        multi_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward propagation</span></span><br><span class="line">        <span class="comment"># You will find that we use multi_network here for forward propagation</span></span><br><span class="line">        output = multi_network(data) </span><br><span class="line">        loss = multi_criterion(output, target) </span><br><span class="line">                </span><br><span class="line">        <span class="comment"># Accuracy</span></span><br><span class="line">        pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># survival_rate is the threshold</span></span><br><span class="line">        correct += (pred == target).<span class="built_in">sum</span>().item()</span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># backword propagation</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        multi_optimizer.step()</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Muti Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;\t Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, </span><br><span class="line">                cur_count, </span><br><span class="line">                <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * cur_count / <span class="built_in">len</span>(train_dataloader.dataset), </span><br><span class="line">                loss.item(), </span><br><span class="line">                correct, <span class="built_in">len</span>(train_dataloader.dataset),</span><br><span class="line">                <span class="number">100.</span> * correct / <span class="built_in">len</span>(train_dataloader.dataset))</span><br><span class="line">            )</span><br><span class="line">            train_losses.append(loss.item())</span><br><span class="line">            train_counter.append((batch_idx*<span class="number">16</span>) + ((epoch-<span class="number">1</span>)*<span class="built_in">len</span>(train_dataloader.dataset)))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(train_dataloader.dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_multi</span>():</span><br><span class="line">    multi_network.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            <span class="comment"># forward propagation</span></span><br><span class="line">            output = multi_network(data)</span><br><span class="line">            test_loss += multi_criterion(output, target).item()</span><br><span class="line">            <span class="comment"># Accuracy</span></span><br><span class="line">            pred = (output &gt;= <span class="number">0.5</span>).<span class="built_in">float</span>()  <span class="comment"># 0.5 is the threshold</span></span><br><span class="line">            correct += (pred == target).<span class="built_in">sum</span>().item()</span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_dataloader.dataset)</span><br><span class="line">    test_losses.append(test_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nMulti Test set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, </span><br><span class="line">        correct, </span><br><span class="line">        <span class="built_in">len</span>(test_dataloader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_dataloader.dataset))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> correct / <span class="built_in">len</span>(test_dataloader.dataset)</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Retrain model</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">test_multi()</span><br><span class="line"></span><br><span class="line">multi_train_accuracy_list = []</span><br><span class="line">multi_test_accuracy_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">    multi_train_accuracy_list.append(train_multi(epoch))</span><br><span class="line">    multi_test_accuracy_list.append(test_multi())</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>Draw the plot again: You can try to increase the number of epochs to 500, and you will see the overfitting phenomenon!</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot(multi_train_accuracy_list, color=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line">plt.plot(multi_test_accuracy_list, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.ylim(<span class="number">0.5</span>, <span class="number">0.9</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train Accuracy&#x27;</span>, <span class="string">&#x27;Test Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Train Accuracy&#x27;</span>, <span class="string">&#x27;Mutli Test Accuracy&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/eVxVkM6.png" alt=""></p>
<h2 id="Advanced-Version">Advanced Version</h2>
<p>If you wish to dynamically adjust the number of neurons and hidden layers, you can use the following approach:</p>
<ul>
<li><code>neurons</code>: The initial number of neurons. If set to 1024, it will decrease from 1024 to 16, dividing by 2 each time, until the number of neurons is less than 16.</li>
<li><code>num_layers</code>: The number of hidden layers.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">neurons = <span class="number">1024</span> </span><br><span class="line">num_layers = <span class="number">5</span> </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, D_out, neurons, num_layers</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = neurons</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, neurons)</span><br><span class="line">        self.linears = nn.ModuleList()  <span class="comment"># Note that if you want to create multiple layers with for loops, you have to use nn.ModuleList() to create</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.linears.append(nn.Linear(neurons, <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>)))</span><br><span class="line">            neurons = <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>) </span><br><span class="line">        self.output = nn.Linear(neurons, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()  <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.linears:</span><br><span class="line">            y = F.relu(layer(y))</span><br><span class="line">        y_pred = self.sigmoid(self.output(y))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h1 id="Step6-Use-Dropout">Step6. Use Dropout</h1>
<p>Here we can use dropout to avoid overfitting, mainly by randomly turning off some neurons during forward propagation, so as to avoid overfitting.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, H, D_out, num_layers, dropout_prob</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, H)</span><br><span class="line">        self.linear1 = nn.Linear(H, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">64</span>, <span class="number">32</span>)</span><br><span class="line">        self.linear4 = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">16</span>, D_out)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout_prob)  <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y_relu = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        y_relu = F.relu(self.linear1(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear2(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear3(y_relu))</span><br><span class="line">        y_relu = F.relu(self.linear4(y_relu))</span><br><span class="line">        y_relu = self.dropout(y_relu)  <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">        y_pred = self.sigmoid(self.output(y_relu))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>At this point, you will find that the overfitting phenomenon is not so serious! Here are the results when the epoch number is set to 200.</p>
<blockquote>
<p>Without Dropout<br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/BxmZNLL.png" alt=""></p>
</blockquote>
<blockquote>
<p>With Dropout<br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/wj5vFvt.png" alt=""></p>
</blockquote>
<h2 id="Advanced">Advanced</h2>
<p>The difference in the advanced version is that the number of dropout layers is the same as the number of hidden layers, and the number of dropout layers decreases with the number of hidden layers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in, D_out, neurons, num_layers, dropout_prob=<span class="number">0.8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerNet, self).__init__()</span><br><span class="line">        neurons = neurons</span><br><span class="line">        self.<span class="built_in">input</span> = nn.Linear(D_in, neurons)</span><br><span class="line">        self.linears = nn.ModuleList()  </span><br><span class="line">        self.dropouts = nn.ModuleList() <span class="comment">#  ======&gt; dropout layer</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.linears.append(nn.Linear(neurons, <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>)))</span><br><span class="line">            self.dropouts.append(nn.Dropout(p=dropout_prob)) <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">            neurons = <span class="built_in">max</span>(neurons // <span class="number">2</span>, <span class="number">16</span>) </span><br><span class="line">        self.output = nn.Linear(neurons, D_out) <span class="comment"># output layer</span></span><br><span class="line">        self.sigmoid = nn.Sigmoid()  <span class="comment"># Sigmoid activation for binary classification</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = F.relu(self.<span class="built_in">input</span>(x))</span><br><span class="line">        <span class="keyword">for</span> layer, dropout <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, self.dropouts):</span><br><span class="line">            y = F.relu(layer(y))</span><br><span class="line">            y = dropout(y) <span class="comment"># ======&gt; dropout layer</span></span><br><span class="line">        y_pred = self.sigmoid(self.output(y))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io/en">Shannon Hung</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io/en/posts/pytorch-titanic-nn/">https://shannonhung.github.io/en/posts/pytorch-titanic-nn/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/en/tags/Machine-Learning/">Machine Learning</a></div><div class="post_share"><div class="social-share" data-image="/en/img/cover/mac_coding.jpeg" data-sites="facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/en/posts/PyTorch-Mac-GPU/" title="MAC OS -  PyTorch on Mac OS with GPU support"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/mac_ai_gpu.jpeg" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">MAC OS -  PyTorch on Mac OS with GPU support</div></div></a></div><div class="next-post pull-right"><a href="/en/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/CNN-bg.jpeg" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/en/img/loading-icon.gif" data-original="/en/img/dudu-me.png" onerror="this.onerror=null;this.src='/en/en/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shannon Hung</div><div class="author-info__description">In order to avoid forgetfulness, I started to record</div></div><div class="card-info-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">11</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ShannonHung"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ShannonHung" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hsuanhung036@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">I am a graduate student at the Taiwan University of Science and Technology - Institute of Information Management, planning to pursue a dual-degree program in Germany this year. The reason behind setting up this website is my rather short memory span. While I have learned a lot, I tend to forget much of it. To combat forgetfulness, I have embarked on my note-taking journey.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Environment-Setup-and-Assignment-Requirements"><span class="toc-text">Environment Setup and Assignment Requirements</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Assignment-Requirements"><span class="toc-text">Assignment Requirements</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#In-Simple-Terms"><span class="toc-text">In Simple Terms</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step-1-Data-Preprocessing"><span class="toc-text">Step 1: Data Preprocessing</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Data-Preprocessing"><span class="toc-text">1.1 Data Preprocessing</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step2-Building-the-Neural-Network"><span class="toc-text">Step2. Building the Neural Network</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step3-Model-Training"><span class="toc-text">Step3. Model Training</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step4-Generate-Results"><span class="toc-text">Step4. Generate Results</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step5-Make-Overfitting"><span class="toc-text">Step5. Make Overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Version"><span class="toc-text">Advanced Version</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Step6-Use-Dropout"><span class="toc-text">Step6. Use Dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced"><span class="toc-text">Advanced</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/en/posts/textract-multi-column/" title="How to Handle Multi-Column Text Sorting with Amazon Textract"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/ocr-scan.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="How to Handle Multi-Column Text Sorting with Amazon Textract"/></a><div class="content"><a class="title" href="/en/posts/textract-multi-column/" title="How to Handle Multi-Column Text Sorting with Amazon Textract">How to Handle Multi-Column Text Sorting with Amazon Textract</a><time datetime="2024-03-19T16:40:35.000Z" title="Created 2024-03-20 00:40:35">2024-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/lstm-emotion.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="Twitter Dataset - Using LSTM to predict the emotion of the article"/></a><div class="content"><a class="title" href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article">Twitter Dataset - Using LSTM to predict the emotion of the article</a><time datetime="2023-11-16T06:30:45.000Z" title="Created 2023-11-16 14:30:45">2023-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/coco-object-diagnoise/" title="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/object-detection.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection"/></a><div class="content"><a class="title" href="/en/posts/coco-object-diagnoise/" title="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection">COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection</a><time datetime="2023-11-03T02:57:54.000Z" title="Created 2023-11-03 10:57:54">2023-11-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/flower-ml.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"/></a><div class="content"><a class="title" href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN">Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN</a><time datetime="2023-10-31T06:27:13.000Z" title="Created 2023-10-31 14:27:13">2023-10-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/CNN-bg.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard"/></a><div class="content"><a class="title" href="/en/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard">CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard</a><time datetime="2023-10-24T06:40:52.000Z" title="Created 2023-10-24 14:40:52">2023-10-24</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Shannon Hung</div><div class="footer_custom_text">Hi, welcome to Shannon <a target="_blank" rel="noopener external nofollow noreferrer" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">EN</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/en/js/utils.js"></script><script src="/en/js/main.js"></script><script src="/en/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: '51917ecdc184bb98b226',
      clientSecret: 'e48b44c1908c14b74ff7513f06c7cb892a4f4748',
      repo: 'shannonhung.github.io',
      owner: 'ShannonHung',
      admin: ['ShannonHung'],
      id: 'f9f330e0976fb5362c74c488ddfc11a2',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script data-pjax src="/en/self/btf.js"></script><script data-pjax src="/en/self/tw_en.js"></script><script id="canvas_nest" defer="defer" color="139,71,38" opacity="0.5" zIndex="-1" count="500" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/en/js/search/local-search.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>