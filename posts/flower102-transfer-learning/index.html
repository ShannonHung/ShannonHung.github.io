<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Flower102 Dataset - ä½¿ç”¨ Transfer Learning è¨“ç·´ + ä½¿ç”¨ Batch Normalization æ–¼ CNN | Shannon's Blog ğŸŸ æŠ€è¡“ | ç”Ÿæ´» | æ—…è¡Œ</title><meta name="author" content="Shannon Hung"><meta name="copyright" content="Shannon Hung"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="å‰è¨€ æœ€è¿‘é¸äº†ä¸€å ‚AIèª²ç¨‹ï¼Œé€™æ˜¯ç¬¬å››å€‹ä½œæ¥­ï¼Œä¸»è¦æ•™æˆå…§å®¹ç‚ºä»¥ä¸‹ä¸»é¡Œï¼š  Pick a dataset and train a model on it. Transfer Learning - Fine Tuning. Batch Normalization in CNN.  ä¸»è¦åƒè€ƒä»¥ä¸‹ç¶²ç«™ï¼š  Flower102 Dataset Transfer Learning DataSet of Pyto">
<meta property="og:type" content="article">
<meta property="og:title" content="Flower102 Dataset - ä½¿ç”¨ Transfer Learning è¨“ç·´ + ä½¿ç”¨ Batch Normalization æ–¼ CNN">
<meta property="og:url" content="https://shannonhung.github.io/posts/flower102-transfer-learning/">
<meta property="og:site_name" content="Shannon&#39;s Blog ğŸŸ æŠ€è¡“ | ç”Ÿæ´» | æ—…è¡Œ">
<meta property="og:description" content="å‰è¨€ æœ€è¿‘é¸äº†ä¸€å ‚AIèª²ç¨‹ï¼Œé€™æ˜¯ç¬¬å››å€‹ä½œæ¥­ï¼Œä¸»è¦æ•™æˆå…§å®¹ç‚ºä»¥ä¸‹ä¸»é¡Œï¼š  Pick a dataset and train a model on it. Transfer Learning - Fine Tuning. Batch Normalization in CNN.  ä¸»è¦åƒè€ƒä»¥ä¸‹ç¶²ç«™ï¼š  Flower102 Dataset Transfer Learning DataSet of Pyto">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://shannonhung.github.io/img/cover/flower-ml.jpeg">
<meta property="article:published_time" content="2023-10-31T06:27:13.000Z">
<meta property="article:modified_time" content="2023-11-02T12:15:03.680Z">
<meta property="article:author" content="Shannon Hung">
<meta property="article:tag" content="Mechine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shannonhung.github.io/img/cover/flower-ml.jpeg"><link rel="shortcut icon" href="/img/shannon-icon.png"><link rel="canonical" href="https://shannonhung.github.io/posts/flower102-transfer-learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IAmwAuWZP3fXPtoYru7VJBancFMT2BkhN15HC2iea1o"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XBNKVVH2P4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XBNKVVH2P4');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":-1,"unescape":false,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è©¢çš„å…§å®¹ï¼š${query}","hits_stats":"å…±æ‰¾åˆ° ${hits} ç¯‡æ–‡ç« "}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"ç¹","msgToSimplifiedChinese":"ç°¡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'è¤‡è£½æˆåŠŸ',
    error: 'è¤‡è£½éŒ¯èª¤',
    noSupport: 'ç€è¦½å™¨ä¸æ”¯æ´'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  dateSuffix: {
    just: 'å‰›å‰›',
    min: 'åˆ†é˜å‰',
    hour: 'å°æ™‚å‰',
    day: 'å¤©å‰',
    month: 'å€‹æœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"ä½ å·²åˆ‡æ›ç‚ºç¹é«”ä¸­æ–‡","cht_to_chs":"ä½ å·²åˆ‡æ›ç‚ºç°¡é«”ä¸­æ–‡","day_to_night":"ä½ å·²åˆ‡æ›ç‚ºæ·±è‰²æ¨¡å¼","night_to_day":"ä½ å·²åˆ‡æ›ç‚ºæ·ºè‰²æ¨¡å¼","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'è¼‰å…¥æ›´å¤š'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Flower102 Dataset - ä½¿ç”¨ Transfer Learning è¨“ç·´ + ä½¿ç”¨ Batch Normalization æ–¼ CNN',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-11-02 20:15:03'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/background.css"><link rel="shortcut icon" href="#"/></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/loading-icon.gif" data-original="/img/dudu-me.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">93</div></a><a href="/tags/"><div class="headline">æ¨™ç±¤</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">åˆ†é¡</div><div class="length-num">32</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é </span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> ç›®éŒ„</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> é—œæ–¼æˆ‘</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> ç›¸é—œé€£çµ</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> æ‰¾æ–‡ç« </span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ¨™ç±¤</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ–‡ç« </span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> èªè¨€</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="/"><i class="fa-fw fas fa-c"></i><span> ä¸­æ–‡</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover/flower-ml.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="Shannon's Blog ğŸŸ æŠ€è¡“ | ç”Ÿæ´» | æ—…è¡Œ"><span class="site-name">Shannon's Blog ğŸŸ æŠ€è¡“ | ç”Ÿæ´» | æ—…è¡Œ</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> æœå°‹</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é </span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> ç›®éŒ„</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> é—œæ–¼æˆ‘</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> ç›¸é—œé€£çµ</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> æ‰¾æ–‡ç« </span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ¨™ç±¤</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ–‡ç« </span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> èªè¨€</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="/"><i class="fa-fw fas fa-c"></i><span> ä¸­æ–‡</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Flower102 Dataset - ä½¿ç”¨ Transfer Learning è¨“ç·´ + ä½¿ç”¨ Batch Normalization æ–¼ CNN</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">ç™¼è¡¨æ–¼</span><time class="post-meta-date-created" datetime="2023-10-31T06:27:13.000Z" title="ç™¼è¡¨æ–¼ 2023-10-31 14:27:13">2023-10-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°æ–¼</span><time class="post-meta-date-updated" datetime="2023-11-02T12:15:03.680Z" title="æ›´æ–°æ–¼ 2023-11-02 20:15:03">2023-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Code/">Code</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Code/Mechine-Learning/">Mechine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•¸ç¸½è¨ˆ:</span><span class="word-count">6.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é–±è®€æ™‚é•·:</span><span>30åˆ†é˜</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Flower102 Dataset - ä½¿ç”¨ Transfer Learning è¨“ç·´ + ä½¿ç”¨ Batch Normalization æ–¼ CNN"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é–±è®€é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">è©•è«–æ•¸:</span><a href="/posts/flower102-transfer-learning/#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Mechine-Learning/">Mechine Learning</a></div></div><article class="post-content" id="article-container"><h1 id="å‰è¨€">å‰è¨€</h1>
<p>æœ€è¿‘é¸äº†ä¸€å ‚AIèª²ç¨‹ï¼Œé€™æ˜¯ç¬¬å››å€‹ä½œæ¥­ï¼Œä¸»è¦æ•™æˆå…§å®¹ç‚ºä»¥ä¸‹ä¸»é¡Œï¼š</p>
<ol>
<li>Pick a dataset and train a model on it.</li>
<li>Transfer Learning - Fine Tuning.</li>
<li>Batch Normalization in CNN.</li>
</ol>
<p>ä¸»è¦åƒè€ƒä»¥ä¸‹ç¶²ç«™ï¼š</p>
<ol>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.Flowers102.html#torchvision.datasets.Flowers102">Flower102 Dataset</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/datasets.html">DataSet of Pytorch</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/models.html">Models for transfer learning</a></li>
<li><a href="/posts/ML.html#Transfer-Learning">Shannonâ€™s Blog of Transfer Learning</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18">Resnet18</a></li>
</ol>
<h1 id="ä½œæ¥­è¦æ±‚">ä½œæ¥­è¦æ±‚</h1>
<p>Task:</p>
<ol>
<li><strong>é¸æ“‡ä¸€å€‹DataSet</strong>ï¼š Check out the torchvision <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/datasets.html">DataSet of Pytorch</a> and decide one dataset that you want to use (no<br>
CIFAR, no ImageNet, no FashionMNIST).</li>
<li><strong>å°å‡ºåœ–ç‰‡å’Œè³‡æ–™å¤§å°</strong>ï¼šShow some example images of the dataset in the notebook and print the dataset size.</li>
<li><strong>å»ºæ§‹ä½¿ç”¨Batch Normalizationçš„CNN</strong>ï¼šDesign a CNN to predict on the dataset. Use a similar architecture like last time, but this time<br>
also include batch normalization layers.</li>
<li><strong>ä½¿ç”¨datasetè¨“ç·´æ¨¡å‹ä¸¦å°å‡ºTestingçš„æº–ç¢ºç‡</strong>ï¼šTrain the model on the dataset and measure the accuracy on hold out test data.</li>
<li><strong>ä½¿ç”¨ResNet18ä¾†é€²è¡ŒTransfer-Learning</strong>ï¼šNow use transfer learning to use a pre-trained ResNet18 on the dataset as follows:
<ol>
<li><strong>ä¸æ”¹è®Šåˆ¥äººæ¨¡å‹è¨“ç·´å¥½çš„æ¬Šé‡</strong>ï¼šResNet18 as fixed feature extractor.</li>
<li><strong>ä½¿ç”¨RestNeté€²è¡ŒFineturned</strong>ï¼šResNet18 finetuned on the training data.</li>
</ol>
</li>
<li><strong>ä½¿ç”¨EfficientNet_B5é€²è¡ŒFineturned</strong>ï¼šRepeat step 4 but now use EfficientNet_B5 instead of RestNet18.</li>
<li><strong>æ¯”è¼ƒé€™äº›ä¸åŒçš„æ–¹æ³•ï¼Œä¸¦åˆ—å°å‡ºæº–ç¢ºåº¦</strong>ï¼šCompare the accuracy of the different approaches on the test data and print out the training<br>
times for each approach.</li>
</ol>
<h1 id="Task-0-import-package">Task 0 - import package</h1>
<p>å…ˆä¾†å°å…¥æ‰€éœ€çš„å¥—ä»¶ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CNN </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"></span><br><span class="line"><span class="comment"># others</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset </span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> Flowers102</span><br><span class="line"></span><br><span class="line"><span class="comment"># label </span></span><br><span class="line"><span class="keyword">from</span> scipy.io <span class="keyword">import</span> loadmat</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>
<h1 id="Task-1-é¸æ“‡ä¸€å€‹DataSet">Task 1 - é¸æ“‡ä¸€å€‹DataSet</h1>
<ul>
<li>Ref: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/">ç‚ºä»€éº¼æ˜¯[0.485, 0.456, 0.406]é€²è¡ŒNormalization</a></li>
</ul>
<div class="note info flat"><p><strong>é¸æ“‡ä¸€å€‹DataSet</strong>ï¼š Check out the torchvision <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/datasets.html">DataSet of Pytorch</a> and decide one dataset that you want to use (no<br>
CIFAR, no ImageNet, no FashionMNIST).</p>
</div>
<p>ç‚ºäº†é«”é©— Transfer Learningï¼Œä¸¦ä¸”å¿«é€Ÿè¨“ç·´ã€‚æˆ‘å€‘é€™é‚Šä½¿ç”¨ flower102 ä¾†ä½œç‚ºæˆ‘å€‘çš„è³‡æ–™é›†ã€‚å› ç‚º flower102 æ²’æœ‰æä¾›ä¸­æ–‡çš„ Labelï¼Œæˆ‘ç¶²è·¯ä¸Šæ‰¾å¤§éƒ¨åˆ†éƒ½æ˜¯è®€å–å·²ç¶“å¯«å¥½çš„ <code>.json</code> æˆ–æ˜¯ <code>.txt</code> æª”æ¡ˆï¼Œè©²æª”æ¡ˆæœƒæè¿°æ¯ä¸€å€‹ label index å°æ‡‰çš„ä¸­æ–‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æŒ‡å®šä½ è¦ä¸‹è¼‰çš„è³‡æ–™åŠè·¯å¾‘ å’Œ btach size ä¸€æ¬¡è¨“ç·´çš„é‡</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">data_dir = <span class="string">&#x27;../../Data/flowers-102&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å»ºç«‹ dataset çš„ classes_name </span></span><br><span class="line">json_data = <span class="string">&#x27;&#123;&quot;21&quot;: &quot;fire lily&quot;, &quot;3&quot;: &quot;canterbury bells&quot;, &quot;45&quot;: &quot;bolero deep blue&quot;, &quot;1&quot;: &quot;pink primrose&quot;, &quot;34&quot;: &quot;mexican aster&quot;, &quot;27&quot;: &quot;prince of wales feathers&quot;, &quot;7&quot;: &quot;moon orchid&quot;, &quot;16&quot;: &quot;globe-flower&quot;, &quot;25&quot;: &quot;grape hyacinth&quot;, &quot;26&quot;: &quot;corn poppy&quot;, &quot;79&quot;: &quot;toad lily&quot;, &quot;39&quot;: &quot;siam tulip&quot;, &quot;24&quot;: &quot;red ginger&quot;, &quot;67&quot;: &quot;spring crocus&quot;, &quot;35&quot;: &quot;alpine sea holly&quot;, &quot;32&quot;: &quot;garden phlox&quot;, &quot;10&quot;: &quot;globe thistle&quot;, &quot;6&quot;: &quot;tiger lily&quot;, &quot;93&quot;: &quot;ball moss&quot;, &quot;33&quot;: &quot;love in the mist&quot;, &quot;9&quot;: &quot;monkshood&quot;, &quot;102&quot;: &quot;blackberry lily&quot;, &quot;14&quot;: &quot;spear thistle&quot;, &quot;19&quot;: &quot;balloon flower&quot;, &quot;100&quot;: &quot;blanket flower&quot;, &quot;13&quot;: &quot;king protea&quot;, &quot;49&quot;: &quot;oxeye daisy&quot;, &quot;15&quot;: &quot;yellow iris&quot;, &quot;61&quot;: &quot;cautleya spicata&quot;, &quot;31&quot;: &quot;carnation&quot;, &quot;64&quot;: &quot;silverbush&quot;, &quot;68&quot;: &quot;bearded iris&quot;, &quot;63&quot;: &quot;black-eyed susan&quot;, &quot;69&quot;: &quot;windflower&quot;, &quot;62&quot;: &quot;japanese anemone&quot;, &quot;20&quot;: &quot;giant white arum lily&quot;, &quot;38&quot;: &quot;great masterwort&quot;, &quot;4&quot;: &quot;sweet pea&quot;, &quot;86&quot;: &quot;tree mallow&quot;, &quot;101&quot;: &quot;trumpet creeper&quot;, &quot;42&quot;: &quot;daffodil&quot;, &quot;22&quot;: &quot;pincushion flower&quot;, &quot;2&quot;: &quot;hard-leaved pocket orchid&quot;, &quot;54&quot;: &quot;sunflower&quot;, &quot;66&quot;: &quot;osteospermum&quot;, &quot;70&quot;: &quot;tree poppy&quot;, &quot;85&quot;: &quot;desert-rose&quot;, &quot;99&quot;: &quot;bromelia&quot;, &quot;87&quot;: &quot;magnolia&quot;, &quot;5&quot;: &quot;english marigold&quot;, &quot;92&quot;: &quot;bee balm&quot;, &quot;28&quot;: &quot;stemless gentian&quot;, &quot;97&quot;: &quot;mallow&quot;, &quot;57&quot;: &quot;gaura&quot;, &quot;40&quot;: &quot;lenten rose&quot;, &quot;47&quot;: &quot;marigold&quot;, &quot;59&quot;: &quot;orange dahlia&quot;, &quot;48&quot;: &quot;buttercup&quot;, &quot;55&quot;: &quot;pelargonium&quot;, &quot;36&quot;: &quot;ruby-lipped cattleya&quot;, &quot;91&quot;: &quot;hippeastrum&quot;, &quot;29&quot;: &quot;artichoke&quot;, &quot;71&quot;: &quot;gazania&quot;, &quot;90&quot;: &quot;canna lily&quot;, &quot;18&quot;: &quot;peruvian lily&quot;, &quot;98&quot;: &quot;mexican petunia&quot;, &quot;8&quot;: &quot;bird of paradise&quot;, &quot;30&quot;: &quot;sweet william&quot;, &quot;17&quot;: &quot;purple coneflower&quot;, &quot;52&quot;: &quot;wild pansy&quot;, &quot;84&quot;: &quot;columbine&quot;, &quot;12&quot;: &quot;colt\&#x27;s foot&quot;, &quot;11&quot;: &quot;snapdragon&quot;, &quot;96&quot;: &quot;camellia&quot;, &quot;23&quot;: &quot;fritillary&quot;, &quot;50&quot;: &quot;common dandelion&quot;, &quot;44&quot;: &quot;poinsettia&quot;, &quot;53&quot;: &quot;primula&quot;, &quot;72&quot;: &quot;azalea&quot;, &quot;65&quot;: &quot;californian poppy&quot;, &quot;80&quot;: &quot;anthurium&quot;, &quot;76&quot;: &quot;morning glory&quot;, &quot;37&quot;: &quot;cape flower&quot;, &quot;56&quot;: &quot;bishop of llandaff&quot;, &quot;60&quot;: &quot;pink-yellow dahlia&quot;, &quot;82&quot;: &quot;clematis&quot;, &quot;58&quot;: &quot;geranium&quot;, &quot;75&quot;: &quot;thorn apple&quot;, &quot;41&quot;: &quot;barbeton daisy&quot;, &quot;95&quot;: &quot;bougainvillea&quot;, &quot;43&quot;: &quot;sword lily&quot;, &quot;83&quot;: &quot;hibiscus&quot;, &quot;78&quot;: &quot;lotus lotus&quot;, &quot;88&quot;: &quot;cyclamen&quot;, &quot;94&quot;: &quot;foxglove&quot;, &quot;81&quot;: &quot;frangipani&quot;, &quot;74&quot;: &quot;rose&quot;, &quot;89&quot;: &quot;watercress&quot;, &quot;73&quot;: &quot;water lily&quot;, &quot;46&quot;: &quot;wallflower&quot;, &quot;77&quot;: &quot;passion flower&quot;, &quot;51&quot;: &quot;petunia&quot;&#125;&#x27;</span></span><br><span class="line"><span class="comment"># load data </span></span><br><span class="line">cat_to_name = json.loads(json_data)</span><br><span class="line"><span class="comment"># æŠŠ key è½‰æˆ intï¼Œå› ç‚º dataset çš„ label å¾ 0 é–‹å§‹ã€‚ä½†æ˜¯é€™å€‹ json å¾ 1 é–‹å§‹ï¼Œæ‰€ä»¥æˆ‘å€‘è¦ -1</span></span><br><span class="line">cat_to_name = &#123;<span class="built_in">int</span>(k)-<span class="number">1</span>:v <span class="keyword">for</span> k,v <span class="keyword">in</span> cat_to_name.items()&#125;</span><br><span class="line"><span class="comment"># æ’åºï¼Œè½‰æ›æˆ dic ä¸¦å°å‡ºä¾†</span></span><br><span class="line">class_names = <span class="built_in">dict</span>(<span class="built_in">sorted</span>(cat_to_name.items()))</span><br><span class="line"><span class="built_in">print</span>(class_names)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;pink primrose&#x27;</span>, <span class="number">1</span>: <span class="string">&#x27;hard-leaved pocket orchid&#x27;</span>, <span class="number">2</span>: <span class="string">&#x27;canterbury bells&#x27;</span>, <span class="number">3</span>: <span class="string">&#x27;sweet pea&#x27;</span>, <span class="number">4</span>: <span class="string">&#x27;english marigold&#x27;</span>, <span class="number">5</span>: <span class="string">&#x27;tiger lily&#x27;</span>, <span class="number">6</span>: <span class="string">&#x27;moon orchid&#x27;</span>, <span class="number">7</span>: <span class="string">&#x27;bird of paradise&#x27;</span>, <span class="number">8</span>: <span class="string">&#x27;monkshood&#x27;</span>, <span class="number">9</span>: <span class="string">&#x27;globe thistle&#x27;</span>, <span class="number">10</span>: <span class="string">&#x27;snapdragon&#x27;</span>, <span class="number">11</span>: <span class="string">&quot;colt&#x27;s foot&quot;</span>, <span class="number">12</span>: <span class="string">&#x27;king protea&#x27;</span>, <span class="number">13</span>: <span class="string">&#x27;spear thistle&#x27;</span>, <span class="number">14</span>: <span class="string">&#x27;yellow iris&#x27;</span>, <span class="number">15</span>: <span class="string">&#x27;globe-flower&#x27;</span>, <span class="number">16</span>: <span class="string">&#x27;purple coneflower&#x27;</span>, <span class="number">17</span>: <span class="string">&#x27;peruvian lily&#x27;</span>, <span class="number">18</span>: <span class="string">&#x27;balloon flower&#x27;</span>, <span class="number">19</span>: <span class="string">&#x27;giant white arum lily&#x27;</span>, <span class="number">20</span>: <span class="string">&#x27;fire lily&#x27;</span>, <span class="number">21</span>: <span class="string">&#x27;pincushion flower&#x27;</span>, <span class="number">22</span>: <span class="string">&#x27;fritillary&#x27;</span>, <span class="number">23</span>: <span class="string">&#x27;red ginger&#x27;</span>, <span class="number">24</span>: <span class="string">&#x27;grape hyacinth&#x27;</span>, <span class="number">25</span>: <span class="string">&#x27;corn poppy&#x27;</span>, <span class="number">26</span>: <span class="string">&#x27;prince of wales feathers&#x27;</span>, <span class="number">27</span>: <span class="string">&#x27;stemless gentian&#x27;</span>, <span class="number">28</span>: <span class="string">&#x27;artichoke&#x27;</span>, <span class="number">29</span>: <span class="string">&#x27;sweet william&#x27;</span>, <span class="number">30</span>: <span class="string">&#x27;carnation&#x27;</span>, <span class="number">31</span>: <span class="string">&#x27;garden phlox&#x27;</span>, <span class="number">32</span>: <span class="string">&#x27;love in the mist&#x27;</span>, <span class="number">33</span>: <span class="string">&#x27;mexican aster&#x27;</span>, <span class="number">34</span>: <span class="string">&#x27;alpine sea holly&#x27;</span>, <span class="number">35</span>: <span class="string">&#x27;ruby-lipped cattleya&#x27;</span>, <span class="number">36</span>: <span class="string">&#x27;cape flower&#x27;</span>, <span class="number">37</span>: <span class="string">&#x27;great masterwort&#x27;</span>, <span class="number">38</span>: <span class="string">&#x27;siam tulip&#x27;</span>, <span class="number">39</span>: <span class="string">&#x27;lenten rose&#x27;</span>, <span class="number">40</span>: <span class="string">&#x27;barbeton daisy&#x27;</span>, <span class="number">41</span>: <span class="string">&#x27;daffodil&#x27;</span>, <span class="number">42</span>: <span class="string">&#x27;sword lily&#x27;</span>, <span class="number">43</span>: <span class="string">&#x27;poinsettia&#x27;</span>, <span class="number">44</span>: <span class="string">&#x27;bolero deep blue&#x27;</span>, <span class="number">45</span>: <span class="string">&#x27;wallflower&#x27;</span>, <span class="number">46</span>: <span class="string">&#x27;marigold&#x27;</span>, <span class="number">47</span>: <span class="string">&#x27;buttercup&#x27;</span>, <span class="number">48</span>: <span class="string">&#x27;oxeye daisy&#x27;</span>, <span class="number">49</span>: <span class="string">&#x27;common dandelion&#x27;</span>, <span class="number">50</span>: <span class="string">&#x27;petunia&#x27;</span>, <span class="number">51</span>: <span class="string">&#x27;wild pansy&#x27;</span>, <span class="number">52</span>: <span class="string">&#x27;primula&#x27;</span>, <span class="number">53</span>: <span class="string">&#x27;sunflower&#x27;</span>, <span class="number">54</span>: <span class="string">&#x27;pelargonium&#x27;</span>, <span class="number">55</span>: <span class="string">&#x27;bishop of llandaff&#x27;</span>, <span class="number">56</span>: <span class="string">&#x27;gaura&#x27;</span>, <span class="number">57</span>: <span class="string">&#x27;geranium&#x27;</span>, <span class="number">58</span>: <span class="string">&#x27;orange dahlia&#x27;</span>, <span class="number">59</span>: <span class="string">&#x27;pink-yellow dahlia&#x27;</span>, <span class="number">60</span>: <span class="string">&#x27;cautleya spicata&#x27;</span>, <span class="number">61</span>: <span class="string">&#x27;japanese anemone&#x27;</span>, <span class="number">62</span>: <span class="string">&#x27;black-eyed susan&#x27;</span>, <span class="number">63</span>: <span class="string">&#x27;silverbush&#x27;</span>, <span class="number">64</span>: <span class="string">&#x27;californian poppy&#x27;</span>, <span class="number">65</span>: <span class="string">&#x27;osteospermum&#x27;</span>, <span class="number">66</span>: <span class="string">&#x27;spring crocus&#x27;</span>, <span class="number">67</span>: <span class="string">&#x27;bearded iris&#x27;</span>, <span class="number">68</span>: <span class="string">&#x27;windflower&#x27;</span>, <span class="number">69</span>: <span class="string">&#x27;tree poppy&#x27;</span>, <span class="number">70</span>: <span class="string">&#x27;gazania&#x27;</span>, <span class="number">71</span>: <span class="string">&#x27;azalea&#x27;</span>, <span class="number">72</span>: <span class="string">&#x27;water lily&#x27;</span>, <span class="number">73</span>: <span class="string">&#x27;rose&#x27;</span>, <span class="number">74</span>: <span class="string">&#x27;thorn apple&#x27;</span>, <span class="number">75</span>: <span class="string">&#x27;morning glory&#x27;</span>, <span class="number">76</span>: <span class="string">&#x27;passion flower&#x27;</span>, <span class="number">77</span>: <span class="string">&#x27;lotus lotus&#x27;</span>, <span class="number">78</span>: <span class="string">&#x27;toad lily&#x27;</span>, <span class="number">79</span>: <span class="string">&#x27;anthurium&#x27;</span>, <span class="number">80</span>: <span class="string">&#x27;frangipani&#x27;</span>, <span class="number">81</span>: <span class="string">&#x27;clematis&#x27;</span>, <span class="number">82</span>: <span class="string">&#x27;hibiscus&#x27;</span>, <span class="number">83</span>: <span class="string">&#x27;columbine&#x27;</span>, <span class="number">84</span>: <span class="string">&#x27;desert-rose&#x27;</span>, <span class="number">85</span>: <span class="string">&#x27;tree mallow&#x27;</span>, <span class="number">86</span>: <span class="string">&#x27;magnolia&#x27;</span>, <span class="number">87</span>: <span class="string">&#x27;cyclamen&#x27;</span>, <span class="number">88</span>: <span class="string">&#x27;watercress&#x27;</span>, <span class="number">89</span>: <span class="string">&#x27;canna lily&#x27;</span>, <span class="number">90</span>: <span class="string">&#x27;hippeastrum&#x27;</span>, <span class="number">91</span>: <span class="string">&#x27;bee balm&#x27;</span>, <span class="number">92</span>: <span class="string">&#x27;ball moss&#x27;</span>, <span class="number">93</span>: <span class="string">&#x27;foxglove&#x27;</span>, <span class="number">94</span>: <span class="string">&#x27;bougainvillea&#x27;</span>, <span class="number">95</span>: <span class="string">&#x27;camellia&#x27;</span>, <span class="number">96</span>: <span class="string">&#x27;mallow&#x27;</span>, <span class="number">97</span>: <span class="string">&#x27;mexican petunia&#x27;</span>, <span class="number">98</span>: <span class="string">&#x27;bromelia&#x27;</span>, <span class="number">99</span>: <span class="string">&#x27;blanket flower&#x27;</span>, <span class="number">100</span>: <span class="string">&#x27;trumpet creeper&#x27;</span>, <span class="number">101</span>: <span class="string">&#x27;blackberry lily&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>é€™é‚Šæˆ‘ä¸»è¦æ˜¯åƒè€ƒå®˜æ–¹ç¶²ç«™<a href="hhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="external nofollow noreferrer">Transfer Learning</a>çš„å¯«æ³•ï¼Œæ”¹æˆè‡ªå·±æƒ³è¦çš„ dataSetï¼Œä¸¦é–‹å§‹ä¸‹è¼‰æª”æ¡ˆï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Data augmentation and normalization for training</span></span><br><span class="line"><span class="comment"># Just normalization for validation</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">        <span class="comment"># é¦–å…ˆå°åœ–åƒé€²è¡Œè£å‰ªï¼Œç„¶å¾Œå†èª¿æ•´å¤§å°ã€‚å®ƒéš¨æ©Ÿé¸æ“‡ä¸€å€‹çŸ©å½¢å€åŸŸä¸¦è£å‰ªåœ–åƒ</span></span><br><span class="line">        <span class="comment"># ç„¶å¾Œå°‡è£å‰ªçš„åœ–åƒèª¿æ•´ç‚ºæŒ‡å®šçš„å¤§å°ç‚º 224x224 åƒç´ ã€‚</span></span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">        <span class="comment"># è¨­å®šåœ–åƒçš„ç¿»è½‰æ©Ÿç‡ï¼Œé€šå¸¸æ˜¯ä¸€å€‹ 0 åˆ° 1 çš„æ•¸å­—ï¼Œä¾‹å¦‚ 0.5ï¼Œè¡¨ç¤ºæœ‰ 50% çš„æ©Ÿç‡ç¿»è½‰åœ–åƒã€‚Default value is 0.5</span></span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        <span class="comment"># å°‡åœ–åƒè½‰æ›æˆ Tensor</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        <span class="comment"># ç”¨æ•¸å€¼ normalize çš„æ–¹å¼ä¾†æ­£è¦åŒ– image çš„æ•¸å€¼ï¼Œç¬¬ä¸€åƒæ•¸æ˜¯ meanï¼Œç¬¬äºŒå€‹åƒæ•¸æ˜¯ std æ¨™æº–å·®</span></span><br><span class="line">        <span class="comment"># è¨­å®š [0.485, 0.456, 0.406] çš„åŸå› å¯åƒè€ƒï¼šhttps://www.geeksforgeeks.org/how-to-normalize-images-in-pytorch/</span></span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">&#x27;val&#x27;</span>: transforms.Compose([</span><br><span class="line">        <span class="comment"># é€™å€‹æ²’æœ‰éš¨æ©Ÿé¸æ“‡å€åŸŸï¼Œè€Œæ˜¯ç›´æ¥æ•´åœ–åƒçš„å°ºå¯¸ï¼Œä½¿å…¶ç¬¦åˆæŒ‡å®šçš„å¤§å°</span></span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        <span class="comment"># å°‡åœ–åƒçš„ä¸­å¿ƒéƒ¨åˆ†ä¿ç•™ï¼Œç„¶å¾Œèª¿æ•´å°ºå¯¸ä»¥æ»¿è¶³æŒ‡å®šçš„å¤§å°ã€‚</span></span><br><span class="line">        <span class="comment"># ç”¨æ–¼é©—è­‰æˆ–æ¸¬è©¦æ•¸æ“šï¼Œä»¥ç¢ºä¿æ¸¬è©¦åœ–åƒå…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾µï¼Œä¸¦ä¸”ä¸åƒ RandomResizedCrop é‚£æ¨£å…·æœ‰éš¨æ©Ÿæ€§</span></span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># æˆ‘å€‘æŠŠ trainning ç”¨çš„è³‡æ–™ä¸‹è¼‰åˆ° data_dir/train è³‡æ–™å¤¾ï¼Œä¸¦ä¸”ä½¿ç”¨ data_transforms[&quot;train&quot;] é€™å€‹å‡½å¼ä¾†åšè³‡æ–™çš„è½‰æ›</span></span><br><span class="line">train_datasets = Flowers102(root=data_dir+<span class="string">&quot;/train&quot;</span>, split=<span class="string">&quot;train&quot;</span>, download=<span class="literal">True</span>, transform=data_transforms[<span class="string">&quot;train&quot;</span>])</span><br><span class="line"><span class="comment"># æˆ‘å€‘æŠŠ validation ç”¨çš„è³‡æ–™ä¸‹è¼‰åˆ° data_dir/val è³‡æ–™å¤¾ï¼Œä¸¦ä¸”ä½¿ç”¨ data_transforms[&quot;val&quot;] é€™å€‹å‡½å¼ä¾†åšè³‡æ–™çš„è½‰æ›</span></span><br><span class="line">val_datasets = Flowers102(root=data_dir+<span class="string">&quot;/val&quot;</span>, split=<span class="string">&quot;val&quot;</span>, download=<span class="literal">True</span>, transform=data_transforms[<span class="string">&quot;val&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># æŒ‡å®šä¸‹è¼‰ flowers102 çš„è³‡æ–™é›†ï¼Œä¸‹è¼‰ train å’Œ val çš„è³‡æ–™é›†</span></span><br><span class="line">image_datasets = &#123;x: Flowers102(root=data_dir, split=x, download=<span class="literal">True</span>, transform=data_transforms[x])</span><br><span class="line">                    <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># è½‰æ›æˆ DataLoader çš„å½¢å¼ï¼Œä¸¦ä¸”æŒ‡å®š batch_size</span></span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;mps&quot;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;device: &quot;</span>,device)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;image_datasets function call: &quot;</span>, <span class="built_in">dir</span>(image_datasets[<span class="string">&quot;train&quot;</span>]))</span><br></pre></td></tr></table></figure>
<div class="note info flat"><p>é€™æ¨£æˆ‘å€‘å°±å®Œæˆäº†ç¬¬ä¸€å€‹Taskï¼Œä¹Ÿå°±æ˜¯ä¸‹è¼‰å¥½æˆ‘å€‘æƒ³è¦çš„ datasetã€‚</p>
</div>
<h1 id="Task-2-å°å‡ºåœ–ç‰‡å’Œè³‡æ–™å¤§å°">Task 2 - å°å‡ºåœ–ç‰‡å’Œè³‡æ–™å¤§å°</h1>
<div class="note info flat"><ol start="2">
<li><strong>å°å‡ºåœ–ç‰‡å’Œè³‡æ–™å¤§å°</strong>ï¼šShow some example images of the dataset in the notebook and print the dataset size.</li>
</ol>
</div>
<p>åƒè€ƒå®˜æ–¹ç¶²ç«™<a href="hhttps://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html" rel="external nofollow noreferrer">Transfer Learning</a>çš„å¯«æ³•ï¼Œæˆ‘å€‘å…ˆå»ºç«‹</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">imshow</span>(<span class="params">inp, title=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Display image for Tensor.&quot;&quot;&quot;</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get a batch of training data</span></span><br><span class="line">inputs, classes = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;train&#x27;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a grid from batch</span></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x.item() å–å‡º tensor çš„å€¼ï¼Œé€šå¸¸æ˜¯æ•¸å­—ï¼Œç„¶å¾Œåœ¨ class_names dic æ‰¾åˆ°è©²æ•¸å­—å°æ‡‰çš„è‹±æ–‡åå­—</span></span><br><span class="line">imshow(out, title=[class_names[x.item()] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(inputs.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dataset_sizes: &quot;</span>,dataset_sizes)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹ï¼š</p>
</blockquote>
<p><img src="/img/loading-icon.gif" data-original="https://i.imgur.com/dPGyFvN.png" alt=""></p>
<h1 id="Task-3-4-CNN-Batch-Normalization">Task 3 &amp; 4 - CNN + Batch Normalization</h1>
<div class="note info flat"><ol start="3">
<li><strong>å»ºæ§‹ä½¿ç”¨Batch Normalizationçš„CNN</strong>ï¼šDesign a CNN to predict on the dataset. Use a similar architecture like last time, but this time also include batch normalization layers.</li>
<li><strong>ä½¿ç”¨datasetè¨“ç·´æ¨¡å‹ä¸¦å°å‡ºTestingçš„æº–ç¢ºç‡</strong>ï¼šTrain the model on the dataset and measure the accuracy on hold out test data.</li>
</ol>
</div>
<p><strong>æ ¹æ“šæé´»æ¯…æ•™æˆåœ¨ Transfer Learning æåˆ°â€¦</strong><br>
é€šå¸¸æœƒåœ¨ <code>Activation Function</code> ä¹‹å‰åŸ·è¡Œ <code>Batch Normalization</code>ï¼Œæœ‰èˆˆè¶£å¯ä»¥åƒè€ƒé€™å€‹<a href="/posts/ML.html#Feature-Normalization">ç« ç¯€</a>ã€‚<code>Batch Normalization</code> ç°¡å–®ä¾†èªªå°±æ˜¯ä»¥ <code>Batch</code> çš„æ–¹å¼ï¼ŒåŸ·è¡Œ <code>feature normalization</code>ã€‚</p>
<p><strong>ç‚ºä»€éº¼è¦åš feature normalization ?</strong><br>
ä»–å°±æ˜¯ç‚ºäº†è®“ä¸åŒçš„ feature æœ‰é¡ä¼¼æ¥è¿‘çš„æ•¸å€¼ç¯„åœï¼Œé€™æ¨£æ¨¡å‹åœ¨åŸ·è¡ŒGradient Descentçš„æ™‚å€™ï¼Œw1, w2 å° loss çš„å½±éŸ¿æ‰ä¸æœƒå¤ªå¤§ï¼Œä»–å€‘æ“æœ‰ç›¸ä¼¼çš„æ•¸å€¼ç¯„åœï¼Œæ‰èƒ½å¤ å¹³å‡çš„å½±éŸ¿ lossï¼Œè€Œä¸æ˜¯æŸå€‹ w1 å° loss çš„å½±éŸ¿é å¤§æ–¼ w2ã€‚</p>
<blockquote>
<p>å¤§æ¦‚æ˜¯ä¸‹åœ–é€™ç¨®æ•ˆæœã€‚</p>
</blockquote>
<p><img src="/img/loading-icon.gif" data-original="https://i.imgur.com/RB51XXy.png" alt=""></p>
<h2 id="å»ºç«‹-Network">å»ºç«‹ Network</h2>
<div class="note warning flat"><p>è«‹æ³¨æ„ï¼Œæ ¹æ“šä¸åŒçš„ dataset å…¶å°ºå¯¸å¤§å°é‚„æœ‰ hidden layerçš„æ•¸é‡ï¼Œä½ è¦åšå…©å€‹èª¿æ•´ï¼ï¼</p>
<ol>
<li>åœ¨ fully connection layer ä¸­ï¼Œinput è¦æ ¹æ“šä½ çš„ hidden layer åŸ·è¡Œ <code>max-pooling</code> è·Ÿ <code>convolution</code> çš„æ¬¡æ•¸ä¾†æ±ºå®šã€‚</li>
<li>ç„¶å¾Œä½ è¦æ ¹æ“šä½  dataset çš„ categories æ•¸é‡ï¼Œèª¿æ•´æœ€å¾Œä¸€å±¤ output layer çš„outputæ•¸é‡ã€‚</li>
</ol>
<p>è«‹æ³¨æ„ç¨‹å¼ç¢¼ä¸­æ¨™ç¤ºè¨»è§£ç®­é ­<code>&lt;====</code> çš„éƒ¨åˆ†</p>
</div>
<p>æ‰€ä»¥æˆ‘å€‘é€™é‚Šçš„ CNN æ¶æ§‹å¦‚ä¸‹ï¼Œ<code>ä½ å¯ä»¥æ ¹æ“šéœ€æ±‚æ±ºå®šæ˜¯å¦è¦åŸ·è¡Œ dropoutï¼Œä¾†è§£é–‹è¨»è§£</code>ã€‚<br>
ä½†æ˜¯åœ¨æˆ‘çš„æƒ…å¢ƒä¸­ï¼Œæˆ‘æ¸¬è©¦ dropout ä¸¦æ²’æœ‰å¸¶ä¾†æ¯”è¼ƒé«˜çš„æº–ç¢ºç‡ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NewNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NewNet, self).__init__()</span><br><span class="line">        <span class="comment"># Layer 1: 3x3 kernelï¼Œdepth = 32ï¼Œ224-3+1=222 =&gt; 222x222 pixel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        <span class="comment"># self.dropout1 = nn.Dropout(0.5) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 2: Max pooling with 2x2 kernelï¼Œ222/2=111 =&gt; 111x111 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 3: 3x3 kernelï¼Œdepth = 64ï¼Œ111-3+1=109 =&gt; 109x109 pixel</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        <span class="comment"># self.dropout2 = nn.Dropout(0.5) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 4: Max pooling with 2x2 kernelï¼Œ109/2=54 =&gt; 54x54 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 5: 3x3 kernelï¼Œdepth = 128ï¼Œ54-3+1=52 =&gt; 52x52 pixel</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        <span class="comment"># self.dropout3 = nn.Dropout(0.5) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Layer 6: Max pooling with 2x2 kernelï¼Œ52/2=26 =&gt; 26x26 pixel</span></span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Final input is 512ï¼Œpixel is 26*26 =&gt; 128*26*26</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">26</span> * <span class="number">26</span>, <span class="number">2048</span>) <span class="comment"># &lt;==== 128 * 26 * 26 æ ¹æ“š hidden layer ä¾†èª¿æ•´</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">1024</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">1024</span>, <span class="number">512</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">512</span>, <span class="number">102</span>)<span class="comment"># &lt;==== 102 æ ¹æ“š dataset çš„ç¨®é¡æ•¸é‡</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># We put the batch normalization before the activation function. </span></span><br><span class="line">        x = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout1(x) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout </span></span><br><span class="line">        x = F.relu(self.bn2(self.conv2(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout2(x) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout  </span></span><br><span class="line">        x = F.relu(self.bn3(self.conv3(x)))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># x = self.dropout3(x) # å¯æ ¹æ“šéœ€æ±‚å¥—ç”¨ dropout  </span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">26</span> * <span class="number">26</span>) <span class="comment"># &lt;==== 128 * 26 * 26 æ ¹æ“š hidden layer ä¾†èª¿æ•´</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.relu(self.fc3(x))</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = NewNet()</span><br><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>
<p>ä¸¦ä¸”æŒ‡å®š optimizer å’Œ loss functionï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="å»ºç«‹-Training-Func">å»ºç«‹ Training Func</h2>
<p>æˆ‘å€‘éœ€è¦å»ºç«‹ä¸€å€‹fucntionä¾†åŸ·è¡Œè¨“ç·´æ¨¡å‹çš„å‹•ä½œå¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch, start_time</span>):</span><br><span class="line">    net.train()</span><br><span class="line">    cur_count = <span class="number">0</span> </span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloaders[<span class="string">&quot;train&quot;</span>], <span class="number">0</span>):</span><br><span class="line">        cur_count += <span class="built_in">len</span>(data)</span><br><span class="line">        inputs, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        outputs.to(device)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.to(device)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">100</span> == <span class="number">99</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;[<span class="subst">&#123;epoch&#125;</span>, <span class="subst">&#123;batch_idx + <span class="number">1</span>:5d&#125;</span>] loss: <span class="subst">&#123;running_loss / <span class="number">100</span>:<span class="number">.3</span>f&#125;</span> time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time))&#125;</span> sec.&#x27;</span>)</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<h2 id="å»ºç«‹-Testing-Func">å»ºç«‹ Testing Func</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>(): </span><br><span class="line">    net.<span class="built_in">eval</span>()  <span class="comment"># set model to evaluation mode</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    class_correct = [<span class="number">0</span>] * <span class="built_in">len</span>(class_names)  </span><br><span class="line">    class_total = [<span class="number">0</span>] * <span class="built_in">len</span>(class_names)  </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dataloaders[<span class="string">&quot;val&quot;</span>]:</span><br><span class="line">            images, labels = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">            outputs = net(images) </span><br><span class="line"></span><br><span class="line">            <span class="comment"># select top 3 predictions</span></span><br><span class="line">            _, predicted = torch.topk(outputs, <span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># check if predicted labels are in true labels</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(labels)):</span><br><span class="line">                total += <span class="number">1</span></span><br><span class="line">                class_total[labels[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> labels[i] <span class="keyword">in</span> predicted[i]:</span><br><span class="line">                    correct += <span class="number">1</span></span><br><span class="line">                    class_correct[labels[i]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    class_accuracies = [class_correct[i] / class_total[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(class_names))]</span><br><span class="line">    accuracy = correct / total</span><br><span class="line">    <span class="keyword">return</span> accuracy, class_accuracies</span><br></pre></td></tr></table></figure>
<h2 id="åŸ·è¡Œ-Training">åŸ·è¡Œ Training</h2>
<p>ç‚ºäº†è®“è¨“ç·´éç¨‹ä¸­ï¼Œæˆ‘å€‘å¯ä»¥çœ‹åˆ°è¨“ç·´çš„ç‹€æ³ï¼Œæ‰€ä»¥æˆ‘å€‘æ¯ 100 å€‹ batch å°±å°å‡ºä¸€æ¬¡è¨“ç·´çš„ç‹€æ³ï¼Œä¸¦ä¸”æ¯ 5 å€‹ epoch å°±å°å‡ºä¸€æ¬¡ test çš„ç‹€æ³ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">100</span> </span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line">accuracy, class_accuracies = test()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test data (top-1): <span class="subst">&#123;<span class="number">100</span> * accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_epochs - <span class="number">1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;============ Epoch: <span class="subst">&#123;epoch&#125;</span> ==========&quot;</span>)</span><br><span class="line">    train(epoch, start_time)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ¯ 5 å€‹ epoch åŸ·è¡Œä¸€æ¬¡ testï¼Œçœ‹ä¸€ä¸‹è¨“ç·´ç‹€æ³</span></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">        accuracy, class_accuracies = test()</span><br><span class="line">        <span class="comment"># print accuracies</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Accuracy on test data (top-1): <span class="subst">&#123;<span class="number">100</span> * accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Finished Training. Total elapsed time: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - start_time) / <span class="number">60</span>, <span class="number">1</span>)&#125;</span> min&#x27;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Accuracy on test data (top<span class="number">-1</span>): <span class="number">0.0019</span>%</span><br><span class="line">============ Epoch: <span class="number">0</span> ==========</span><br><span class="line">[<span class="number">0</span>,   <span class="number">100</span>] loss: <span class="number">4.984</span> <span class="built_in">time</span> elapsed: <span class="number">40</span> sec.</span><br><span class="line">[<span class="number">0</span>,   <span class="number">200</span>] loss: <span class="number">4.876</span> <span class="built_in">time</span> elapsed: <span class="number">47</span> sec.</span><br><span class="line">...</span><br><span class="line">Accuracy on test data (top<span class="number">-1</span>): <span class="number">35.59</span>%</span><br><span class="line">Finished Training. Total elapsed <span class="built_in">time</span>: <span class="number">67</span> <span class="built_in">min</span></span><br></pre></td></tr></table></figure>
<h1 id="Task-5-4-Transfer-Learningï¼šResnet18">Task 5 &amp; 4 - Transfer Learningï¼šResnet18</h1>
<div class="note info flat"><ol start="4">
<li><strong>ä½¿ç”¨datasetè¨“ç·´æ¨¡å‹ä¸¦å°å‡ºTestingçš„æº–ç¢ºç‡</strong>ï¼šTrain the model on the dataset and measure the accuracy on hold out test data.</li>
<li><strong>ä½¿ç”¨ResNet18ä¾†é€²è¡ŒTransfer-Learning</strong>ï¼šNow use transfer learning to use a pre-trained ResNet18 on the dataset as follows:
<ol>
<li><strong>æŠŠåƒæ•¸fixed</strong>ï¼šResNet18 as fixed feature extractor.</li>
<li><strong>ä½¿ç”¨RestNeté€²è¡ŒFineturned</strong>ï¼šResNet18 finetuned on the training data.</li>
</ol>
</li>
</ol>
</div>
<h2 id="å»ºç«‹-Trainning-Testing-Func">å»ºç«‹ Trainning &amp; Testing Func</h2>
<p>æ ¹æ“šå®˜æ–¹çš„ç¯„ä¾‹<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning</a>ï¼Œæˆ‘æ˜¯ç›´æ¥è¤‡è£½éä¾†çš„ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, criterion, optimizer, scheduler, num_epochs=<span class="number">25</span></span>):</span><br><span class="line">    <span class="comment"># è¨­å®šé–‹å§‹æ™‚é–“ï¼Œç”¨æ–¼logå°å‡ºä»¥çœ‹æ¯å€‹Epochçš„è¨“ç·´æ™‚é–“</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å»ºç«‹ä¸€å€‹æš«å­˜çš„è³‡æ–™å¤¾ï¼Œç”¨æ–¼å­˜æ”¾æœ€å¥½çš„æ¨¡å‹åƒæ•¸</span></span><br><span class="line">    <span class="keyword">with</span> TemporaryDirectory() <span class="keyword">as</span> tempdir:</span><br><span class="line">        best_model_params_path = os.path.join(tempdir, <span class="string">&#x27;best_model_params.pt&#x27;</span>)</span><br><span class="line">        <span class="comment"># é‚„æ²’è¨“ç·´ï¼Œä½†æ˜¯æˆ‘å€‘å…ˆå­˜ç•¶å‰çš„æ¨¡å‹</span></span><br><span class="line">        torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line">        best_acc = <span class="number">0.0</span> <span class="comment"># è¨­å®šç›®å‰æœ€ä½³çš„ accuracy æ˜¯ 0ï¼Œä¸€ä½†æ¯”é€™å€‹æ•¸å­—å¤§ï¼Œå°±æœƒæ›´æ–°è©²æ•¸å€¼ä»¥åˆ¤æ–·ç›®å‰æœ€å¥½çš„æ¨¡å‹</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs - <span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># æ¯ä¸€å€‹ epoch ä¸€ä½† train å®Œï¼Œå°±æœƒé€²è¡Œ validation</span></span><br><span class="line">            <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">                <span class="comment"># åˆ¤æ–·ç›®å‰æ‡‰è©²æ˜¯ training é‚„æ˜¯ validation </span></span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                    model.train()  <span class="comment"># Set model to training mode</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>()   <span class="comment"># Set model to evaluate mode</span></span><br><span class="line"></span><br><span class="line">                running_loss = <span class="number">0.0</span></span><br><span class="line">                running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Iterate over data.</span></span><br><span class="line">                <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                    <span class="comment"># æ”¾åˆ° gpu ä¸­</span></span><br><span class="line">                    inputs = inputs.to(device)</span><br><span class="line">                    labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># æ­¸é›¶æ¢¯åº¦</span></span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># åŸ·è¡Œ forward propagation</span></span><br><span class="line">                    <span class="comment"># track history if only in train</span></span><br><span class="line">                    <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                        outputs = model(inputs)</span><br><span class="line">                        _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>) <span class="comment"># é¸æœ€å¤§çš„é‚£å€‹æ•¸å­—ç•¶ä½œé æ¸¬çš„ label </span></span><br><span class="line">                        loss = criterion(outputs, labels) <span class="comment"># è¨ˆç®—ç­”æ¡ˆå’Œé æ¸¬çš„å·®è· </span></span><br><span class="line"></span><br><span class="line">                        <span class="comment"># backward + optimize only if in training phase</span></span><br><span class="line">                        <span class="comment"># åŸ·è¡Œ backward propagation</span></span><br><span class="line">                        <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                            loss.backward()</span><br><span class="line">                            optimizer.step()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># å› ç‚º batch_size æ˜¯ 4ï¼Œæ‰€ä»¥ loss ä¹˜ä¸Š 4ï¼Œæ‰æ˜¯ä¸€å€‹ batch çš„ loss</span></span><br><span class="line">                    running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                    <span class="comment"># ç®—å‡ºä¸€å€‹ batch ä¸­ï¼Œæœ‰å¤šå°‘ç­”å°çš„</span></span><br><span class="line">                    running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># åªæœ‰åœ¨ training çš„æ™‚å€™ï¼Œæ‰æœƒèª¿æ•´ learning rate</span></span><br><span class="line">                <span class="comment"># scheduler æ˜¯å­¸ç¿’ç‡lrèª¿æ•´å™¨ ç”¨æ–¼åœ¨æ¨¡å‹è¨“ç·´éç¨‹ä¸­èª¿æ•´å­¸ç¿’ç‡lr çš„å€¼</span></span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                    scheduler.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># ä¸€æ•´å€‹ epoch è¨“ç·´å®Œå¾Œï¼Œç®—å‡ºè©² epoch çš„ loss å’Œ accuracy</span></span><br><span class="line">                <span class="comment"># Avg. loss = å…¨éƒ¨çš„ loss / æ­£å€‹ dataset çš„å¤§å° </span></span><br><span class="line">                epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">                <span class="comment"># Avg. Acc = å…¨éƒ¨çš„ç­”å°æ•¸ / æ­£å€‹ dataset çš„å¤§å°</span></span><br><span class="line">                epoch_acc = running_corrects.<span class="built_in">float</span>() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;phase&#125;</span> Loss: <span class="subst">&#123;epoch_loss:<span class="number">.4</span>f&#125;</span> Acc: <span class="subst">&#123;epoch_acc:<span class="number">.4</span>f&#125;</span> Time elapsed: <span class="subst">&#123;<span class="built_in">round</span>((time.time() - since))&#125;</span> sec.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># å¦‚æœåœ¨ validation çš„æ™‚å€™ï¼Œä¸€ä½†ç™¼ç¾ accuracy æ¯”ç›®å‰æœ€å¥½çš„é‚„è¦å¥½ï¼Œå°±æŠŠæ¨¡å‹åƒæ•¸å­˜èµ·ä¾†</span></span><br><span class="line">                <span class="keyword">if</span> phase == <span class="string">&#x27;val&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">                    <span class="comment"># æ›´æ–°ç›®å‰æœ€å¥½çš„ accuracy</span></span><br><span class="line">                    best_acc = epoch_acc</span><br><span class="line">                    <span class="comment"># deep copy the model</span></span><br><span class="line">                    torch.save(model.state_dict(), best_model_params_path)</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">        time_elapsed = time.time() - since</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Training complete in <span class="subst">&#123;time_elapsed // <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>m <span class="subst">&#123;time_elapsed % <span class="number">60</span>:<span class="number">.0</span>f&#125;</span>s&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Best val Acc: <span class="subst">&#123;best_acc:4f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load best model weights</span></span><br><span class="line">        <span class="comment"># ä»¥ç›®å‰æœ€å¥½çš„ model å–å‡ºä¾†ï¼Œç¹¼çºŒä¸‹ä¸€å€‹ epoch çš„è¨“ç·´</span></span><br><span class="line">        model.load_state_dict(torch.load(best_model_params_path))</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h2 id="ä½¿ç”¨-Transfer-Learning">ä½¿ç”¨ Transfer Learning</h2>
<p>æ ¹æ“šè€å¸«çš„è¦æ±‚ï¼Œè¦ä½¿ç”¨ resnet18 ä¾†é€²è¡Œ Transfer Learningï¼Œç›®å‰æ ¹æ“š<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights">å®˜æ–¹èªªæ˜</a>ï¼Œ<code>resent18</code> å¦‚æœä¸çµ¦äºˆåƒæ•¸ï¼Œå‰‡é è¨­å°±æ˜¯ <code>IMAGENET1K_V1</code>ï¼Œç‚ºäº†æ¸…æ¥šæˆ‘å€‘åˆ°åº•ä½¿ç”¨å“ªä¸€å€‹ model çš„åƒæ•¸ï¼Œæˆ‘å€‘é‚„æ˜¯çµ¦äºˆåƒæ•¸ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># num_ftrs is the number of input features for the last layer. </span></span><br><span class="line"><span class="comment"># æŠ“å–æœ€å¾Œä¸€å±¤çš„è¼¸å…¥æ•¸é‡</span></span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># Here the size of each output sample is set to 102.</span></span><br><span class="line"><span class="comment"># model_ft.fc is the final layer of the model, and used for classification.</span></span><br><span class="line"><span class="comment"># è‡ªå·±å»ºç«‹æœ€å¾Œä¸€å±¤ layerï¼Œä¸¦ä¸”æŠŠè¼¸å…¥æ•¸é‡è¨­å®šç‚º num_ftrsï¼Œè¼¸å‡ºæ•¸é‡è¨­å®šç‚º 102ï¼ˆå› ç‚ºé€™å€‹caseæœ‰102å€‹ï¼‰</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function ä½¿ç”¨ CrossEntropyLoss </span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line"><span class="comment"># optimizer ä½¿ç”¨ SGDï¼Œlearning rate = 0.001ï¼Œmomentum = 0.9</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line"><span class="comment"># æ¯ 7 å€‹ epoch å°±æŠŠ learning rate ä¹˜ä¸Š 0.1 ä¾†å° lr é€²è¡Œ decay</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>å¤§æ¦‚æ˜¯é€™ç¨®æ„Ÿè¦ºä¾†é€²è¡ŒTransfer Learning<br>
<img src="/img/loading-icon.gif" data-original="https://i.imgur.com/rygK4KS.png" alt=""></p>
<div class="note warning flat"><p><strong>ç‚ºä»€éº¼è¦èª¿æ•´lr?</strong><br>
å°‡å­¸ç¿’ç‡æ¯éš”ä¸€å®šçš„ epoch é€²è¡Œèª¿æ•´æ˜¯ä¸€ç¨®å¸¸è¦‹çš„å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥ï¼Œç¨±ç‚ºå­¸ç¿’ç‡è¡°æ¸›ï¼ˆlearning rate decayï¼‰æˆ–å­¸ç¿’ç‡èª¿åº¦ï¼ˆlearning rate schedulingï¼‰ã€‚é€™æ¨£çš„æ•ˆæœæ˜¯ï¼š</p>
<ol>
<li>
<p>æé«˜æ¨¡å‹çš„ç©©å®šæ€§ï¼šåœ¨è¨“ç·´éç¨‹ä¸­ï¼Œ<code>ä¸€é–‹å§‹ä½¿ç”¨ç›¸å°è¼ƒå¤§çš„å­¸ç¿’ç‡ï¼Œæœ‰åŠ©æ–¼å¿«é€Ÿæ”¶æ–‚</code>ã€‚ä½†ç•¶è¨“ç·´<code>é è¿‘æœ€ä½³è§£æ™‚ï¼Œè¼ƒå¤§çš„å­¸ç¿’ç‡å¯èƒ½å°è‡´æ¨¡å‹åœ¨æœ€ä½³è§£é™„è¿‘éœ‡ç›ªæˆ–éåº¦èª¿æ•´</code>ã€‚é€éé€±æœŸæ€§åœ°é™ä½å­¸ç¿’ç‡ï¼Œæ¨¡å‹åœ¨è¨“ç·´çš„å¾ŒæœŸæœƒæ›´ç©©å®šï¼Œæ›´æ¥è¿‘æœ€ä½³è§£ã€‚</p>
</li>
<li>
<p>é˜²æ­¢éåº¦æ“¬åˆï¼š<code>é€±æœŸæ€§åœ°é™ä½å­¸ç¿’ç‡æœ‰åŠ©æ–¼é˜²æ­¢æ¨¡å‹åœ¨è¨“ç·´é›†ä¸Šéåº¦æ“¬åˆ</code>ã€‚ç•¶å­¸ç¿’ç‡é™ä½æ™‚ï¼Œæ¨¡å‹æ›´è¬¹æ…åœ°èª¿æ•´åƒæ•¸ï¼Œä¸å¤ªå®¹æ˜“é™·å…¥è¨“ç·´é›†ä¸­çš„å™ªè²ã€‚</p>
</li>
</ol>
<p>åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå­¸ç¿’ç‡èª¿æ•´ç­–ç•¥çš„å…·é«”è¨­ç½®ï¼ˆä¾‹å¦‚ï¼Œ<code>step_size</code> å’Œ <code>gamma</code> çš„å€¼ï¼‰é€šå¸¸æ˜¯æ ¹æ“šè©¦é©—å’Œç¶“é©—ä¾†èª¿æ•´çš„ï¼Œä»¥é”åˆ°æœ€ä½³æ€§èƒ½ã€‚é€šå¸¸ï¼Œé€™äº›åƒæ•¸çš„è¨­ç½®å–æ±ºæ–¼ä½ çš„æ•¸æ“šé›†å¤§å°ã€æ¨¡å‹æ¶æ§‹ã€å•é¡Œçš„é›£åº¦å’Œå…¶ä»–å› ç´ ã€‚</p>
</div>
<h2 id="é–‹å§‹è¨“ç·´">é–‹å§‹è¨“ç·´</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹: æº–ç¢ºç‡ 89.41% æŒºå¥½çš„</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.4280</span> Acc: <span class="number">0.0657</span> Time elapsed: <span class="number">33</span> sec.</span><br><span class="line">val Loss: <span class="number">2.9901</span> Acc: <span class="number">0.3118</span> Time elapsed: <span class="number">58</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.3046</span> Acc: <span class="number">0.2353</span> Time elapsed: <span class="number">87</span> sec.</span><br><span class="line">val Loss: <span class="number">1.6604</span> Acc: <span class="number">0.5941</span> Time elapsed: <span class="number">112</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.5080</span> Acc: <span class="number">0.4029</span> Time elapsed: <span class="number">141</span> sec.</span><br><span class="line">val Loss: <span class="number">1.2243</span> Acc: <span class="number">0.6951</span> Time elapsed: <span class="number">166</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.9871</span> Acc: <span class="number">0.5196</span> Time elapsed: <span class="number">195</span> sec.</span><br><span class="line">val Loss: <span class="number">0.9578</span> Acc: <span class="number">0.7216</span> Time elapsed: <span class="number">219</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.5865</span> Acc: <span class="number">0.6225</span> Time elapsed: <span class="number">249</span> sec.</span><br><span class="line">val Loss: <span class="number">0.6911</span> Acc: <span class="number">0.8108</span> Time elapsed: <span class="number">273</span> sec.</span><br><span class="line">...</span><br><span class="line">val Loss: <span class="number">0.3919</span> Acc: <span class="number">0.8912</span> Time elapsed: <span class="number">1313</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">21</span>m <span class="number">53</span>s</span><br><span class="line">Best val Acc: <span class="number">0.894118</span></span><br></pre></td></tr></table></figure>
<h2 id="ä½¿ç”¨-ResNet18-ä½œç‚º-fixed-feature-extractor">ä½¿ç”¨ ResNet18 ä½œç‚º fixed feature extractor</h2>
<p>å› ç‚ºä½œæ¥­æœ‰è¦æ±‚ï¼Œè¦ä½¿ç”¨ ResNet18 ä½œç‚º fixed feature extractorï¼Œæ‰€ä»¥æˆ‘å€‘è¦æŠŠæ‰€æœ‰çš„åƒæ•¸éƒ½è¨­å®šç‚ºä¸å¯è¨“ç·´ï¼Œåªæœ‰æœ€å¾Œä¸€å±¤çš„åƒæ•¸æ˜¯å¯ä»¥è¨“ç·´çš„ï¼Œ<strong>ç°¡å–®ä¾†èªªå°±æ˜¯åˆ¥äººè¨“ç·´å¥½çš„ model ä½ å°±ä¸è¦æ”¹äººå®¶çš„ weight äº†æ‹‰</strong>ã€‚è¦æ”¹çš„åœ°æ–¹å°±æ˜¯ï¼ŒæŠŠ model çš„æ¯å€‹ parameters çš„ <code>requires_grad</code> éƒ½è¨­å®šç‚º Falseã€‚é€™æ¨£ï¼Œæˆ‘å€‘å°±å¯ä»¥æŠŠ ResNet18 ç•¶ä½œ fixed feature extractor ä¾†ä½¿ç”¨ã€‚</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å…¶ä»–éƒ½è€æ¨£å­</span></span><br><span class="line">model_conv = torchvision.models.resnet18(weights=<span class="string">&#x27;IMAGENET1K_V1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># !!! æ·»åŠ é€™å…©è¡Œï¼ŒæŠŠ requires_grad è¨­å®šç‚º Falseï¼Œé€™æ¨£å°±ä¸æœƒæ›´æ–°è©²åƒæ•¸äº†</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># å…¶ä»–éƒ½è€æ¨£å­</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç„¶å¾Œæˆ‘å€‘é–‹å§‹è¨“ç·´ </span></span><br><span class="line">model_conv_SGD = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹ï¼šæº–ç¢ºç‡ 79.11% æ¯”è¼ƒå·®ä¸€é»ï¼Œä½†æ˜¯é€™æ¨£çš„è¨“ç·´é€Ÿåº¦æœƒæ¯”è¼ƒå¿«</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.6979</span> Acc: <span class="number">0.0176</span> Time elapsed: <span class="number">24</span> sec.</span><br><span class="line">val Loss: <span class="number">3.9863</span> Acc: <span class="number">0.1235</span> Time elapsed: <span class="number">48</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.0589</span> Acc: <span class="number">0.1137</span> Time elapsed: <span class="number">72</span> sec.</span><br><span class="line">val Loss: <span class="number">3.1125</span> Acc: <span class="number">0.3608</span> Time elapsed: <span class="number">95</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.4935</span> Acc: <span class="number">0.2304</span> Time elapsed: <span class="number">119</span> sec.</span><br><span class="line">val Loss: <span class="number">2.5003</span> Acc: <span class="number">0.4912</span> Time elapsed: <span class="number">142</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.1030</span> Acc: <span class="number">0.3422</span> Time elapsed: <span class="number">165</span> sec.</span><br><span class="line">val Loss: <span class="number">2.1583</span> Acc: <span class="number">0.5510</span> Time elapsed: <span class="number">189</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.7367</span> Acc: <span class="number">0.4402</span> Time elapsed: <span class="number">212</span> sec.</span><br><span class="line">val Loss: <span class="number">1.7064</span> Acc: <span class="number">0.6304</span> Time elapsed: <span class="number">236</span> sec.</span><br><span class="line">...</span><br><span class="line">val Loss: <span class="number">1.0910</span> Acc: <span class="number">0.7824</span> Time elapsed: <span class="number">1179</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">19</span>m <span class="number">39</span>s</span><br><span class="line">Best val Acc: <span class="number">0.791176</span></span><br></pre></td></tr></table></figure>
<h1 id="Task-6-4-Transfer-Learningï¼šEfficientNet-B5">Task 6 &amp; 4 - Transfer Learningï¼šEfficientNet_B5</h1>
<div class="note info flat"><ol start="4">
<li><strong>ä½¿ç”¨datasetè¨“ç·´æ¨¡å‹ä¸¦å°å‡ºTestingçš„æº–ç¢ºç‡</strong>ï¼šTrain the model on the dataset and measure the accuracy on hold out test data.</li>
<li><strong>ä½¿ç”¨EfficientNet_B5é€²è¡ŒFineturned</strong>ï¼šRepeat step 4 but now use EfficientNet_B5 instead of RestNet18.</li>
</ol>
</div>
<p>æ¥ä¸‹ä¾†ï¼Œæˆ‘å€‘éœ€è¦æŠŠ RestNet18 æ ¹æ“šé¡Œç›®è¦æ±‚æ›æˆåˆ¥çš„è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œä½ å¯èƒ½æœƒéœ€è¦å…ˆé€é pip å®‰è£ <code>efficientnet_pytorch</code>ï¼š</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install efficientnet_pytorch</span><br></pre></td></tr></table></figure>
<p>ç„¶å¾Œå†åŸ·è¡Œä¸‹é¢çš„ç¨‹å¼ç¢¼ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> efficientnet_pytorch <span class="keyword">import</span> EfficientNet</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained EfficientNet-B5 model</span></span><br><span class="line">model_ft = EfficientNet.from_pretrained(<span class="string">&#x27;efficientnet-b5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸€æ¨£ï¼Œæˆ‘å€‘å…ˆå–å¾—è©² model æœ€å¾Œä¸€å±¤ layer çš„è¼¸å…¥æ•¸é‡</span></span><br><span class="line">num_ftrs = model_ft._fc.in_features</span><br><span class="line"><span class="comment"># å»ºç«‹ä¸€å€‹æ–°çš„ layerï¼Œè¼¸å…¥æ•¸é‡æ˜¯ num_ftrsï¼Œè¼¸å‡ºæ•¸é‡æ˜¯ 102ï¼ˆå› ç‚ºé€™å€‹caseæœ‰102å€‹ï¼‰</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">102</span>)</span><br><span class="line"><span class="comment"># æŠŠ model æ”¾åˆ° GPU ä¸­</span></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Observe that all parameters are being optimized</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Decay LR by a factor of 0.1 every 7 epochs</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é–‹å§‹è¨“ç·´</span></span><br><span class="line">model_ft_effb5 = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>çµæœå¦‚ä¸‹ï¼šæº–ç¢ºç‡ 73.33% å¥½åƒæ²’æœ‰æ¯”è¼ƒå¥½ã€‚</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">Epoch <span class="number">0</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">6.2860</span> Acc: <span class="number">0.0157</span> Time elapsed: <span class="number">156</span> sec.</span><br><span class="line">val Loss: <span class="number">5.6637</span> Acc: <span class="number">0.0382</span> Time elapsed: <span class="number">200</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">1</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">4.8955</span> Acc: <span class="number">0.1039</span> Time elapsed: <span class="number">322</span> sec.</span><br><span class="line">val Loss: <span class="number">4.5101</span> Acc: <span class="number">0.2392</span> Time elapsed: <span class="number">365</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">2</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.8566</span> Acc: <span class="number">0.2422</span> Time elapsed: <span class="number">485</span> sec.</span><br><span class="line">val Loss: <span class="number">3.6194</span> Acc: <span class="number">0.4265</span> Time elapsed: <span class="number">529</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">3</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">3.0979</span> Acc: <span class="number">0.3637</span> Time elapsed: <span class="number">653</span> sec.</span><br><span class="line">val Loss: <span class="number">2.8613</span> Acc: <span class="number">0.5539</span> Time elapsed: <span class="number">696</span> sec.</span><br><span class="line"></span><br><span class="line">Epoch <span class="number">4</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">2.4323</span> Acc: <span class="number">0.4725</span> Time elapsed: <span class="number">818</span> sec.</span><br><span class="line">val Loss: <span class="number">2.2894</span> Acc: <span class="number">0.6725</span> Time elapsed: <span class="number">863</span> sec.</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Epoch <span class="number">24</span>/<span class="number">24</span></span><br><span class="line"><span class="comment">----------</span></span><br><span class="line">train Loss: <span class="number">1.3168</span> Acc: <span class="number">0.7343</span> Time elapsed: <span class="number">4769</span> sec.</span><br><span class="line">val Loss: <span class="number">1.3167</span> Acc: <span class="number">0.8167</span> Time elapsed: <span class="number">4812</span> sec.</span><br><span class="line"></span><br><span class="line">Training complete <span class="keyword">in</span> <span class="number">80</span>m <span class="number">12</span>s</span><br><span class="line">Best val Acc: <span class="number">0.821569</span></span><br></pre></td></tr></table></figure>
<h1 id="Task-7-è¨è«–">Task 7 - è¨è«–</h1>
<div class="note info flat"><ol start="7">
<li><strong>æ¯”è¼ƒé€™äº›ä¸åŒçš„æ–¹æ³•ï¼Œä¸¦åˆ—å°å‡ºæº–ç¢ºåº¦</strong>ï¼šCompare the accuracy of the different approaches on the test data and print out the training</li>
</ol>
</div>
<p>å¾å‰é¢é–‹å§‹ï¼Œæˆ‘å€‘æ¸¬è©¦äº†å¹¾å€‹æ–¹æ³•ï¼š</p>
<ul>
<li><a href="#Task-3-4-CNN-Batch-Normalization">ä½¿ç”¨è‡ªå·±å»ºç«‹çš„ CNN</a></li>
<li><a href="#Task-5-4-Transfer-Learning%EF%BC%9AResnet18">ä½¿ç”¨Transfer Learning Resnet18</a></li>
<li><a href="#Task-6-4-Transfer-Learning%EF%BC%9AEfficientNet-B5">ä½¿ç”¨Transfer Learning EfficientNet-B5</a></li>
</ul>
<p>ä»–å€‘çš„æ•¸æ“šå¤§æ¦‚å¦‚ä¸‹ï¼š</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Training Time</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>è‡ªå»º CNN</td>
<td><code>35%</code></td>
<td><code>è¶…é1å°æ™‚</code></td>
<td>æœ€å·®</td>
</tr>
<tr>
<td>Resnet18</td>
<td><strong>89.41%</strong></td>
<td>21åˆ†é˜</td>
<td>æº–ç¢ºç‡æœ€é«˜</td>
</tr>
<tr>
<td>Resnet18 (fixed feature extractor)</td>
<td>79.11%</td>
<td><strong>19åˆ†é˜</strong></td>
<td>æ™‚é–“æœ€çŸ­</td>
</tr>
<tr>
<td>EfficientNet-B5</td>
<td>82.15%</td>
<td>80åˆ†é˜</td>
<td>æ™®æ™®</td>
</tr>
</tbody>
</table>
<div class="note warning flat"><p><strong>çµè«–</strong></p>
<ul>
<li>å¦‚æœä½¿ç”¨ Transfer Learning å¯æ˜é¡¯æ„Ÿå—åˆ°ï¼Œæº–ç¢ºç‡æ˜é¡¯æé«˜ï¼Œä¸¦ä¸”è¨“ç·´æ™‚é–“å¤§å¹…ç¸®çŸ­ã€‚</li>
<li>å†è€…ï¼Œä»¥ç›®å‰çš„æ¡ˆä¾‹ä¾†èªªï¼Œä¸è¦fixed model çš„åƒæ•¸ï¼Œæº–ç¢ºç‡æ¯”è¼ƒå¥½ï¼Œé›–ç„¶ç›¸å°çš„æ™‚é–“ä¹Ÿæœƒæ¯”è¼ƒé•·ï¼Œå› ç‚ºè¦åšgradient descentã€‚</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io">Shannon Hung</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é€£çµ: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io/posts/flower102-transfer-learning/">https://shannonhung.github.io/posts/flower102-transfer-learning/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæ¬Šè²æ˜: </span><span class="post-copyright-info">æœ¬éƒ¨è½æ ¼æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥è²æ˜å¤–ï¼Œå‡æ¡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> è¨±å¯å”è­°ã€‚è½‰è¼‰è«‹è¨»æ˜ä¾†è‡ª <a href="https://shannonhung.github.io" target="_blank">Shannon's Blog ğŸŸ æŠ€è¡“ | ç”Ÿæ´» | æ—…è¡Œ</a>ï¼ <em>å¦‚æœä½ è¦ºå¾—æˆ‘çš„æ–‡ç« æœ‰å¹«åŠ©ï¼Œå¸Œæœ›ä½ å¯ä»¥åˆ°æˆ‘çš„ github çµ¦æˆ‘ä¸€å€‹ star â­ï¸ <a href="https://github.com/ShannonHung/ShannonHung.github.io" rel="external nofollow noreferrer" target="_blank">Shannon Blog Repo</a></em></span></div></div><div class="post_share"><div class="social-share" data-image="/img/cover/flower-ml.jpeg" data-sites="facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - ä½¿ç”¨ Pytorch æ­å»º CNN + å•Ÿç”¨ GPU + çµæœå±•ç¤ºè‡³ TensorBoard"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/CNN-bg.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">CIFAR10 Dataset - ä½¿ç”¨ Pytorch æ­å»º CNN + å•Ÿç”¨ GPU + çµæœå±•ç¤ºè‡³ TensorBoard</div></div></a></div><div class="next-post pull-right"><a href="/posts/coco-object-diagnoise/" title="COCO Dataset - ä½¿ç”¨ Faster RCNN + MobileNet é€²è¡Œ Object Detection"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/object-detection.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">COCO Dataset - ä½¿ç”¨ Faster RCNN + MobileNet é€²è¡Œ Object Detection</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸é—œæ¨è–¦</span></div><div class="relatedPosts-list"><div><a href="/posts/ML/" title="All basic concept of Mechine Learning - ML çš„é‡é»çŸ¥è­˜æ•´ç†"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/ai-knowledge.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-30</div><div class="title">All basic concept of Mechine Learning - ML çš„é‡é»çŸ¥è­˜æ•´ç†</div></div></a></div><div><a href="/posts/bert-nbme-score-clinical-patient-notes/" title="NBME - ä½¿ç”¨ DeBERTa æ¨¡å‹åˆ†æç—…äººç—…ä¾‹"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/clinical-patient.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-05-30</div><div class="title">NBME - ä½¿ç”¨ DeBERTa æ¨¡å‹åˆ†æç—…äººç—…ä¾‹</div></div></a></div><div><a href="/posts/text-classification/" title="è³‡æ–™åˆ†ææ¦‚å¿µå¤§å…¨ - TF, IDF, TF-IDF, Text Classification æ¨£æ¨£ä¾†"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/ai-knowledge.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-01</div><div class="title">è³‡æ–™åˆ†ææ¦‚å¿µå¤§å…¨ - TF, IDF, TF-IDF, Text Classification æ¨£æ¨£ä¾†</div></div></a></div><div><a href="/posts/coco-object-diagnoise/" title="COCO Dataset - ä½¿ç”¨ Faster RCNN + MobileNet é€²è¡Œ Object Detection"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/object-detection.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-03</div><div class="title">COCO Dataset - ä½¿ç”¨ Faster RCNN + MobileNet é€²è¡Œ Object Detection</div></div></a></div><div><a href="/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - ä½¿ç”¨ LSTM é æ¸¬æ–‡ç« çš„æƒ…ç·’"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/lstm-emotion.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-16</div><div class="title">Twitter Dataset - ä½¿ç”¨ LSTM é æ¸¬æ–‡ç« çš„æƒ…ç·’</div></div></a></div><div><a href="/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - ä½¿ç”¨ Pytorch æ­å»º CNN + å•Ÿç”¨ GPU + çµæœå±•ç¤ºè‡³ TensorBoard"><img class="cover" src="/img/loading-icon.gif" data-original="/img/cover/CNN-bg.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-24</div><div class="title">CIFAR10 Dataset - ä½¿ç”¨ Pytorch æ­å»º CNN + å•Ÿç”¨ GPU + çµæœå±•ç¤ºè‡³ TensorBoard</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> è©•è«–</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/loading-icon.gif" data-original="/img/dudu-me.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shannon Hung</div><div class="author-info__description">çˆ²äº†é¿å…å¥å¿˜ï¼Œè€Œé–‹å§‹è¨˜éŒ„</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">93</div></a><a href="/tags/"><div class="headline">æ¨™ç±¤</div><div class="length-num">65</div></a><a href="/categories/"><div class="headline">åˆ†é¡</div><div class="length-num">32</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ShannonHung"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ShannonHung" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hsuanhung036@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://www.linkedin.com/in/hung-yi-hsuan/" rel="external nofollow noreferrer" target="_blank" title="LinkedIn"><i class="fab fa-linkedin" style="color: #0077b5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">æˆ‘æ˜¯è‡ºç£ç§‘æŠ€å¤§å­¸-è³‡ç®¡æ‰€çš„ç ”ç©¶ç”Ÿï¼Œä»Šå¹´é è¨ˆå‰å¾€å¾·åœ‹é›™è¯ã€‚é–‹å§‹æ¶è¨­æ­¤ç¶²ç«™çš„åŸå› æ˜¯ï¼Œå› ç‚ºè‡ªå·±çš„è¨˜æ†¶åŠ›å¤ªéæ–¼çŸ­æš«ï¼Œé›–ç„¶å­¸äº†å¾ˆå¤šï¼Œä½†ä¹Ÿå¿˜è¨˜å¾ˆå¤šã€‚ç‚ºäº†é¿å…å¥å¿˜ï¼Œé–‹å§‹æˆ‘çš„ç­†è¨˜ç”Ÿæ´»ã€‚</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®éŒ„</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">å‰è¨€</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%9C%E6%A5%AD%E8%A6%81%E6%B1%82"><span class="toc-text">ä½œæ¥­è¦æ±‚</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-0-import-package"><span class="toc-text">Task 0 - import package</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-1-%E9%81%B8%E6%93%87%E4%B8%80%E5%80%8BDataSet"><span class="toc-text">Task 1 - é¸æ“‡ä¸€å€‹DataSet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-2-%E5%8D%B0%E5%87%BA%E5%9C%96%E7%89%87%E5%92%8C%E8%B3%87%E6%96%99%E5%A4%A7%E5%B0%8F"><span class="toc-text">Task 2 - å°å‡ºåœ–ç‰‡å’Œè³‡æ–™å¤§å°</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-3-4-CNN-Batch-Normalization"><span class="toc-text">Task 3 &amp; 4 - CNN + Batch Normalization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B-Network"><span class="toc-text">å»ºç«‹ Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B-Training-Func"><span class="toc-text">å»ºç«‹ Training Func</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B-Testing-Func"><span class="toc-text">å»ºç«‹ Testing Func</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%B7%E8%A1%8C-Training"><span class="toc-text">åŸ·è¡Œ Training</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-5-4-Transfer-Learning%EF%BC%9AResnet18"><span class="toc-text">Task 5 &amp; 4 - Transfer Learningï¼šResnet18</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BB%BA%E7%AB%8B-Trainning-Testing-Func"><span class="toc-text">å»ºç«‹ Trainning &amp; Testing Func</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Transfer-Learning"><span class="toc-text">ä½¿ç”¨ Transfer Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%96%8B%E5%A7%8B%E8%A8%93%E7%B7%B4"><span class="toc-text">é–‹å§‹è¨“ç·´</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-ResNet18-%E4%BD%9C%E7%82%BA-fixed-feature-extractor"><span class="toc-text">ä½¿ç”¨ ResNet18 ä½œç‚º fixed feature extractor</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-6-4-Transfer-Learning%EF%BC%9AEfficientNet-B5"><span class="toc-text">Task 6 &amp; 4 - Transfer Learningï¼šEfficientNet_B5</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-7-%E8%A8%8E%E8%AB%96"><span class="toc-text">Task 7 - è¨è«–</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/transformers/" title="æ‰‹æŠŠæ‰‹å¸¶ä½ å¯¦ä½œ Hugging Face Transformers (å…¥é–€ç¯‡)"><img src="/img/loading-icon.gif" data-original="/img/cover/changing.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="æ‰‹æŠŠæ‰‹å¸¶ä½ å¯¦ä½œ Hugging Face Transformers (å…¥é–€ç¯‡)"/></a><div class="content"><a class="title" href="/posts/transformers/" title="æ‰‹æŠŠæ‰‹å¸¶ä½ å¯¦ä½œ Hugging Face Transformers (å…¥é–€ç¯‡)">æ‰‹æŠŠæ‰‹å¸¶ä½ å¯¦ä½œ Hugging Face Transformers (å…¥é–€ç¯‡)</a><time datetime="2025-12-31T06:27:13.000Z" title="ç™¼è¡¨æ–¼ 2025-12-31 14:27:13">2025-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/leetcode-sum-up/" title="LeetCode - åˆ·é¡Œä¹‹æ—…çš„ç¸½çµèˆ‡å¿ƒå¾—"><img src="/img/loading-icon.gif" data-original="/img/mountain-germany.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode - åˆ·é¡Œä¹‹æ—…çš„ç¸½çµèˆ‡å¿ƒå¾—"/></a><div class="content"><a class="title" href="/posts/leetcode-sum-up/" title="LeetCode - åˆ·é¡Œä¹‹æ—…çš„ç¸½çµèˆ‡å¿ƒå¾—">LeetCode - åˆ·é¡Œä¹‹æ—…çš„ç¸½çµèˆ‡å¿ƒå¾—</a><time datetime="2025-06-07T02:33:54.000Z" title="ç™¼è¡¨æ–¼ 2025-06-07 10:33:54">2025-06-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/leetcode-322-Coin-Change/" title="LeetCode #322 Coin Change - åˆ·é¡Œä¹‹æ—…"><img src="/img/loading-icon.gif" data-original="/img/cover/coins.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode #322 Coin Change - åˆ·é¡Œä¹‹æ—…"/></a><div class="content"><a class="title" href="/posts/leetcode-322-Coin-Change/" title="LeetCode #322 Coin Change - åˆ·é¡Œä¹‹æ—…">LeetCode #322 Coin Change - åˆ·é¡Œä¹‹æ—…</a><time datetime="2024-08-15T04:08:54.000Z" title="ç™¼è¡¨æ–¼ 2024-08-15 12:08:54">2024-08-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/leetcode-139-Word-Break/" title="LeetCode #139 Word Break - åˆ·é¡Œä¹‹æ—…"><img src="/img/loading-icon.gif" data-original="/img/cover/leetcode.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode #139 Word Break - åˆ·é¡Œä¹‹æ—…"/></a><div class="content"><a class="title" href="/posts/leetcode-139-Word-Break/" title="LeetCode #139 Word Break - åˆ·é¡Œä¹‹æ—…">LeetCode #139 Word Break - åˆ·é¡Œä¹‹æ—…</a><time datetime="2024-08-12T06:08:54.000Z" title="ç™¼è¡¨æ–¼ 2024-08-12 14:08:54">2024-08-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/leetcode-198-House-Robber/" title="LeetCode #198 House Robber - åˆ·é¡Œä¹‹æ—…"><img src="/img/loading-icon.gif" data-original="/img/cover/thief.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LeetCode #198 House Robber - åˆ·é¡Œä¹‹æ—…"/></a><div class="content"><a class="title" href="/posts/leetcode-198-House-Robber/" title="LeetCode #198 House Robber - åˆ·é¡Œä¹‹æ—…">LeetCode #198 House Robber - åˆ·é¡Œä¹‹æ—…</a><time datetime="2024-08-11T06:08:54.000Z" title="ç™¼è¡¨æ–¼ 2024-08-11 14:08:54">2024-08-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Shannon Hung</div><div class="footer_custom_text">Hi, welcome to Shannon <a target="_blank" rel="noopener external nofollow noreferrer" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é–±è®€æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="å–®æ¬„å’Œé›™æ¬„åˆ‡æ›"><i class="fas fa-arrows-alt-h"></i></button><a id="to_comment" href="#post-comment" title="ç›´é”è©•è«–"><i class="fas fa-comments"></i></a></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è¨­å®š"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®éŒ„"><i class="fas fa-list-ul"></i></button><button id="translateLink" type="button" title="ç°¡ç¹è½‰æ›">ç¹</button><button id="darkmode" type="button" title="æ·ºè‰²å’Œæ·±è‰²æ¨¡å¼è½‰æ›"><i class="fas fa-adjust"></i></button><button id="go-up" type="button" title="è¿”å›é ‚éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: '51917ecdc184bb98b226',
      clientSecret: 'e48b44c1908c14b74ff7513f06c7cb892a4f4748',
      repo: 'shannonhung.github.io',
      owner: 'ShannonHung',
      admin: ['ShannonHung'],
      id: '77690eac4a6ec422f10e47c072d2e8a4',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script data-pjax src="/self/btf.js"></script><script data-pjax src="/self/tw_en.js"></script><script id="canvas_nest" defer="defer" color="139,71,38" opacity="0.5" zIndex="-1" count="500" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœå°‹</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  è³‡æ–™åº«è¼‰å…¥ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœå°‹æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>