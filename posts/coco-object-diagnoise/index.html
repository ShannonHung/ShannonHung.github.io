<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection | Shannon's Blog üêü Tech | Life | Travel</title><meta name="author" content="Shannon Hung"><meta name="copyright" content="Shannon Hung"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Introduction Recently I took an AI course, the main content is the following topics:  Learn about Coco dataset User pre-trained version of Faster R-CNN to predict the bounding box Calculate IoU  Homew">
<meta property="og:type" content="article">
<meta property="og:title" content="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection">
<meta property="og:url" content="https://shannonhung.github.io/en/posts/coco-object-diagnoise/">
<meta property="og:site_name" content="Shannon&#39;s Blog üêü Tech | Life | Travel">
<meta property="og:description" content="Introduction Recently I took an AI course, the main content is the following topics:  Learn about Coco dataset User pre-trained version of Faster R-CNN to predict the bounding box Calculate IoU  Homew">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://shannonhung.github.io/en/img/cover/object-detection.jpeg">
<meta property="article:published_time" content="2023-11-03T02:57:54.000Z">
<meta property="article:modified_time" content="2023-12-09T22:10:02.898Z">
<meta property="article:author" content="Shannon Hung">
<meta property="article:tag" content="Mechine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://shannonhung.github.io/en/img/cover/object-detection.jpeg"><link rel="shortcut icon" href="/en/img/shannon-icon.png"><link rel="canonical" href="https://shannonhung.github.io/en/posts/coco-object-diagnoise/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="IAmwAuWZP3fXPtoYru7VJBancFMT2BkhN15HC2iea1o"/><link rel="stylesheet" href="/en/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-XBNKVVH2P4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-XBNKVVH2P4');
</script><script>const GLOBAL_CONFIG = {
  root: '/en/',
  algolia: undefined,
  localSearch: {"path":"/en/search.xml","preload":false,"top_n_per_article":-1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":3,"translateDelay":0,"msgToTraditionalChinese":"ÁπÅ","msgToSimplifiedChinese":"Á∞°"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-10 06:10:02'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/en/css/style.css"><link rel="stylesheet" href="/en/css/background.css"><link rel="shortcut icon" href="#"/></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/en/img/loading-icon.gif" data-original="/en/img/dudu-me.png" onerror="onerror=null;src='/en/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">13</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home Page</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categroies</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About Me</span></a></div><div class="menus_item"><a class="site-page" href="/en/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> Find Posts</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/tags/"><i class="fa-fw fas fa-tags"></i><span> By Tags</span></a></li><li><a class="site-page child" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> By Posts</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://shannonhung.github.io/"><i class="fa-fw fas fa-c"></i><span> ‰∏≠Êñá</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/en/img/cover/object-detection.jpeg')"><nav id="nav"><span id="blog-info"><a href="/en/" title="Shannon's Blog üêü Tech | Life | Travel"><span class="site-name">Shannon's Blog üêü Tech | Life | Travel</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-home"></i><span> Home Page</span></a></div><div class="menus_item"><a class="site-page" href="/en/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categroies</span></a></div><div class="menus_item"><a class="site-page" href="/en/about/"><i class="fa-fw fas fa-heart"></i><span> About Me</span></a></div><div class="menus_item"><a class="site-page" href="/en/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> Find Posts</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/tags/"><i class="fa-fw fas fa-tags"></i><span> By Tags</span></a></li><li><a class="site-page child" href="/en/archives/"><i class="fa-fw fas fa-archive"></i><span> By Posts</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-language"></i><span> Language</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/en/"><i class="fa-fw fas fa-e"></i><span> English</span></a></li><li><a class="site-page child" href="https://shannonhung.github.io/"><i class="fa-fw fas fa-c"></i><span> ‰∏≠Êñá</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-11-03T02:57:54.000Z" title="Created 2023-11-03 10:57:54">2023-11-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-12-09T22:10:02.898Z" title="Updated 2023-12-10 06:10:02">2023-12-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Code/">Code</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/en/categories/Code/Mechine-Learning/">Mechine Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>19mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/en/posts/coco-object-diagnoise/#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Introduction">Introduction</h1>
<p>Recently I took an AI course, the main content is the following topics:</p>
<ol>
<li>Learn about Coco dataset</li>
<li>User pre-trained version of Faster R-CNN to predict the bounding box</li>
<li>Calculate IoU</li>
</ol>
<h1 id="Homework-Requirement">Homework Requirement</h1>
<ol>
<li><strong>Download the Coco Collection</strong>*: download the files ‚Äú2017 Val images [5/1GB]‚Äù and ‚Äú2017 Train/Val annotations [241MB]‚Äù from the Coco page.<br>
Download from Coco page. You can load them into your notebook using the pycocotools library.</li>
<li><strong>Randomly select ten from the dataset</strong>: 10 images are randomly selected from this dataset.</li>
<li><strong>Predict the box using the pre-trained model FasterR-CNN</strong>: use a pre-trained version of the Faster R-CNN (Resnet50 backbone) to predict the bounding box of the object on the 10 images. of the bounding box. Only regions with scores greater than 0.8 are retained.</li>
<li><strong>isualize the model together with the answer visualization</strong>*: Visualize the predicted bounding boxes and label together with the ground truth bounding<br>
boxes and label. Show all 10 pairs of images side by side in the jupyter notebook.</li>
<li><strong>Use another pre-trained model Mobilnet</strong>: Repeat the above steps using the Mobilenet backbone of the Faster R-CNN.</li>
<li><strong>Calculate IoU Compare Models</strong>: Which backbone provides better results? Calculate the IoU for both methods.</li>
</ol>
<h1 id="Task-1-Downloading-the-COCO-Dataset">Task 1: Downloading the COCO Dataset</h1>
<div class="note info flat"><p><strong>Task 1</strong></p>
<ol>
<li><strong>Download the COCO Dataset</strong>: Obtain the files ‚Äú2017 Val images [5/1GB]‚Äù and ‚Äú2017 Train/Val annotations [241MB]‚Äù from the Coco page. Utilize the pycocotools library to import them into your notebook.</li>
</ol>
</div>
<p>You can follow this guide to proceed with the download: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://jason-chen-1992.weebly.com/home/coco-dataset">Download COCO Dataset</a><br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/BieHtLG.png" alt=""></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">‚îú‚îÄ‚îÄ annotations <span class="comment"># These are annotation files</span></span><br><span class="line">‚îÇ¬†¬† ‚îú‚îÄ‚îÄ captions_train2017.json</span><br><span class="line">‚îÇ¬†¬† ‚îú‚îÄ‚îÄ captions_val2017.json</span><br><span class="line">‚îÇ¬†¬† ‚îú‚îÄ‚îÄ instances_train2017.json</span><br><span class="line">‚îÇ¬†¬† ‚îú‚îÄ‚îÄ instances_val2017.json</span><br><span class="line">‚îÇ¬†¬† ‚îú‚îÄ‚îÄ person_keypoints_train2017.json</span><br><span class="line">‚îÇ¬†¬† ‚îî‚îÄ‚îÄ person_keypoints_val2017.json</span><br><span class="line">‚îî‚îÄ‚îÄ val2017 <span class="comment"># This is the image set </span></span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000139.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000285.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000632.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000724.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000776.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000785.jpg</span><br><span class="line">    ‚îú‚îÄ‚îÄ 000000000802.jpg</span><br><span class="line">    ... </span><br></pre></td></tr></table></figure>
<ul>
<li>Download these two files as shown in the image.</li>
<li>After downloading, the folder structure upon extraction will resemble the one above.</li>
</ul>
<h1 id="Task-2-Randomly-Select-Ten-Images">Task 2: Randomly Select Ten Images</h1>
<div class="note info flat"><p><strong>Task 2</strong><br>
2. <strong>Randomly Select Ten Images from the Dataset</strong>: Pick 10 images randomly from this dataset.</p>
</div>
<p>Here, we‚Äôll primarily do a few things:</p>
<ul>
<li>Import necessary libraries.</li>
<li>Set up the COCO API to allow it to access relevant information from our dataset, such as bounding box positions, label locations, and image information.</li>
<li>Visualize images and perform annotations.</li>
<li>Randomly select ten images.</li>
</ul>
<p>Let‚Äôs begin by importing the necessary libraries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># CNN </span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> torch.backends.cudnn <span class="keyword">as</span> cudnn</span><br><span class="line"></span><br><span class="line"><span class="comment"># others</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tempfile <span class="keyword">import</span> TemporaryDirectory</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># torchvision</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataset </span></span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cudnn.benchmark = <span class="literal">True</span></span><br><span class="line">plt.ion()   <span class="comment"># interactive mode</span></span><br></pre></td></tr></table></figure>
<h2 id="Setting-up-the-COCO-API">Setting up the COCO API</h2>
<p>COCO provides an API to access datasets. By providing it with a JSON file, we can easily retrieve the necessary information such as images, labels, bounding boxes, and more.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify dataset location</span></span><br><span class="line">cocoRoot = <span class="string">&quot;../../Data/Coco/&quot;</span></span><br><span class="line">dataType = <span class="string">&quot;val2017&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set annotation file location</span></span><br><span class="line">annFile = os.path.join(cocoRoot, <span class="string">f&#x27;annotations/instances_<span class="subst">&#123;dataType&#125;</span>.json&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Annotation file: <span class="subst">&#123;annFile&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># # initialize COCO api for instance annotations</span></span><br><span class="line">coco=COCO(annFile)</span><br><span class="line">coco </span><br></pre></td></tr></table></figure>
<blockquote>
<p>Result</p>
</blockquote>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Annotation file: ../../Data/Coco/annotations/instances_val2017.json</span><br><span class="line"><span class="comment">-- Indicates successful annotation file read</span></span><br><span class="line">loading annotations into memory...</span><br><span class="line">Done (t=<span class="number">0.35</span>s)</span><br><span class="line">creating index...</span><br><span class="line">index created!</span><br></pre></td></tr></table></figure>
<h2 id="Annotation-Visualization">Annotation Visualization</h2>
<p>To ensure familiarity with the COCO-provided API, here‚Äôs an exercise focusing on the following:</p>
<ul>
<li>Obtaining image info by ID</li>
<li>Retrieving annotation info by ID</li>
<li>Learning to draw bounding boxes and labels on images</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> Rectangle</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a function that, given an image ID, plots the image with bounding boxes and labels</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_image_with_annotations</span>(<span class="params">coco, cocoRoot, dataType, imgId, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Get image information</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgId)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Get image location for visualization</span></span><br><span class="line">    imPath = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])    </span><br><span class="line">    <span class="comment"># Read the image</span></span><br><span class="line">    im = cv2.imread(imPath)</span><br><span class="line">    <span class="comment"># Convert color space: OpenCV defaults to BGR, but matplotlib to RGB, so conversion is needed</span></span><br><span class="line">    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Find all annotations (bounding boxes) for the image</span></span><br><span class="line">    annIds = coco.getAnnIds(imgIds=imgInfo[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">    <span class="comment"># Load all annotation information: bounding box coordinates, labels, accuracies</span></span><br><span class="line">    anns = coco.loadAnns(annIds)</span><br><span class="line">    all_labels = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Extract bounding box coordinates, labels, accuracies</span></span><br><span class="line">    <span class="keyword">for</span> ann <span class="keyword">in</span> anns:</span><br><span class="line">        <span class="comment"># Specifically select information related to the bounding box: returns (x, y) of the lower-left corner, width, height</span></span><br><span class="line">        x, y, w, h = ann[<span class="string">&#x27;bbox&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get label text information: load category name by category ID</span></span><br><span class="line">        label = coco.loadCats(ann[<span class="string">&#x27;category_id&#x27;</span>])[<span class="number">0</span>][<span class="string">&quot;name&quot;</span>]</span><br><span class="line">        all_labels.add(label)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw bounding boxes using provided coordinates</span></span><br><span class="line">        rect = Rectangle((x, y), w, h, linewidth=<span class="number">2</span>, edgecolor=<span class="string">&#x27;r&#x27;</span>, facecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># Draw the image: if sorting of images is needed, ax parameter specifies the position</span></span><br><span class="line">        <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            plt.gca().add_patch(rect) </span><br><span class="line">            plt.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ax.add_patch(rect)</span><br><span class="line">            ax.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Display the image with a title</span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        plt.imshow(im)</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">f&#x27;Annotations: <span class="subst">&#123;all_labels&#125;</span>&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_title(<span class="string">f&#x27;Annotations: <span class="subst">&#123;all_labels&#125;</span>&#x27;</span>, color=<span class="string">&#x27;r&#x27;</span>, loc=<span class="string">&#x27;center&#x27;</span>, pad=<span class="number">20</span>)</span><br><span class="line">        ax.imshow(im)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the tenth image</span></span><br><span class="line">imgIds = coco.getImgIds()</span><br><span class="line">imgId = imgIds[<span class="number">10</span>]</span><br><span class="line"><span class="comment"># Plot the image with annotations</span></span><br><span class="line">plot_image_with_annotations(coco, cocoRoot, dataType, imgId)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Result</p>
</blockquote>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/C0nZWY9.png" alt=""></p>
<h2 id="Randomly-Select-Ten-Images">Randomly Select Ten Images</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_select</span>(<span class="params">coco, cocoRoot, dataType, num_images=<span class="number">10</span></span>):</span><br><span class="line">    <span class="comment"># Get IDs for all images</span></span><br><span class="line">    imgIds = coco.getImgIds()</span><br><span class="line">    <span class="comment"># Randomly select num_images IDs from this set</span></span><br><span class="line">    selected_imgIds = random.sample(imgIds, num_images)</span><br><span class="line">    <span class="comment"># Call the plot_image_with_annotations function for each selected ID</span></span><br><span class="line">    <span class="keyword">for</span> imgId <span class="keyword">in</span> selected_imgIds:</span><br><span class="line">        <span class="comment"># Plot images based on their IDs</span></span><br><span class="line">        plot_image_with_annotations(coco, cocoRoot, dataType, imgId)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Print out all selected IDs</span></span><br><span class="line">    <span class="keyword">return</span> selected_imgIds</span><br><span class="line">    </span><br><span class="line">valid_ids = random_select(coco, cocoRoot, dataType, num_images=<span class="number">10</span>)</span><br><span class="line">valid_ids</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Result</p>
</blockquote>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/TUiI82h.png" alt=""></p>
<h1 id="Task-3-5-FasterR-CNN-v-s-Mobilnet">Task 3+5: FasterR-CNN v.s Mobilnet</h1>
<div class="note info flat"><p><strong>Task 3 &amp; 5</strong><br>
3. <strong>Predicting bboxes using the pre-trained model FasterR-CNN</strong>ÔºöUse a pre-trained version of Faster R-CNN (Resnet50 backbone) to predict the bounding box<br>
of objects on the 10 images. Only keep regions that have a score &gt; 0.8.<br>
5. <strong>Using another pre-trained model Mobilnet</strong>ÔºöRepeat the steps from above using a Mobilenet backbone for the Faster R-CNN.</p>
</div>
<h2 id="Using-pre-train-model">Using pre-train model</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># using pre-train model (FasterR-CNN)</span></span><br><span class="line">model_res = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=<span class="string">&quot;FasterRCNN_ResNet50_FPN_Weights.DEFAULT&quot;</span>)</span><br><span class="line">model_res.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># using pre-train model  (Mobilenet)</span></span><br><span class="line">model_mobile = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=torchvision.models.detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights)</span><br><span class="line">model_mobile.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h2 id="Convert-image-to-tensor">Convert image to tensor</h2>
<p>We need to be able to take the image out of the picture based on the position of the image. Then we need to convert the image read from the book into a tensor before we can put it into the model for prediction. So we made two functions:</p>
<ul>
<li>One is to read the picture</li>
<li>One is to convert the picture to a tensor.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_image</span>(<span class="params">imgIdx</span>):</span><br><span class="line">    <span class="comment"># Get image information</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgIdx)[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># Get image location for visualization</span></span><br><span class="line">    imPath = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])    </span><br><span class="line">    <span class="comment"># Read the image path </span></span><br><span class="line">    <span class="built_in">print</span>(imPath)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># Read the image </span></span><br><span class="line">        <span class="keyword">return</span> Image.<span class="built_in">open</span>(imPath)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the picture to a tensor </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pil2tensor</span>(<span class="params">pil_image</span>):</span><br><span class="line">    <span class="comment"># Use unsqueeze(0) because the model still contains the batch size dimension, a total of four dimensions (batch_size, channel-RGB, height, width)</span></span><br><span class="line">    <span class="comment"># But the picture has only one picture without batch size, the picture is converted to tensor will only have three dimensions (channel-RGB, height, width), so we need to add a dimensions </span></span><br><span class="line">    <span class="comment"># /255 is because the input of the model is a number between 0 and 1, and the value of the picture is 0~255, so it needs to be divided by 255 for normalization </span></span><br><span class="line">    <span class="keyword">return</span> torchvision.transforms.PILToTensor()(pil_image).unsqueeze(<span class="number">0</span>) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure>
<h2 id="Training-the-model">Training the model</h2>
<p>After the pre-training model is loaded, we need to train the model. The training process is as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Save the prediction result </span></span><br><span class="line">predictions_res = []</span><br><span class="line">predictions_mobile = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Recursively call each id, these ids are the 10 ids we randomly selected above</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> valid_ids:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">    <span class="comment"># transform to tensor from PIL image</span></span><br><span class="line">    img_as_tensor = pil2tensor(load_image(i))</span><br><span class="line">    <span class="comment"># put the tensor to resnet model</span></span><br><span class="line">    prediction = model_res(img_as_tensor)</span><br><span class="line">    <span class="comment"># Save the prediction result: the prediction result is a dictionary, which contains the predicted bounding box, label, and accuracy </span></span><br><span class="line">    predictions_res.append(prediction)</span><br><span class="line">    <span class="comment"># put the tensor to mobilenet model</span></span><br><span class="line">    prediction = model_mobile(img_as_tensor)</span><br><span class="line">    <span class="comment"># Save the prediction result: the prediction result is a dictionary, which contains the predicted bounding box, label, and accuracy</span></span><br><span class="line">    predictions_mobile.append(prediction)</span><br></pre></td></tr></table></figure>
<h2 id="Only-select-the-prediction-results-0-8">Only select the prediction results &gt; 0.8</h2>
<p>After the model is trained, we need to select the prediction results that are greater than 0.8. The reason is that the model will predict a lot of bounding boxes, but we only need the bounding boxes with high accuracy. So we need to filter out the bounding boxes with low accuracy. The code is as follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">filter_valid_boxes</span>(<span class="params">predictions, threshold=<span class="number">0.8</span></span>):</span><br><span class="line">    <span class="comment"># Used to store the filtered prediction results </span></span><br><span class="line">    valid_boxes_list = []</span><br><span class="line">    <span class="comment"># Recursively call each prediction result </span></span><br><span class="line">    <span class="keyword">for</span> prediction <span class="keyword">in</span> predictions:</span><br><span class="line">        valid_boxes_for_this_prediction = []</span><br><span class="line">        <span class="comment"># Recursively call each bounding box </span></span><br><span class="line">        <span class="keyword">for</span> box, label, score <span class="keyword">in</span> <span class="built_in">zip</span>(prediction[<span class="number">0</span>][<span class="string">&quot;boxes&quot;</span>], prediction[<span class="number">0</span>][<span class="string">&quot;labels&quot;</span>], prediction[<span class="number">0</span>][<span class="string">&quot;scores&quot;</span>]):</span><br><span class="line">            <span class="comment"># Only keep the predicted bounding box with accuracy greater than threshold </span></span><br><span class="line">            <span class="keyword">if</span> score &gt;= threshold: </span><br><span class="line">                <span class="comment"># Save the predicted bounding box, label, and accuracy </span></span><br><span class="line">                valid_boxes_for_this_prediction.append((box, label, score))</span><br><span class="line">        <span class="comment"># If none of the predicted boxes in this picture have an accuracy greater than threshold, store an empty list </span></span><br><span class="line">        valid_boxes_list.append(valid_boxes_for_this_prediction)</span><br><span class="line">    <span class="comment"># Return the filtered prediction result </span></span><br><span class="line">    <span class="keyword">return</span> valid_boxes_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set threshold to 0.8 and get the prediction results of resnet and mobilenet </span></span><br><span class="line">valid_boxes_res = filter_valid_boxes(predictions_res, threshold=<span class="number">0.8</span>)</span><br><span class="line">valid_boxes_mobile = filter_valid_boxes(predictions_mobile, threshold=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Task-4-6-Visualization-IoU">Task 4+6: Visualization + IoU</h1>
<div class="note info flat"><p><strong>Tasks 4 &amp; 6</strong></p>
<ol>
<li><strong>Visualize the model together with the solution</strong>: Visualize the predicted bounding boxes and label together with the ground truth bounding</li>
<li><strong>CalculateIoU to compare models</strong>: Which backbone delivers the better results? Calculate the IoU for both approaches.</li>
</ol>
</div>
<p>There are a few important points in visual dialog, the steps are as follows:</p>
<ul>
<li>We need to know the id of the image first, and get the annotation information based on the id, then we can Calculate the IoU.</li>
<li>We take the annotation information and the model information to conduct the IoU Calculate.</li>
<li>We read the location of the image in the computer, and according to the path of the image, we draw the image through plt first.</li>
<li>We read the location of the picture in the computer, and based on the path of the picture, we draw the picture through plt first, and then based on the picture, we can draw the prediction box and label on it, as well as the average value of the IoU.</li>
</ul>
<p>The following program is the procedure described above, we will draw the results of both models and calculate the average of the IoU.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Can put the results of different models into this function, and it will return the average value of IoU </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_annotated_results</span>(<span class="params">imgId, valid_boxes, model_name, color=<span class="string">&#x27;g&#x27;</span>, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># Load the image</span></span><br><span class="line">    imgInfo = coco.loadImgs(imgId)[<span class="number">0</span>]</span><br><span class="line">    image_path = os.path.join(cocoRoot, dataType, imgInfo[<span class="string">&#x27;file_name&#x27;</span>])</span><br><span class="line">    image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get the correct bounding box results </span></span><br><span class="line">    annIds = coco.getAnnIds(imgIds=imgInfo[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">    anns = coco.loadAnns(annIds)</span><br><span class="line">    bbox_tlist_anns = torch.tensor([ann[<span class="string">&quot;bbox&quot;</span>] <span class="keyword">for</span> ann <span class="keyword">in</span> anns]) <span class="comment"># tensor.shape[2,4]</span></span><br><span class="line">    <span class="comment"># because our bounding box is x,y,w,h which is the coordinate of the lower left corner of the box (x,y) + the length and width of the box </span></span><br><span class="line">    <span class="comment"># But torchvision Calculate the box_iou must give the coordinates of the lower left corner (x,y) and the coordinates of the upper right corner (x2,y2), so we need to Calculate (x2,y2) through (x+w, y+h) to get the coordinates of the upper right corner </span></span><br><span class="line">    <span class="comment"># x,y,w,h -&gt; x1,y1,x2,y2 = x,y,x+w,y+h </span></span><br><span class="line">    bbox_tlist_anns[:, <span class="number">2</span>] = bbox_tlist_anns[:, <span class="number">0</span>] + bbox_tlist_anns[:, <span class="number">2</span>]</span><br><span class="line">    bbox_tlist_anns[:, <span class="number">3</span>] = bbox_tlist_anns[:, <span class="number">1</span>] + bbox_tlist_anns[:, <span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># From resultsvalid_boxes, we only need the box part, so we use (box, _, _)  </span></span><br><span class="line">    <span class="comment"># Use stack because we want to stack all the boxes together to become a tensor </span></span><br><span class="line">    bbox_tlist_model = torch.stack([box <span class="keyword">for</span> box, _, _ <span class="keyword">in</span> valid_boxes]) <span class="comment"># turn [4] to tensor.shape[2,4]</span></span><br><span class="line">    <span class="comment"># use box_iou ‰æÜCalculate IoU </span></span><br><span class="line">    iou = torchvision.ops.box_iou(bbox_tlist_anns, bbox_tlist_model) <span class="comment"># get IoU </span></span><br><span class="line">    <span class="comment"># Get the maximum value of each predicted box in ann, and then Calculate the average value of IoU </span></span><br><span class="line">    avg_iou = np.mean([t.cpu().detach().numpy().<span class="built_in">max</span>() <span class="keyword">for</span> t <span class="keyword">in</span> iou]) <span class="comment"># calculate the mean of IoU</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># display image label </span></span><br><span class="line">    all_labels = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Start drawing the prediction box </span></span><br><span class="line">    <span class="keyword">for</span> boxes <span class="keyword">in</span> valid_boxes:</span><br><span class="line">        <span class="comment"># Get the information of the prediction box, including box, label, and accuracy </span></span><br><span class="line">        box, label, score = boxes</span><br><span class="line">        <span class="comment"># Get the text information of the label: load category name by category ID</span></span><br><span class="line">        label = coco.loadCats(label.item())[<span class="number">0</span>][<span class="string">&quot;name&quot;</span>]</span><br><span class="line">        <span class="comment"># Save the label for later display </span></span><br><span class="line">        all_labels.add(label)</span><br><span class="line">        <span class="comment"># Because the results returned by the model are two coordinates, the lower left corner and the upper right corner, so we need to convert them into x,y,w,h form and put them into Rectangle </span></span><br><span class="line">        x, y, x2, y2 = box.detach().numpy() <span class="comment"># x,y,w,h -&gt; x,y,x2-x,y2-y</span></span><br><span class="line">        rect = Rectangle((x, y), x2 - x, y2 - y, linewidth=<span class="number">2</span>, edgecolor=color, facecolor=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Draw the picture: if you need to sort the picture, you can specify where to draw the picture through ax </span></span><br><span class="line">        <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># gca can get the current axes, if not, it will automatically create one, and then draw the prediction box through add_patch </span></span><br><span class="line">            plt.gca().add_patch(rect) </span><br><span class="line">            <span class="comment"># Draw the label on the prediction box </span></span><br><span class="line">            plt.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=color)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># add_patch can add a patch to the current axes, and then draw the prediction box on the ax </span></span><br><span class="line">            ax.add_patch(rect)</span><br><span class="line">            <span class="comment"># Draw the label on the prediction box </span></span><br><span class="line">            ax.text(x, y, <span class="string">f&#x27;<span class="subst">&#123;label&#125;</span>&#x27;</span>, fontsize=<span class="number">10</span>, color=<span class="string">&#x27;w&#x27;</span>, backgroundcolor=color)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># display image and give it a title, the title is the label that appears in this picture, and the average value of IoU </span></span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>: <span class="subst">&#123;all_labels&#125;</span> \n IoU: <span class="subst">&#123;avg_iou:<span class="number">.4</span>f&#125;</span>&#x27;</span>, color=color)</span><br><span class="line">        plt.imshow(image)</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_title(<span class="string">f&#x27;<span class="subst">&#123;model_name&#125;</span>: <span class="subst">&#123;all_labels&#125;</span> \n I0U: <span class="subst">&#123;avg_iou:<span class="number">.4</span>f&#125;</span>&#x27;</span>, color=color)</span><br><span class="line">        ax.imshow(image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg_iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">res_iou = []</span><br><span class="line">mobile_iou = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Recursively call each id, where id is one of the 10 random ids we selected above</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(valid_ids)):</span><br><span class="line">    <span class="comment"># Create a 1x3 grid of images, each image sized 15x5</span></span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Draw the truth image and display it in the center</span></span><br><span class="line">    plot_image_with_annotations(coco, cocoRoot, dataType, valid_ids[i], ax=axs[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Draw the predicted results from two different models on the left and right sides respectively, and return the IoU</span></span><br><span class="line">    i_mobil_iou = display_annotated_results(valid_ids[i], valid_boxes_mobile[i], <span class="string">&quot;mobile&quot;</span>, color=<span class="string">&#x27;g&#x27;</span>, ax=axs[<span class="number">0</span>])</span><br><span class="line">    i_res_iou = display_annotated_results(valid_ids[i], valid_boxes_res[i], <span class="string">&quot;ResNet&quot;</span>, color=<span class="string">&#x27;b&#x27;</span>, ax=axs[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the IoU of each image to assess the overall performance of the model</span></span><br><span class="line">    mobile_iou.append(i_mobil_iou)</span><br><span class="line">    res_iou.append(i_res_iou)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Organize the layout</span></span><br><span class="line">    plt.tight_layout()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the mean of the IoU list</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;ResNet: Avg.&quot;</span>, np.mean(res_iou), <span class="string">&quot;; each IoU:&quot;</span>, res_iou)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MobileNet: Avg.&quot;</span>, np.mean(mobile_iou), <span class="string">&quot;; each IoU:&quot;</span>, mobile_iou)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Result</p>
</blockquote>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/LjCVWdY.png" alt=""></p>
<h1 id="Supplement-IoU">Supplement: IoU</h1>
<ul>
<li>Ref: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/IAMoldpan/article/details/78799857">https://blog.csdn.net/IAMoldpan/article/details/78799857</a></li>
<li>Ref: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></li>
</ul>
<div class="note info flat"><p>IoU (Intersection over Union) is a metric used to evaluate object detection algorithms. It is defined as the <code>intersection area</code> of the <code>predicted box</code> and the <code>true box</code> divided by their <code>union area</code>. The value ranges between 0 and 1, where a higher value indicates a greater overlap between the predicted and true boxes, signifying more accurate predictions.</p>
</div>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/VzMudvr.png" alt=""><br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/OKroIoL.png" alt=""><br>
Source: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></p>
<div class="note info flat"><p><strong>From the example above, you might be wondering, what exactly does the following code segment do?</strong></p>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.ops.box_iou(bbox_tlist_anns, bbox_tlist_model) </span><br></pre></td></tr></table></figure>
<ul>
<li>Essentially, it calculates the Intersection over Union (IoU) between all predicted bounding boxes of the ground truth and all predicted bounding boxes of the model. This process returns a tensor with the shape (number of ground truth bounding boxes, number of model‚Äôs predicted bounding boxes). Refer to the images below.<br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/kQ6IVMY.png" alt=""><br>
<img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/lnjmgeD.png" alt=""></li>
</ul>
<p><strong>Here, we only need to obtain the maximum IoU for each ground truth bounding box and calculate the average value, so we use the following code</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># After obtaining the maximum value for each predicted box of the annotation (see supplementary IoU for details), calculate the average IoU</span></span><br><span class="line">avg_iou = np.mean([t.cpu().detach().numpy().<span class="built_in">max</span>() <span class="keyword">for</span> t <span class="keyword">in</span> iou]) <span class="comment"># calculate the mean of IoU</span></span><br></pre></td></tr></table></figure>
<div class="note warning flat"><p>You might be curious whether using functions like <code>max()</code>, <code>mean()</code>, <code>sum()</code> will affect our results?</p>
</div>
<p><img src="/en/img/loading-icon.gif" data-original="https://i.imgur.com/lnQtu1r.png" alt=""><br>
<strong>As we can see from the above image</strong></p>
<ul>
<li>Using <code>sum()</code>, you may find that the value can exceed 1, which is not a reasonable range for IoU values.</li>
<li>Using <code>max()</code>, it chooses, for each ground truth bounding box, the closest predicted bounding box from the model as its IoU. Then, we can obtain the maximum IoU values for <code>all predicted bounding boxes</code> of the ground truth and calculate the average to determine the overall IoU.</li>
<li>Using <code>mean()</code> poses a problem as the IoU calculation will never be 1. This is because it considers the IoUs of other bounding boxes, which lowers the overall IoU. For instance, if the ground truth has two bounding boxes <code>[A1,A2]</code>, and the model also predicts two <code>[B1,B2]</code>, it‚Äôs clear that B1 predicts A1, and B2 predicts A2, and the model predicts accurately. However, using mean will incorrectly consider B1 as A2 and B2 as A1, which is wrong, and these pairs have low IoUs. Thus, using <code>mean()</code> in this way will unjustly lower the IoU, making it unreasonable.</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io/en">Shannon Hung</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://shannonhung.github.io/en/posts/coco-object-diagnoise/">https://shannonhung.github.io/en/posts/coco-object-diagnoise/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/en/tags/Mechine-Learning/">Mechine Learning</a></div><div class="post_share"><div class="social-share" data-image="/en/img/cover/object-detection.jpeg" data-sites="facebook"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/flower-ml.jpeg" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN</div></div></a></div><div class="next-post pull-right"><a href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/lstm-emotion.jpeg" onerror="onerror=null;src='/en/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">Twitter Dataset - Using LSTM to predict the emotion of the article</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/lstm-emotion.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-16</div><div class="title">Twitter Dataset - Using LSTM to predict the emotion of the article</div></div></a></div><div><a href="/en/posts/PyTorch-Mac-GPU-Setup/" title="MAC OS -  PyTorch on Mac OS with GPU support"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/mac_ai_gpu.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-30</div><div class="title">MAC OS -  PyTorch on Mac OS with GPU support</div></div></a></div><div><a href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/flower-ml.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-31</div><div class="title">Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN</div></div></a></div><div><a href="/en/posts/TensorFlow-Windows-GPU-Setup/" title="Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/mac_ai_gpu.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-04</div><div class="title">Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow</div></div></a></div><div><a href="/en/posts/pytorch-CNN-TensorBoard/" title="CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard"><img class="cover" src="/en/img/loading-icon.gif" data-original="/en/img/cover/CNN-bg.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-24</div><div class="title">CIFAR10 Dataset - Using Pytorch to build CNN + activate GPU + output the result to TensorBoard</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/en/img/loading-icon.gif" data-original="/en/img/dudu-me.png" onerror="this.onerror=null;this.src='/en/en/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Shannon Hung</div><div class="author-info__description">In order to avoid forgetfulness, I started to record</div></div><div class="card-info-data site-data is-center"><a href="/en/archives/"><div class="headline">Articles</div><div class="length-num">13</div></a><a href="/en/tags/"><div class="headline">Tags</div><div class="length-num">12</div></a><a href="/en/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/ShannonHung"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ShannonHung" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hsuanhung036@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">I am a graduate student at the Taiwan University of Science and Technology - Institute of Information Management, planning to pursue a dual-degree program in Germany this year. The reason behind setting up this website is my rather short memory span. While I have learned a lot, I tend to forget much of it. To combat forgetfulness, I have embarked on my note-taking journey.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Homework-Requirement"><span class="toc-text">Homework Requirement</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-1-Downloading-the-COCO-Dataset"><span class="toc-text">Task 1: Downloading the COCO Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-2-Randomly-Select-Ten-Images"><span class="toc-text">Task 2: Randomly Select Ten Images</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Setting-up-the-COCO-API"><span class="toc-text">Setting up the COCO API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Annotation-Visualization"><span class="toc-text">Annotation Visualization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Randomly-Select-Ten-Images"><span class="toc-text">Randomly Select Ten Images</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-3-5-FasterR-CNN-v-s-Mobilnet"><span class="toc-text">Task 3+5: FasterR-CNN v.s Mobilnet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Using-pre-train-model"><span class="toc-text">Using pre-train model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Convert-image-to-tensor"><span class="toc-text">Convert image to tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-the-model"><span class="toc-text">Training the model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Only-select-the-prediction-results-0-8"><span class="toc-text">Only select the prediction results &gt; 0.8</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Task-4-6-Visualization-IoU"><span class="toc-text">Task 4+6: Visualization + IoU</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Supplement-IoU"><span class="toc-text">Supplement: IoU</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/en/posts/TensorFlow-Windows-GPU-Setup/" title="Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/mac_ai_gpu.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow"/></a><div class="content"><a class="title" href="/en/posts/TensorFlow-Windows-GPU-Setup/" title="Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow">Windows - A Step-by-Step Guide to Enable CUDA GPU on TensorFlow</a><time datetime="2024-09-04T12:55:39.000Z" title="Created 2024-09-04 20:55:39">2024-09-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/textract-multi-column/" title="How to Handle Multi-Column Text Sorting with Amazon Textract"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/ocr-scan.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="How to Handle Multi-Column Text Sorting with Amazon Textract"/></a><div class="content"><a class="title" href="/en/posts/textract-multi-column/" title="How to Handle Multi-Column Text Sorting with Amazon Textract">How to Handle Multi-Column Text Sorting with Amazon Textract</a><time datetime="2024-03-19T16:40:35.000Z" title="Created 2024-03-20 00:40:35">2024-03-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/lstm-emotion.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="Twitter Dataset - Using LSTM to predict the emotion of the article"/></a><div class="content"><a class="title" href="/en/posts/nlp-twitter-emotion-diagnoise/" title="Twitter Dataset - Using LSTM to predict the emotion of the article">Twitter Dataset - Using LSTM to predict the emotion of the article</a><time datetime="2023-11-16T06:30:45.000Z" title="Created 2023-11-16 14:30:45">2023-11-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/coco-object-diagnoise/" title="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/object-detection.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection"/></a><div class="content"><a class="title" href="/en/posts/coco-object-diagnoise/" title="COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection">COCO Dataset - use Faster RCNN + MobileNet to conduct Object Detection</a><time datetime="2023-11-03T02:57:54.000Z" title="Created 2023-11-03 10:57:54">2023-11-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"><img src="/en/img/loading-icon.gif" data-original="/en/img/cover/flower-ml.jpeg" onerror="this.onerror=null;this.src='/en/img/404.jpg'" alt="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN"/></a><div class="content"><a class="title" href="/en/posts/flower102-transfer-learning/" title="Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN">Flower102 Dataset - Using Transfer Learning to train + Using Batch Normalization in CNN</a><time datetime="2023-10-31T06:27:13.000Z" title="Created 2023-10-31 14:27:13">2023-10-31</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Shannon Hung</div><div class="footer_custom_text">Hi, welcome to Shannon <a target="_blank" rel="noopener external nofollow noreferrer" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="translateLink" type="button" title="Toggle Between Traditional Chinese And Simplified Chinese">EN</button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/en/js/utils.js"></script><script src="/en/js/main.js"></script><script src="/en/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: '51917ecdc184bb98b226',
      clientSecret: 'e48b44c1908c14b74ff7513f06c7cb892a4f4748',
      repo: 'shannonhung.github.io',
      owner: 'ShannonHung',
      admin: ['ShannonHung'],
      id: '570389e892f381708281be8a6f0eae9e',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script data-pjax src="/en/self/btf.js"></script><script data-pjax src="/en/self/tw_en.js"></script><script id="canvas_nest" defer="defer" color="139,71,38" opacity="0.5" zIndex="-1" count="500" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/en/js/search/local-search.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 3,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>